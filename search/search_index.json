{"config":{"indexing":"full","lang":["it"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"a riga di comando \u00b6 La riga di comando (la CLI ) ha reso facile e comode tante delle cose che faccio con il PC. Allora ho voluto dedicargli uno spazio, sia per farne conoscere i pregi a chi ancora li ignora o sfrutta poco, sia per mettere a fattor comune alcuni dei miei appunti . \u00c8 un argomento molto ampio, che qui verr\u00e0 esploso sopratutto su due filoni principali: i suoi strumenti per leggere , trasformare e analizzare file di testo strutturati ; le cose che mi piacciono , che mi sembrano \"geniali\", irrinunciabili, buffe, \" mai pi\u00f9 senza \", ecc.. \u00c8 dedicato sopratutto al suo utilizzo nel mondo Linux ed \u00e8 quindi per tutti .","title":"Home"},{"location":"#a-riga-di-comando","text":"La riga di comando (la CLI ) ha reso facile e comode tante delle cose che faccio con il PC. Allora ho voluto dedicargli uno spazio, sia per farne conoscere i pregi a chi ancora li ignora o sfrutta poco, sia per mettere a fattor comune alcuni dei miei appunti . \u00c8 un argomento molto ampio, che qui verr\u00e0 esploso sopratutto su due filoni principali: i suoi strumenti per leggere , trasformare e analizzare file di testo strutturati ; le cose che mi piacciono , che mi sembrano \"geniali\", irrinunciabili, buffe, \" mai pi\u00f9 senza \", ecc.. \u00c8 dedicato sopratutto al suo utilizzo nel mondo Linux ed \u00e8 quindi per tutti .","title":"a riga di comando"},{"location":"charts/","text":"Hardcoded \u00b6 ```vegalite { \"description\": \"A simple bar chart with embedded data.\", \"data\": { \"values\": [ {\"a\": \"A\", \"b\": 28}, {\"a\": \"B\", \"b\": 55}, {\"a\": \"C\", \"b\": 43}, {\"a\": \"D\", \"b\": 91}, {\"a\": \"E\", \"b\": 81}, {\"a\": \"F\", \"b\": 53}, {\"a\": \"G\", \"b\": 19}, {\"a\": \"H\", \"b\": 87}, {\"a\": \"I\", \"b\": 52} ] }, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } } ``` { \"description\": \"A simple bar chart with embedded data.\", \"data\": { \"values\": [ {\"a\": \"A\", \"b\": 28}, {\"a\": \"B\", \"b\": 55}, {\"a\": \"C\", \"b\": 43}, {\"a\": \"D\", \"b\": 91}, {\"a\": \"E\", \"b\": 81}, {\"a\": \"F\", \"b\": 53}, {\"a\": \"G\", \"b\": 19}, {\"a\": \"H\", \"b\": 87}, {\"a\": \"I\", \"b\": 52} ] }, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } } DA JSON \u00b6 ```vegalite { \"description\": \"A simple bar chart with embedded data.\", \"data\": {\"url\" : \"data/basic_bar_chart.json\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } } ``` { \"description\": \"A simple bar chart with embedded data.\", \"data\": {\"url\" : \"data/basic_bar_chart.json\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } } DA CSV \u00b6 ```vegalite { \"description\": \"A simple bar chart with embedded data.\", \"data\": {\"url\" : \"data/basic_bar_chart.csv\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } } ``` { \"description\": \"A simple bar chart with embedded data.\", \"data\": {\"url\" : \"data/basic_bar_chart.csv\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } }","title":"Charts"},{"location":"charts/#hardcoded","text":"```vegalite { \"description\": \"A simple bar chart with embedded data.\", \"data\": { \"values\": [ {\"a\": \"A\", \"b\": 28}, {\"a\": \"B\", \"b\": 55}, {\"a\": \"C\", \"b\": 43}, {\"a\": \"D\", \"b\": 91}, {\"a\": \"E\", \"b\": 81}, {\"a\": \"F\", \"b\": 53}, {\"a\": \"G\", \"b\": 19}, {\"a\": \"H\", \"b\": 87}, {\"a\": \"I\", \"b\": 52} ] }, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } } ``` { \"description\": \"A simple bar chart with embedded data.\", \"data\": { \"values\": [ {\"a\": \"A\", \"b\": 28}, {\"a\": \"B\", \"b\": 55}, {\"a\": \"C\", \"b\": 43}, {\"a\": \"D\", \"b\": 91}, {\"a\": \"E\", \"b\": 81}, {\"a\": \"F\", \"b\": 53}, {\"a\": \"G\", \"b\": 19}, {\"a\": \"H\", \"b\": 87}, {\"a\": \"I\", \"b\": 52} ] }, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } }","title":"Hardcoded"},{"location":"charts/#da-json","text":"```vegalite { \"description\": \"A simple bar chart with embedded data.\", \"data\": {\"url\" : \"data/basic_bar_chart.json\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } } ``` { \"description\": \"A simple bar chart with embedded data.\", \"data\": {\"url\" : \"data/basic_bar_chart.json\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } }","title":"DA JSON"},{"location":"charts/#da-csv","text":"```vegalite { \"description\": \"A simple bar chart with embedded data.\", \"data\": {\"url\" : \"data/basic_bar_chart.csv\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } } ``` { \"description\": \"A simple bar chart with embedded data.\", \"data\": {\"url\" : \"data/basic_bar_chart.csv\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": { \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"} } }","title":"DA CSV"},{"location":"cli/","text":"La CLI \u00b6 La riga di comando - in inglese CLI , la Command Line Intereface - \u00e8 un'interfaccia di dialogo testuale tra utente e computer , che interpreta i comandi inseriti da tastiera e li esegue. \u00c8 utilizzabile anche tramite script . Esistono decine di interfacce a riga di comando, per tutti i sistemi operativi. Qui si far\u00e0 riferimento a una generica CLI, in un sistema operativo Linux. \u00c8 possibile attivarla in tutti i sistemi operativi: Linux (ovviamente), Mac OS (che \u00e8 basato su un sistema operativo Unix), Windows (il modo consigliato per Win \u00e8 Windows Subsystem for Linux ), ChromeOS, ecc.. La CLI (chiamata anche shell ) inizialmente pu\u00f2 intimorire un po', ma pu\u00f2 essere un'ancora di salvezza per tantissime operazioni/elaborazioni. Esistono diversi sistemi di shell (distinti, ma ampiamente compatibili tra loro), tra i quali il pi\u00f9 popolare \u00e8 Bash, acronimo di \"Bourne again shell\". Bash \u00e8 sia una raccolta di utility (come grep , un potente strumento per fare ricerche di testo), che un linguaggio di programmazione, con i suoi for loop per ripetere operazioni su pi\u00f9 file. Molte discipline computazionali, come la bioinformatica, fanno affidamento sulla riga di comando. Ma tutte le persone che utilizzano un personal computer possono trarne vantaggio, perch\u00e9 il mouse non \u00e8 scalabile, mentre con la tastiera si pu\u00f2 sollevare il mondo \ud83d\ude43. Alcuni esempi. Operazioni su pi\u00f9 file in blocco \u00b6 Forse la caratteristica pi\u00f9 potente della shell \u00e8 la capacit\u00e0 di ripetere semplici processi su pi\u00f9 file . Una persona potrebbe, ad esempio, rinominare sistematicamente i propri file ed aggiungere al loro nome la data, o convertirli da un formato all'altro. Elvira Almiraghi, per i suoi studi di post-dottorato, aveva la necessit\u00e0 di aprire centinaia di immagini presenti in una cartella, invertirne i colori e modificarne anche luminosit\u00e0, saturazione e tonalit\u00e0. Operazioni di questo tipo, con un mouse , possono durare ore. Elvira, con la riga di comando e sfruttando ImageMagick - una utility dedicata alle immagini - ha semplificato tutto in queste righe: for file in *.png ; do convert $file -channel RGB -negate -modulate 100 ,100,200 out_ $file done Gestire file di grandi dimensioni \u00b6 Alcuni file di testo strutturato hanno dimensioni che li rendono inutilizzabili con la gran parte del software . Pasubio Giovinezza, un esperto di DNA, si trova giornalmente ad esempio a gestire file con milioni di varianti genetiche, che non si possono aprire con un foglio elettronico, perch\u00e9 in questo tipo di applicazioni le dimensioni di un foglio sono limitate a circa 1 milione di righe. Con la riga di comando e con le sue utility (come awk e cut ), Pasubio pu\u00f2 filtrare dai suoi file \"grandi\" solo le righe che superano una certa soglia di qualit\u00e0, estrarne soltanto alcune colonne e salvare tutto in un nuovo file. Aassumendo che il punteggio di qualit\u00e0 sia nella colonna 4, che la soglia sia a 50 e le colonne desiderate sono 1\u20134: awk -F, '{ if ($4 > 50) print $0 }' datafile.csv | cut -d, -f1-4 > newdatafile.csv Mettere in fila pi\u00f9 operazioni: la magia del \"pipe\" \u00b6 I comandi della shell nascono per fare una sola cosa e bene. Ad esempio cut estrae una o pi\u00f9 colonne da una tabella, wc conta le parole, le righe o i caratteri di un file, awk applica dei filtri alle righe basati su una o pi\u00f9 condizioni, e sed modifica flussi di stringhe di testo. Questi comandi possono essere messi tutti insieme, utilizzando il | (il \"pipe\", il tubo), una funzionalit\u00e0 della shell che incanala l' output di un comando verso un altro. Giorgio Perozzi, un amministratore di sistema, dice che \"il | d\u00e0 la possibilit\u00e0 di trovare rapidamente soluzioni a problemi quotidiani e prototiparne rapidamente tante\". Il comando di sotto, per contare i nomi univoci di geni da una sequenza, avrebbe fatto comodo a Pasubio: cut -f1 GEOdataset.csv | sed -E 's/^>//' | sort | uniq | wc -l Questo \u00e8 quello che fa il comando soprastante: estrae la prima prima colonna ( cut ), rimuove il carattere > eventualmente presente a inizio riga ( sed ), ordina il tutto alfabeticamente ( sort ), estrae i valori univoci ( uniq ) e stampa il numero di righe di output ( wc ). Eseguire processi in parallelo \u00b6 Valentina Gherardini, lavora in un centro di calcolo che fornisce accesso remoto a circa 14.000 nodi, verso terabyte di dati. Supponiamo, dice la Gherardini, che un geofisico debba svolgere un flusso di lavoro computazionale per analizzare set di dati sismici, che ogni set richieda un giorno per essere elaborato sul proprio computer e che il ricercatore abbia 60 di questi set di dati. \"Sono due mesi ininterrotti di elaborazione\", dice. Ma, inviando il lavoro a un cluster di computer, in connessione sicura con ssh , aprendo un portale crittografato da remoto, il ricercatore pu\u00f2 parallelizzare i suoi calcoli su 60 computer. \"Invece di due mesi, ci vuole un giorno.\" La parallelizzazione dei processi, si realizza in modo ricco ed efficiente nella shell . Una utility dedicata \u00e8 parallel . Automatizzare i processi \u00b6 I comandi della shell possono essere archiviati in file di testo chiamati script , che possono essere salvati, condivisi e sottoposti a versioning , migliorandone la riproducibilit\u00e0. Possono anche essere automatizzati. Utilizzando il comando cron , gli utenti possono pianificare l'esecuzione degli script . Ad esempio, afferma Perozzi, alcuni siti Web suggeriscono agli utenti che intendono fare scraping /download dei loro contenuti, di farlo in orari \"notturni\" - ad esempio tra le 21:00 e le 5:00. \"Puoi far eseguire [lo script] solo negli orari suggeriti\". Un esempio \u00e8 quello del \"NCBI National Center for Biotechnology Information\", in cui nelle linee guida si legge \" Run retrieval scripts on weekends or between 9 pm and 5 am Eastern Time weekdays for any series of more than 100 requests. \"","title":"La CLI"},{"location":"cli/#la-cli","text":"La riga di comando - in inglese CLI , la Command Line Intereface - \u00e8 un'interfaccia di dialogo testuale tra utente e computer , che interpreta i comandi inseriti da tastiera e li esegue. \u00c8 utilizzabile anche tramite script . Esistono decine di interfacce a riga di comando, per tutti i sistemi operativi. Qui si far\u00e0 riferimento a una generica CLI, in un sistema operativo Linux. \u00c8 possibile attivarla in tutti i sistemi operativi: Linux (ovviamente), Mac OS (che \u00e8 basato su un sistema operativo Unix), Windows (il modo consigliato per Win \u00e8 Windows Subsystem for Linux ), ChromeOS, ecc.. La CLI (chiamata anche shell ) inizialmente pu\u00f2 intimorire un po', ma pu\u00f2 essere un'ancora di salvezza per tantissime operazioni/elaborazioni. Esistono diversi sistemi di shell (distinti, ma ampiamente compatibili tra loro), tra i quali il pi\u00f9 popolare \u00e8 Bash, acronimo di \"Bourne again shell\". Bash \u00e8 sia una raccolta di utility (come grep , un potente strumento per fare ricerche di testo), che un linguaggio di programmazione, con i suoi for loop per ripetere operazioni su pi\u00f9 file. Molte discipline computazionali, come la bioinformatica, fanno affidamento sulla riga di comando. Ma tutte le persone che utilizzano un personal computer possono trarne vantaggio, perch\u00e9 il mouse non \u00e8 scalabile, mentre con la tastiera si pu\u00f2 sollevare il mondo \ud83d\ude43. Alcuni esempi.","title":"La CLI"},{"location":"cli/#operazioni-su-piu-file-in-blocco","text":"Forse la caratteristica pi\u00f9 potente della shell \u00e8 la capacit\u00e0 di ripetere semplici processi su pi\u00f9 file . Una persona potrebbe, ad esempio, rinominare sistematicamente i propri file ed aggiungere al loro nome la data, o convertirli da un formato all'altro. Elvira Almiraghi, per i suoi studi di post-dottorato, aveva la necessit\u00e0 di aprire centinaia di immagini presenti in una cartella, invertirne i colori e modificarne anche luminosit\u00e0, saturazione e tonalit\u00e0. Operazioni di questo tipo, con un mouse , possono durare ore. Elvira, con la riga di comando e sfruttando ImageMagick - una utility dedicata alle immagini - ha semplificato tutto in queste righe: for file in *.png ; do convert $file -channel RGB -negate -modulate 100 ,100,200 out_ $file done","title":"Operazioni su pi\u00f9 file in blocco"},{"location":"cli/#gestire-file-di-grandi-dimensioni","text":"Alcuni file di testo strutturato hanno dimensioni che li rendono inutilizzabili con la gran parte del software . Pasubio Giovinezza, un esperto di DNA, si trova giornalmente ad esempio a gestire file con milioni di varianti genetiche, che non si possono aprire con un foglio elettronico, perch\u00e9 in questo tipo di applicazioni le dimensioni di un foglio sono limitate a circa 1 milione di righe. Con la riga di comando e con le sue utility (come awk e cut ), Pasubio pu\u00f2 filtrare dai suoi file \"grandi\" solo le righe che superano una certa soglia di qualit\u00e0, estrarne soltanto alcune colonne e salvare tutto in un nuovo file. Aassumendo che il punteggio di qualit\u00e0 sia nella colonna 4, che la soglia sia a 50 e le colonne desiderate sono 1\u20134: awk -F, '{ if ($4 > 50) print $0 }' datafile.csv | cut -d, -f1-4 > newdatafile.csv","title":"Gestire file di grandi dimensioni"},{"location":"cli/#mettere-in-fila-piu-operazioni-la-magia-del-pipe","text":"I comandi della shell nascono per fare una sola cosa e bene. Ad esempio cut estrae una o pi\u00f9 colonne da una tabella, wc conta le parole, le righe o i caratteri di un file, awk applica dei filtri alle righe basati su una o pi\u00f9 condizioni, e sed modifica flussi di stringhe di testo. Questi comandi possono essere messi tutti insieme, utilizzando il | (il \"pipe\", il tubo), una funzionalit\u00e0 della shell che incanala l' output di un comando verso un altro. Giorgio Perozzi, un amministratore di sistema, dice che \"il | d\u00e0 la possibilit\u00e0 di trovare rapidamente soluzioni a problemi quotidiani e prototiparne rapidamente tante\". Il comando di sotto, per contare i nomi univoci di geni da una sequenza, avrebbe fatto comodo a Pasubio: cut -f1 GEOdataset.csv | sed -E 's/^>//' | sort | uniq | wc -l Questo \u00e8 quello che fa il comando soprastante: estrae la prima prima colonna ( cut ), rimuove il carattere > eventualmente presente a inizio riga ( sed ), ordina il tutto alfabeticamente ( sort ), estrae i valori univoci ( uniq ) e stampa il numero di righe di output ( wc ).","title":"Mettere in fila pi\u00f9 operazioni: la magia del \"pipe\""},{"location":"cli/#eseguire-processi-in-parallelo","text":"Valentina Gherardini, lavora in un centro di calcolo che fornisce accesso remoto a circa 14.000 nodi, verso terabyte di dati. Supponiamo, dice la Gherardini, che un geofisico debba svolgere un flusso di lavoro computazionale per analizzare set di dati sismici, che ogni set richieda un giorno per essere elaborato sul proprio computer e che il ricercatore abbia 60 di questi set di dati. \"Sono due mesi ininterrotti di elaborazione\", dice. Ma, inviando il lavoro a un cluster di computer, in connessione sicura con ssh , aprendo un portale crittografato da remoto, il ricercatore pu\u00f2 parallelizzare i suoi calcoli su 60 computer. \"Invece di due mesi, ci vuole un giorno.\" La parallelizzazione dei processi, si realizza in modo ricco ed efficiente nella shell . Una utility dedicata \u00e8 parallel .","title":"Eseguire processi in parallelo"},{"location":"cli/#automatizzare-i-processi","text":"I comandi della shell possono essere archiviati in file di testo chiamati script , che possono essere salvati, condivisi e sottoposti a versioning , migliorandone la riproducibilit\u00e0. Possono anche essere automatizzati. Utilizzando il comando cron , gli utenti possono pianificare l'esecuzione degli script . Ad esempio, afferma Perozzi, alcuni siti Web suggeriscono agli utenti che intendono fare scraping /download dei loro contenuti, di farlo in orari \"notturni\" - ad esempio tra le 21:00 e le 5:00. \"Puoi far eseguire [lo script] solo negli orari suggeriti\". Un esempio \u00e8 quello del \"NCBI National Center for Biotechnology Information\", in cui nelle linee guida si legge \" Run retrieval scripts on weekends or between 9 pm and 5 am Eastern Time weekdays for any series of more than 100 requests. \"","title":"Automatizzare i processi"},{"location":"grazie/","text":"Grazie \u00b6 Gabriele John GDAL/OGR Jeroen Janssens","title":"Grazie"},{"location":"grazie/#grazie","text":"Gabriele John GDAL/OGR Jeroen Janssens","title":"Grazie"},{"location":"letture/","text":"Letture consigliate \u00b6 Libri \u00b6 Questi testi sono stati un riferimento per arrivare allo sviluppo di questo progetto; in particolare la I versione del volume di Jeroen Janssens (che ringraziamo molto), di cui nel 2021 \u00e8 stata pubblicata la II versione: Janssens, Jeroen . Data Science at the Command Line, 2e. https://www.datascienceatthecommandline.com/2e/ Vince, Buffalo . Bioinformatics Data Skills. https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/ William, Shotts . The Linux Command Line. https://linuxcommand.org/tlcl.php Robert Mesibov. A data cleaner's cookbook. https://www.datafix.com.au/cookbook/ Siti \u00b6 Bashing data, https://www.datafix.com.au/BASHing/index.html The Programming Historian, https://programminghistorian.org/en/ explain shell, https://explainshell.com/ Spunti \u00b6 Five reasons why researchers should learn to love the command line (NdR: \u00e8 stato utilizzato per scrivere gran parte della pagina CLI ), https://www.nature.com/articles/d41586-021-00263-0 Nine simple ways to make it easier to (re)use your data, https://ojs.library.queensu.ca/index.php/IEE/article/view/4608","title":"Letture"},{"location":"letture/#letture-consigliate","text":"","title":"Letture consigliate"},{"location":"letture/#libri","text":"Questi testi sono stati un riferimento per arrivare allo sviluppo di questo progetto; in particolare la I versione del volume di Jeroen Janssens (che ringraziamo molto), di cui nel 2021 \u00e8 stata pubblicata la II versione: Janssens, Jeroen . Data Science at the Command Line, 2e. https://www.datascienceatthecommandline.com/2e/ Vince, Buffalo . Bioinformatics Data Skills. https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/ William, Shotts . The Linux Command Line. https://linuxcommand.org/tlcl.php Robert Mesibov. A data cleaner's cookbook. https://www.datafix.com.au/cookbook/","title":"Libri"},{"location":"letture/#siti","text":"Bashing data, https://www.datafix.com.au/BASHing/index.html The Programming Historian, https://programminghistorian.org/en/ explain shell, https://explainshell.com/","title":"Siti"},{"location":"letture/#spunti","text":"Five reasons why researchers should learn to love the command line (NdR: \u00e8 stato utilizzato per scrivere gran parte della pagina CLI ), https://www.nature.com/articles/d41586-021-00263-0 Nine simple ways to make it easier to (re)use your data, https://ojs.library.queensu.ca/index.php/IEE/article/view/4608","title":"Spunti"},{"location":"monografie/","text":"Monografie \u00b6 In questa sezione delle guide dedicate ad alcuni strumenti per la lettura , trasformazione e analisi di file di testo strutturati , molto consigliati: Miller ; VisiData .","title":"Introduzione"},{"location":"monografie/#monografie","text":"In questa sezione delle guide dedicate ad alcuni strumenti per la lettura , trasformazione e analisi di file di testo strutturati , molto consigliati: Miller ; VisiData .","title":"Monografie"},{"location":"partecipa/","text":"Partecipa \u00b6 Se ti piace a riga di comando e vuoi dare una mano, puoi farlo in queste modalit\u00e0: segnalare qualcosa da correggere; proporre una nuovo paragrafo, pagina, sezione, ecc.. Lo spazio per farlo sono le discussioni su GitHub .","title":"Partecipa"},{"location":"partecipa/#partecipa","text":"Se ti piace a riga di comando e vuoi dare una mano, puoi farlo in queste modalit\u00e0: segnalare qualcosa da correggere; proporre una nuovo paragrafo, pagina, sezione, ecc.. Lo spazio per farlo sono le discussioni su GitHub .","title":"Partecipa"},{"location":"dati/","text":"","title":"Introduzione"},{"location":"dati/converti/","text":"Converti \u00b6","title":"Converti"},{"location":"dati/converti/#converti","text":"","title":"Converti"},{"location":"dati/esplora/","text":"Esplora \u00b6 In questo spazio facciamo riferimento a dati che sono archiviati come testo , una modalit\u00e0 molto diffusa (si pensi ai formati JSON , XML , CSV , TTL , YAML , ecc.) e per la quale gli strumenti a riga di comando hanno supporto nativo: si aspettano stringhe di testo come input . Tanti strumenti di base e nativi sono impareggiabili nell'esplorare file di testo, in termini di rapidit\u00e0 , opzioni e modalit\u00e0 per farlo. Sono spesso (in modo evidente) pi\u00f9 comodi della gran parte di quelli con interfaccia grafica. Visualizzare \u00b6 Uno dei modi per conoscere un file \u00e8 quello di visualizzarne i contenuti . Queste sono alcune delle utility classiche per farlo nel terminale. cat \u00b6 L' utility tipica \u00e8 cat che \"stampa\" a schermo uno o pi\u00f9 file (in questo caso concatena e stampa). Se si vuole visualizzare il file colored-shapes.csv , il comando \u00e8 semplicemente: cat colored-shapes.csv Vengono per\u00f2 stampate a schermo tutte le righe , e con file molto grandi pu\u00f2 essere un po' lento e sopratutto poco informativo , perch\u00e9 non si legger\u00e0 l'intestazione che in alcuni formati (come i CSV ) \u00e8 un elemento di esplorazione dei dati prezioso. Tip Esiste anche la versione di cat , che consente di leggere anche file di testo compressi: zcat . head \u00b6 Per visualizzare soltanto le prime righe c'\u00e8 l' utility head , che in maniere predefinita stampa le prime 10 righe: head colored-shapes.csv color,shape,flag,i,u,v,w,x yellow,triangle,1,56,0.632170,0.988721,0.436498,5.798188 red,square,1,80,0.219668,0.001257,0.792778,2.944117 red,circle,1,84,0.209017,0.290052,0.138103,5.065034 red,square,0,243,0.956274,0.746720,0.775542,7.117831 purple,triangle,0,257,0.435535,0.859129,0.812290,5.753095 red,square,0,322,0.201551,0.953110,0.771991,5.612050 purple,triangle,0,328,0.684281,0.582372,0.801405,5.805148 yellow,circle,1,370,0.603365,0.423708,0.639785,7.006414 yellow,circle,1,440,0.285656,0.833516,0.635058,6.350036 Con l'opzione -n numeroDiRighe \u00e8 possibile scegliere il numero di righe da visualizzare: head -n 3 colored-shapes.csv color,shape,flag,i,u,v,w,x yellow,triangle,1,56,0.632170,0.988721,0.436498,5.798188 red,square,1,80,0.219668,0.001257,0.792778,2.944117 Per un formato come il CSV il comando head \u00e8 prezioso, perch\u00e9 nella gran parte dei casi restituisce una buona visione del file: qual \u00e8 il separatore dei campi, quali sono i campi, qual \u00e8 il separatore dei decimali (se presenti), se ci sono caratteri speciali (come le \" ), ecc.. tail \u00b6 Ma \u00e8 bene poter vedere anche le ultime righe , per valutare se la struttura \u00e8 identica a quella di intestazione e se ci sono \"strani\" contenuti (alle volte purtroppo ci sono note di testo, campi calcolati). Il comando \u00e8 tail : tail colored-shapes.csv Di default, le ultime 10 righe; si pu\u00f2 definire quante se ne desiderano sempre con l'opzione -n numeroDiRighe . less \u00b6 Un altro strumento consigliato (ce sono tanti altri), per visualizzare ed esplorare il contenuto di un file di testo \u00e8 less , che consente di sfogliare il contenuto schermata dopo schermata , man mano che lo schermo viene riempito: less colored-shapes.csv Alcune informazioni di base su less con l'o\"pzione -S si disabilita il world wrap con Space si scorrono le schermate (vedi immagine sotto); con / si attiva la ricerca di una stringa; con n il risultato successivo della ricerca; con N il risultato successivo della ricerca; con G si va alla fine del file; con g si va all'inizio; con Q si esce da less . Tip In questo modo - con head , tail e less - \u00e8 possibile esplorare anche file di testo di grandi dimensioni . Le utility di questa sezione per\u00f2 fanno soltanto visualizzare contenuti, non estraggono informazioni (numero di righe, colonne, encoding , dimensioni, ecc..). Informazioni sui file \u00b6 file \u00b6 L'utility di base, preinstallata in tutti i sistemi Linux \u00e8 file , che \u00e8 utile per avere informazioni sul tipo di file . file base_category.csv base_category.csv: CSV text Con l'opzione -i si ottengono informazioni sull' encoding . file -i encoding_iso-8859-1.csv encoding_iso-8859-1.csv: application/csv; charset=iso-8859-1 Conoscere l' encoding di un file di testo strutturato, da usare per fare analisi e trasformazione di dati \u00e8 un elemento essenziale, perch\u00e9 se \"mappato\" scorrettamente porta a una lettura errata dei contenuti del file. stat \u00b6 stat fornisce moltissime informazioni sui file, come le dimensioni, i permessi, la data di modifica, ecc.: stat encoding_iso-8859-1.csv File: encoding_iso-8859-1.csv Size: 33 Blocks: 0 IO Block: 512 regular file Device: 2dh/45d Inode: 266275327968286903 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 1000/userName) Gid: ( 1000/userName) Access: 2022-02-19 09:17:54.648775600 +0100 Modify: 2022-02-19 09:17:54.637142300 +0100 Change: 2022-02-19 12:44:32.134296200 +0100 Birth: - Encoding \u00b6 Conoscere la codifica dei caratteri \u00e8 un elemento chiave per la loro lettura. Specie con formati di testo come il CSV , in cui le informazioni sull' encoding non sono scritte all'interno del file (se va bene sono riportate in un testo che descrive il file ). Si pu\u00f2 provare - come visto sopra - a estrapolare con file (o con chardet ). Un'esperienza comune di barriera all'uso di un file di testo, di cui non si conosce l' encoding , \u00e8 quella ad esempio della errata lettura dei caratteri accentati, in cui un file come questo , viene letto male ( Cefal\ufffd , sarebbe Cefal\u00f9 ): encoding_iso-8859-1.csv id,nomeComune 1,Cefal\ufffd 2,Milano 3,Orr\ufffd Tip Spesso le utility hanno impostato come encoding standard di lettura l' UTF-8 . Per leggere correttamente il file di sopra se ne pu\u00f2 provare a estrapolare la codifica corretta con file (o con chardet ) e poi se serve trasformarla in quella di output desiderata. Le \"misure\" \u00b6 A un file testuale di dati \u00e8 importante \" prendere le misure \", come il numero di righe, il numero di caratteri, numero di colonne (se \u00e8 un file a griglia tabellare), ecc.. wc \u00b6 wc \u00e8 una delle utility pi\u00f9 importanti per \" esplorare \" file di testo . Questo il file di input di esempio: wc-01.csv id,titolo 1,La spada nella roccia 2,Il mago di oz comando output descrizione wc wc-01.csv 3 9 50 wc-01.csv numero di righe, parole e byte wc -l wc-01.csv 3 wc-01.csv numero di righe wc -w wc-01.csv 9 wc-01.csv numero di parole (separate da spazio) wc -c wc-01.csv 50 wc-01.csv numero di byte wc --max-line-length wc-01.csv 23 wc-01.csv il numero di caratteri della riga pi\u00f9 lunga Tip Se si vuole soltanto il numero di righe , senza il nome del file , il comando \u00e8 <wc-01.csv wc -l wc non \u00e8 per\u00f2 un comando che interpreta il tipo di formato , e quindi in presenza di un file di input come questo sottostante - un CSV di due record (pi\u00f9 intestazione) e due colonne - il conteggio delle righe sar\u00e0 pari a 8. 1 2 3 4 5 6 7 8 FieldA,FieldB \"Come Quando Fuori Piove\",Ciao Miao,\"Uno Due Tre\" Questo file - vedi anteprima sottostante - contiene dei ritorni a capo nelle celle, e per ognuno viene conteggiata una riga. FieldA FieldB Come Quando Fuori Piove Ciao Miao Uno Due Tre Attenzione Per poter conteggiare il numero di record di un CSV come questo, \u00e8 necessario usare una utility che tenga conto delle caratteristiche del formato ( CSV aware ), come ad esempio il fatto che in una cella ci possono essere pi\u00f9 righe. Miller \u00b6 Miller \u00e8 una straordinaria utility per leggere, analizzare e trasformare file di testo strutturati . \u00c8 capace di interpretare le caratteristiche dei formati che supporta ( CSV , TSV , JSON , ecc.) e quindi ad esempio contare correttamente il numero di record di un file CSV , anche in presenza di celle con ritorni a capo (vedi esempio soprastante). mlr --csv tail -n 1 then put '$righe=NR' then cut -f righe wc-02.csv righe 2 Alcune note: --csv per impostare formato di input e output ; tail -n 1 per estrarre l'ultima riga; put '$righe=NR' per creare il campo righe e valorizzarlo con il numero di riga della riga corrente - NR - che qui \u00e8 l'ultima; cut -f righe , per avere in output soltanto il campo precedentemente creato. Se si vuole in output anche il numero di colonne , il comando si pu\u00f2 modificare in questo modo: mlr --csv tail -n 1 then put '$righe=NR;$colonne=NF-1' then cut -f righe,colonne wc-02.csv Con $colonne=NF-1 \u00e8 stato aggiunto un campo che conta il numero di campi - NF - presenti nella riga corrente, a cui \u00e8 stato sottratto 1, perch\u00e9 \u00e8 stata aggiunta la colonna che d\u00e0 conto del numero di righe, che non fa parte delle colonne pre esistenti nel file di input . In output : righe,colonne 2,2 Un esempio pi\u00f9 eclatante \u00e8 quando il file di input \u00e8 un JSON come quello sottostante, composto da 2 \"record\" (qui \u00e8 improprio come termine, ma per dare l'idea), ognuno con due campi. 1 2 3 4 5 6 7 8 9 10 [ { \"id\" : 1 , \"titolo\" : \"La spada nella roccia\" }, { \"id\" : 2 , \"titolo\" : \"Il mago di oz\" } ] Il comando mlr --j2c tail -n 1 then put '$righe=NR;$colonne=NF-1' then cut -f righe,colonne input.json restituir\u00e0 sempre ( --j2c \u00e8 per trasformare l'input in JSON in CSV ) righe,colonne 2,2 Info A Miller \u00e8 dedicata una monografia di questo sito.","title":"Esplora"},{"location":"dati/esplora/#esplora","text":"In questo spazio facciamo riferimento a dati che sono archiviati come testo , una modalit\u00e0 molto diffusa (si pensi ai formati JSON , XML , CSV , TTL , YAML , ecc.) e per la quale gli strumenti a riga di comando hanno supporto nativo: si aspettano stringhe di testo come input . Tanti strumenti di base e nativi sono impareggiabili nell'esplorare file di testo, in termini di rapidit\u00e0 , opzioni e modalit\u00e0 per farlo. Sono spesso (in modo evidente) pi\u00f9 comodi della gran parte di quelli con interfaccia grafica.","title":"Esplora"},{"location":"dati/esplora/#visualizzare","text":"Uno dei modi per conoscere un file \u00e8 quello di visualizzarne i contenuti . Queste sono alcune delle utility classiche per farlo nel terminale.","title":"Visualizzare"},{"location":"dati/esplora/#cat","text":"L' utility tipica \u00e8 cat che \"stampa\" a schermo uno o pi\u00f9 file (in questo caso concatena e stampa). Se si vuole visualizzare il file colored-shapes.csv , il comando \u00e8 semplicemente: cat colored-shapes.csv Vengono per\u00f2 stampate a schermo tutte le righe , e con file molto grandi pu\u00f2 essere un po' lento e sopratutto poco informativo , perch\u00e9 non si legger\u00e0 l'intestazione che in alcuni formati (come i CSV ) \u00e8 un elemento di esplorazione dei dati prezioso. Tip Esiste anche la versione di cat , che consente di leggere anche file di testo compressi: zcat .","title":"cat"},{"location":"dati/esplora/#head","text":"Per visualizzare soltanto le prime righe c'\u00e8 l' utility head , che in maniere predefinita stampa le prime 10 righe: head colored-shapes.csv color,shape,flag,i,u,v,w,x yellow,triangle,1,56,0.632170,0.988721,0.436498,5.798188 red,square,1,80,0.219668,0.001257,0.792778,2.944117 red,circle,1,84,0.209017,0.290052,0.138103,5.065034 red,square,0,243,0.956274,0.746720,0.775542,7.117831 purple,triangle,0,257,0.435535,0.859129,0.812290,5.753095 red,square,0,322,0.201551,0.953110,0.771991,5.612050 purple,triangle,0,328,0.684281,0.582372,0.801405,5.805148 yellow,circle,1,370,0.603365,0.423708,0.639785,7.006414 yellow,circle,1,440,0.285656,0.833516,0.635058,6.350036 Con l'opzione -n numeroDiRighe \u00e8 possibile scegliere il numero di righe da visualizzare: head -n 3 colored-shapes.csv color,shape,flag,i,u,v,w,x yellow,triangle,1,56,0.632170,0.988721,0.436498,5.798188 red,square,1,80,0.219668,0.001257,0.792778,2.944117 Per un formato come il CSV il comando head \u00e8 prezioso, perch\u00e9 nella gran parte dei casi restituisce una buona visione del file: qual \u00e8 il separatore dei campi, quali sono i campi, qual \u00e8 il separatore dei decimali (se presenti), se ci sono caratteri speciali (come le \" ), ecc..","title":"head"},{"location":"dati/esplora/#tail","text":"Ma \u00e8 bene poter vedere anche le ultime righe , per valutare se la struttura \u00e8 identica a quella di intestazione e se ci sono \"strani\" contenuti (alle volte purtroppo ci sono note di testo, campi calcolati). Il comando \u00e8 tail : tail colored-shapes.csv Di default, le ultime 10 righe; si pu\u00f2 definire quante se ne desiderano sempre con l'opzione -n numeroDiRighe .","title":"tail"},{"location":"dati/esplora/#less","text":"Un altro strumento consigliato (ce sono tanti altri), per visualizzare ed esplorare il contenuto di un file di testo \u00e8 less , che consente di sfogliare il contenuto schermata dopo schermata , man mano che lo schermo viene riempito: less colored-shapes.csv Alcune informazioni di base su less con l'o\"pzione -S si disabilita il world wrap con Space si scorrono le schermate (vedi immagine sotto); con / si attiva la ricerca di una stringa; con n il risultato successivo della ricerca; con N il risultato successivo della ricerca; con G si va alla fine del file; con g si va all'inizio; con Q si esce da less . Tip In questo modo - con head , tail e less - \u00e8 possibile esplorare anche file di testo di grandi dimensioni . Le utility di questa sezione per\u00f2 fanno soltanto visualizzare contenuti, non estraggono informazioni (numero di righe, colonne, encoding , dimensioni, ecc..).","title":"less"},{"location":"dati/esplora/#informazioni-sui-file","text":"","title":"Informazioni sui file"},{"location":"dati/esplora/#file","text":"L'utility di base, preinstallata in tutti i sistemi Linux \u00e8 file , che \u00e8 utile per avere informazioni sul tipo di file . file base_category.csv base_category.csv: CSV text Con l'opzione -i si ottengono informazioni sull' encoding . file -i encoding_iso-8859-1.csv encoding_iso-8859-1.csv: application/csv; charset=iso-8859-1 Conoscere l' encoding di un file di testo strutturato, da usare per fare analisi e trasformazione di dati \u00e8 un elemento essenziale, perch\u00e9 se \"mappato\" scorrettamente porta a una lettura errata dei contenuti del file.","title":"file"},{"location":"dati/esplora/#stat","text":"stat fornisce moltissime informazioni sui file, come le dimensioni, i permessi, la data di modifica, ecc.: stat encoding_iso-8859-1.csv File: encoding_iso-8859-1.csv Size: 33 Blocks: 0 IO Block: 512 regular file Device: 2dh/45d Inode: 266275327968286903 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 1000/userName) Gid: ( 1000/userName) Access: 2022-02-19 09:17:54.648775600 +0100 Modify: 2022-02-19 09:17:54.637142300 +0100 Change: 2022-02-19 12:44:32.134296200 +0100 Birth: -","title":"stat"},{"location":"dati/esplora/#encoding","text":"Conoscere la codifica dei caratteri \u00e8 un elemento chiave per la loro lettura. Specie con formati di testo come il CSV , in cui le informazioni sull' encoding non sono scritte all'interno del file (se va bene sono riportate in un testo che descrive il file ). Si pu\u00f2 provare - come visto sopra - a estrapolare con file (o con chardet ). Un'esperienza comune di barriera all'uso di un file di testo, di cui non si conosce l' encoding , \u00e8 quella ad esempio della errata lettura dei caratteri accentati, in cui un file come questo , viene letto male ( Cefal\ufffd , sarebbe Cefal\u00f9 ): encoding_iso-8859-1.csv id,nomeComune 1,Cefal\ufffd 2,Milano 3,Orr\ufffd Tip Spesso le utility hanno impostato come encoding standard di lettura l' UTF-8 . Per leggere correttamente il file di sopra se ne pu\u00f2 provare a estrapolare la codifica corretta con file (o con chardet ) e poi se serve trasformarla in quella di output desiderata.","title":"Encoding"},{"location":"dati/esplora/#le-misure","text":"A un file testuale di dati \u00e8 importante \" prendere le misure \", come il numero di righe, il numero di caratteri, numero di colonne (se \u00e8 un file a griglia tabellare), ecc..","title":"Le \"misure\""},{"location":"dati/esplora/#wc","text":"wc \u00e8 una delle utility pi\u00f9 importanti per \" esplorare \" file di testo . Questo il file di input di esempio: wc-01.csv id,titolo 1,La spada nella roccia 2,Il mago di oz comando output descrizione wc wc-01.csv 3 9 50 wc-01.csv numero di righe, parole e byte wc -l wc-01.csv 3 wc-01.csv numero di righe wc -w wc-01.csv 9 wc-01.csv numero di parole (separate da spazio) wc -c wc-01.csv 50 wc-01.csv numero di byte wc --max-line-length wc-01.csv 23 wc-01.csv il numero di caratteri della riga pi\u00f9 lunga Tip Se si vuole soltanto il numero di righe , senza il nome del file , il comando \u00e8 <wc-01.csv wc -l wc non \u00e8 per\u00f2 un comando che interpreta il tipo di formato , e quindi in presenza di un file di input come questo sottostante - un CSV di due record (pi\u00f9 intestazione) e due colonne - il conteggio delle righe sar\u00e0 pari a 8. 1 2 3 4 5 6 7 8 FieldA,FieldB \"Come Quando Fuori Piove\",Ciao Miao,\"Uno Due Tre\" Questo file - vedi anteprima sottostante - contiene dei ritorni a capo nelle celle, e per ognuno viene conteggiata una riga. FieldA FieldB Come Quando Fuori Piove Ciao Miao Uno Due Tre Attenzione Per poter conteggiare il numero di record di un CSV come questo, \u00e8 necessario usare una utility che tenga conto delle caratteristiche del formato ( CSV aware ), come ad esempio il fatto che in una cella ci possono essere pi\u00f9 righe.","title":"wc"},{"location":"dati/esplora/#miller","text":"Miller \u00e8 una straordinaria utility per leggere, analizzare e trasformare file di testo strutturati . \u00c8 capace di interpretare le caratteristiche dei formati che supporta ( CSV , TSV , JSON , ecc.) e quindi ad esempio contare correttamente il numero di record di un file CSV , anche in presenza di celle con ritorni a capo (vedi esempio soprastante). mlr --csv tail -n 1 then put '$righe=NR' then cut -f righe wc-02.csv righe 2 Alcune note: --csv per impostare formato di input e output ; tail -n 1 per estrarre l'ultima riga; put '$righe=NR' per creare il campo righe e valorizzarlo con il numero di riga della riga corrente - NR - che qui \u00e8 l'ultima; cut -f righe , per avere in output soltanto il campo precedentemente creato. Se si vuole in output anche il numero di colonne , il comando si pu\u00f2 modificare in questo modo: mlr --csv tail -n 1 then put '$righe=NR;$colonne=NF-1' then cut -f righe,colonne wc-02.csv Con $colonne=NF-1 \u00e8 stato aggiunto un campo che conta il numero di campi - NF - presenti nella riga corrente, a cui \u00e8 stato sottratto 1, perch\u00e9 \u00e8 stata aggiunta la colonna che d\u00e0 conto del numero di righe, che non fa parte delle colonne pre esistenti nel file di input . In output : righe,colonne 2,2 Un esempio pi\u00f9 eclatante \u00e8 quando il file di input \u00e8 un JSON come quello sottostante, composto da 2 \"record\" (qui \u00e8 improprio come termine, ma per dare l'idea), ognuno con due campi. 1 2 3 4 5 6 7 8 9 10 [ { \"id\" : 1 , \"titolo\" : \"La spada nella roccia\" }, { \"id\" : 2 , \"titolo\" : \"Il mago di oz\" } ] Il comando mlr --j2c tail -n 1 then put '$righe=NR;$colonne=NF-1' then cut -f righe,colonne input.json restituir\u00e0 sempre ( --j2c \u00e8 per trasformare l'input in JSON in CSV ) righe,colonne 2,2 Info A Miller \u00e8 dedicata una monografia di questo sito.","title":"Miller"},{"location":"dati/trasforma/","text":"Trasforma \u00b6 Stuttura \u00b6 Da wide a long \u00b6 Da long a wide \u00b6 Pivot \u00b6 Encoding \u00b6 Tante volte \u00e8 necessario trasformare i file da una codifica di caratteri a un'altra. Un caso classico \u00e8 quello di un lavoro in cui la gran parte dei dati \u00e8 in UTF-8 , ma alcuni file \"esterni\" sono in Windows-1252 e sono da uniformare ai precedenti. \u00c8 disponibile l' utility iconv , che si occupa proprio di questo. Ad esempio per trasformare un file da Windows-1252 a UTF-8 , il comando \u00e8: iconv -f Windows-1252 -t UTF-8 input >output","title":"Trasforma"},{"location":"dati/trasforma/#trasforma","text":"","title":"Trasforma"},{"location":"dati/trasforma/#stuttura","text":"","title":"Stuttura"},{"location":"dati/trasforma/#da-wide-a-long","text":"","title":"Da wide a long"},{"location":"dati/trasforma/#da-long-a-wide","text":"","title":"Da long a wide"},{"location":"dati/trasforma/#pivot","text":"","title":"Pivot"},{"location":"dati/trasforma/#encoding","text":"Tante volte \u00e8 necessario trasformare i file da una codifica di caratteri a un'altra. Un caso classico \u00e8 quello di un lavoro in cui la gran parte dei dati \u00e8 in UTF-8 , ma alcuni file \"esterni\" sono in Windows-1252 e sono da uniformare ai precedenti. \u00c8 disponibile l' utility iconv , che si occupa proprio di questo. Ad esempio per trasformare un file da Windows-1252 a UTF-8 , il comando \u00e8: iconv -f Windows-1252 -t UTF-8 input >output","title":"Encoding"},{"location":"maipiusenza/","text":"ciao mondo \u00b6","title":"ciao mondo"},{"location":"maipiusenza/#ciao-mondo","text":"","title":"ciao mondo"},{"location":"miller/","text":"Miller \u00b6 Il suo eccezionale autore - John Kerl - definisce Miller come awk , sed , cut , join e sort per file di testo strutturati come CSV , TSV e JSON . Ha una ottima documentazione ufficiale in inglese, consultabile qui . Installazione \u00b6 Miller si pu\u00f2 installare in diversi modi: Miller 5, la versione stabile attuale Linux: yum install miller o apt-get install miller in dipendenza della versione di Linux; MacOS: brew update e brew install miller , o sudo port selfupdate e sudo port install miller , in dipendenza delle preference di Homebrew o MacPorts . Windows: choco install miller utilizzando Chocolatey . Miller 6, la versione scritta in GO, ancora in pre-release scaricando gli eseguibili compilati per Linux, MacOS e Windows visitando https://github.com/johnkerl/miller/actions , selezionando l'ultima build , e facendo click su Artifacts . Primi passi \u00b6 Miller fa spesso riferimento nei suoi sub comandi al toolkit di UNIX e ai suoi esegubili come cat , tail , cut , sort , etc.. Un esempio per iniziare: stampare a schermo il contenuto di un file : comando output mlr --csv cat base.csv nome,dataNascita,altezza,peso andy,1973-05-08,176,86.5 chiara,1993-12-13,162,58.3 guido,2001-01-22,196,90.4 Nel comando di sopra cat \u00e8 uno dei verbi di Miller. Esistono altri tipi di sub comandi, che invece replicano alcune delle caratteristiche di awk , come filter e put . I sub comandi di Miller si chiamano verbi formati ; verbi ; script ( DSL )","title":"Miller"},{"location":"miller/#miller","text":"Il suo eccezionale autore - John Kerl - definisce Miller come awk , sed , cut , join e sort per file di testo strutturati come CSV , TSV e JSON . Ha una ottima documentazione ufficiale in inglese, consultabile qui .","title":"Miller"},{"location":"miller/#installazione","text":"Miller si pu\u00f2 installare in diversi modi: Miller 5, la versione stabile attuale Linux: yum install miller o apt-get install miller in dipendenza della versione di Linux; MacOS: brew update e brew install miller , o sudo port selfupdate e sudo port install miller , in dipendenza delle preference di Homebrew o MacPorts . Windows: choco install miller utilizzando Chocolatey . Miller 6, la versione scritta in GO, ancora in pre-release scaricando gli eseguibili compilati per Linux, MacOS e Windows visitando https://github.com/johnkerl/miller/actions , selezionando l'ultima build , e facendo click su Artifacts .","title":"Installazione"},{"location":"miller/#primi-passi","text":"Miller fa spesso riferimento nei suoi sub comandi al toolkit di UNIX e ai suoi esegubili come cat , tail , cut , sort , etc.. Un esempio per iniziare: stampare a schermo il contenuto di un file : comando output mlr --csv cat base.csv nome,dataNascita,altezza,peso andy,1973-05-08,176,86.5 chiara,1993-12-13,162,58.3 guido,2001-01-22,196,90.4 Nel comando di sopra cat \u00e8 uno dei verbi di Miller. Esistono altri tipi di sub comandi, che invece replicano alcune delle caratteristiche di awk , come filter e put . I sub comandi di Miller si chiamano verbi formati ; verbi ; script ( DSL )","title":"Primi passi"},{"location":"miller/contributi/","text":"Contributi \u00b6 Da quando abbiamo iniziato a usare Miller alcuni anni fa, abbiamo aperto diverse issue nel repository ufficiale, con idee, domande, proposte e segnalazioni di bug. Alcune sono diventate nuove caratteristiche di Miller e ci fa un particolare piacere tenerne nota. Tutto questo \u00e8 possibile grazie al suo straordinario autore, John Kerl, che cura in modo esemplare il rapporto con la comunit\u00e0 che utilizza questa applicazione. 2022-02-08 | Aggiunto il natural sorting \u00b6 \u00c8 possibile usare il natural sorting 1 , sia per il verbo sort , che per la funzione DSL sort . Grazie a Salvatore Fiandaca per l'ispirazione . Vedi #874 e #872 . 2022-02-07 | Espansa sintassi di strptime \u00b6 Prima non era possibile utilizzare %j . Vedi #914 . 2022-02-06 | Supporto pi\u00f9 rigoroso al formato TSV \u00b6 Prima era gestito come uno speciale CSV , in realt\u00e0 sono formati differenti. Vedi #922 e #923 . 2022-01-09 | Supporto al formato JSON Lines \u00b6 Prima non era supportato. Vedi #755 e #844 . 2022-01-09 | JSON di output corretto \u00b6 Prima non era supportato di default . Vedi #755 e #844 . 2021-10-10 | Gestione corretta del carattere pipe nella creazione di Markdown \u00b6 Prima un carettere | in una cella di input, faceva produrre Markdown di output scorretti. Vedi #610 . https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/ \u21a9","title":"Contributi"},{"location":"miller/contributi/#contributi","text":"Da quando abbiamo iniziato a usare Miller alcuni anni fa, abbiamo aperto diverse issue nel repository ufficiale, con idee, domande, proposte e segnalazioni di bug. Alcune sono diventate nuove caratteristiche di Miller e ci fa un particolare piacere tenerne nota. Tutto questo \u00e8 possibile grazie al suo straordinario autore, John Kerl, che cura in modo esemplare il rapporto con la comunit\u00e0 che utilizza questa applicazione.","title":"Contributi"},{"location":"miller/contributi/#2022-02-08-aggiunto-il-natural-sorting","text":"\u00c8 possibile usare il natural sorting 1 , sia per il verbo sort , che per la funzione DSL sort . Grazie a Salvatore Fiandaca per l'ispirazione . Vedi #874 e #872 .","title":"2022-02-08 | Aggiunto il natural sorting"},{"location":"miller/contributi/#2022-02-07-espansa-sintassi-di-strptime","text":"Prima non era possibile utilizzare %j . Vedi #914 .","title":"2022-02-07 | Espansa sintassi di strptime"},{"location":"miller/contributi/#2022-02-06-supporto-piu-rigoroso-al-formato-tsv","text":"Prima era gestito come uno speciale CSV , in realt\u00e0 sono formati differenti. Vedi #922 e #923 .","title":"2022-02-06 | Supporto pi\u00f9 rigoroso al formato TSV"},{"location":"miller/contributi/#2022-01-09-supporto-al-formato-json-lines","text":"Prima non era supportato. Vedi #755 e #844 .","title":"2022-01-09 | Supporto al formato JSON Lines"},{"location":"miller/contributi/#2022-01-09-json-di-output-corretto","text":"Prima non era supportato di default . Vedi #755 e #844 .","title":"2022-01-09 | JSON di output corretto"},{"location":"miller/contributi/#2021-10-10-gestione-corretta-del-carattere-pipe-nella-creazione-di-markdown","text":"Prima un carettere | in una cella di input, faceva produrre Markdown di output scorretti. Vedi #610 . https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/ \u21a9","title":"2021-10-10 | Gestione corretta del carattere pipe nella creazione di Markdown"},{"location":"miller/dsl/","text":"","title":"DSL"},{"location":"miller/eterogeneita_record/","text":"Eterogeneit\u00e0 dei record \u00b6 Per Miller il modo nativo di \"pensare\" ai dati \u00e8 per record eterogenei : ogni record non deve necessariamente avere lo stesso numero di campi degli altri. I record non hanno lo stesso numero di campi nome=andy,dataNascita=1973-05-08,altezza=176,peso=86.5,comuneNascita=Roma nome=chiara,dataNascita=1993-12-13,altezza=162,peso=58.3,comuneNascita=Milano nome=guido,altezza=196,peso=90.4,comuneNascita=Roma nome=sara,dataNascita=2000-02-22,altezza=166,peso=70.4,comuneNascita=Roma nome=giulia,dataNascita=1997-08-13,altezza=169,peso=68.3 Terminologia \u00b6 Esistono tre tipi di eterogeneit\u00e0: ragged , irregular e sparse . Dati rettangolari \u00b6 Qui un esempio di dati \"rettangolari\", in formato CSV : 3 i campi previsti, e ogni record ha valorizzato 3 campi. eterogenita.csv a,b,c 1,2,3 4,5,6 7,8,9 Dati rettangolari, ma con celle vuote \u00b6 Questo \u00e8 un file che non ha record eterogenei, ma ha alcune celle vuote (per il campo b a riga 2 e per il campo a a riga 3). eterogeneita_vuoti.csv a,b,c 1,2,3 4,,6 ,8,9 In Miller \u00e8 possibile usare il verbo fill-empty , per attribuirgli un valore (ad esempio NA ). La gestione dei valori sconosciuti o nulli \u00e8 tra la buone pratiche della pubblicazione di dati in formato CSV. mlr --icsv --opprint fill-empty -v NA ./eterogeneita_vuoti.csv a b c 1 2 3 4 NA 6 NA 8 9 Il verbo fill-empty richiede Miller >= 6.0 Ragged \u00b6 Questo \u00e8 il caso di eterogeneit\u00e0, dovuta a errori di struttura del file. Come questo di sotto. eterogeneita_ragged.csv a,b,c 1,2,3 4,5 7,8,9,10 Se si prova semplicemente a stamparlo a schermo, Miller restituisce un errore: mlr --csv cat ./eterogeneita_ragged.csv a,b,c 1,2,3 mlr : mlr: CSV header/data length mismatch 3 != 2 at filename eterogeneita_ragged.csv row 3. Questi gli errori del file, che ha un'intestazione composta da 3 campi: nella riga 2, ci sono 2 campi e non 3; nella riga 4, ci sono 4 campi e non 3. Miller \u00e8 in grado di gestire anche questa eterogeneit\u00e0, che deriva da errori, utilizzando il flag --allow-ragged-csv-input : mlr --csv --allow-ragged-csv-input ./eterogeneita_ragged.csv a,b,c,4 1,2,3, 4,5,, 7,8,9,10 Per il primo errore \u00e8 stata aggiunta una cella vuota; per il secondo \u00e8 stato aggiunto un campo a cui \u00e8 stato assegnato un'etichetta numerica - 4 - corrispondente al suo ordine nella lista dei campi. Irregular \u00b6 Un altro tipo di eterogeneit\u00e0 \u00e8 legata a campi ordinati diversamente per ogni riga: eterogeneita_irregular.json {\"a\": 1, \"b\": 2, \"c\": 3} {\"c\": 6, \"a\": 4, \"b\": 5} {\"b\": 8, \"c\": 9, \"a\": 7} Se fosse necessario uniformare l'ordine si possono usare i verbi regularize o sort-within-records . Il verbo regularize riordina le righe nello stesso ordine della prima (qualunque sia l'ordine); il verbo sort-in-records usa semplicemente l'ordine alfabetico. Sparse \u00b6 In ultimo c'\u00e8 l'eterogeneit\u00e0 pi\u00f9 frequente, legata a record che non sono composti tutti dagli stessi campi. Come ad esempio il JSON sottostante. eterogeneita_sparse.json { \"host\": \"xy01.east\", \"status\": \"running\", \"volume\": \"/dev/sda1\" } { \"host\": \"xy92.west\", \"status\": \"running\" } { \"purpose\": \"failover\", \"host\": \"xy55.east\", \"volume\": \"/dev/sda1\", \"reimaged\": true } Si pu\u00f2 utilizzare il verbo unsparsify per fare in modo che tutti i record abbiano gli stessi campi. mlr --json unsparsify ./eterogeneita_sparse.json { \"host\": \"xy01.east\", \"status\": \"running\", \"volume\": \"/dev/sda1\", \"purpose\": \"\", \"reimaged\": \"\" } { \"host\": \"xy92.west\", \"status\": \"running\", \"volume\": \"\", \"purpose\": \"\", \"reimaged\": \"\" } { \"host\": \"xy55.east\", \"status\": \"\", \"volume\": \"/dev/sda1\", \"purpose\": \"failover\", \"reimaged\": true }","title":"Eterogeneit\u00e0 record"},{"location":"miller/eterogeneita_record/#eterogeneita-dei-record","text":"Per Miller il modo nativo di \"pensare\" ai dati \u00e8 per record eterogenei : ogni record non deve necessariamente avere lo stesso numero di campi degli altri. I record non hanno lo stesso numero di campi nome=andy,dataNascita=1973-05-08,altezza=176,peso=86.5,comuneNascita=Roma nome=chiara,dataNascita=1993-12-13,altezza=162,peso=58.3,comuneNascita=Milano nome=guido,altezza=196,peso=90.4,comuneNascita=Roma nome=sara,dataNascita=2000-02-22,altezza=166,peso=70.4,comuneNascita=Roma nome=giulia,dataNascita=1997-08-13,altezza=169,peso=68.3","title":"Eterogeneit\u00e0 dei record"},{"location":"miller/eterogeneita_record/#terminologia","text":"Esistono tre tipi di eterogeneit\u00e0: ragged , irregular e sparse .","title":"Terminologia"},{"location":"miller/eterogeneita_record/#dati-rettangolari","text":"Qui un esempio di dati \"rettangolari\", in formato CSV : 3 i campi previsti, e ogni record ha valorizzato 3 campi. eterogenita.csv a,b,c 1,2,3 4,5,6 7,8,9","title":"Dati rettangolari"},{"location":"miller/eterogeneita_record/#dati-rettangolari-ma-con-celle-vuote","text":"Questo \u00e8 un file che non ha record eterogenei, ma ha alcune celle vuote (per il campo b a riga 2 e per il campo a a riga 3). eterogeneita_vuoti.csv a,b,c 1,2,3 4,,6 ,8,9 In Miller \u00e8 possibile usare il verbo fill-empty , per attribuirgli un valore (ad esempio NA ). La gestione dei valori sconosciuti o nulli \u00e8 tra la buone pratiche della pubblicazione di dati in formato CSV. mlr --icsv --opprint fill-empty -v NA ./eterogeneita_vuoti.csv a b c 1 2 3 4 NA 6 NA 8 9 Il verbo fill-empty richiede Miller >= 6.0","title":"Dati rettangolari, ma con celle vuote"},{"location":"miller/eterogeneita_record/#ragged","text":"Questo \u00e8 il caso di eterogeneit\u00e0, dovuta a errori di struttura del file. Come questo di sotto. eterogeneita_ragged.csv a,b,c 1,2,3 4,5 7,8,9,10 Se si prova semplicemente a stamparlo a schermo, Miller restituisce un errore: mlr --csv cat ./eterogeneita_ragged.csv a,b,c 1,2,3 mlr : mlr: CSV header/data length mismatch 3 != 2 at filename eterogeneita_ragged.csv row 3. Questi gli errori del file, che ha un'intestazione composta da 3 campi: nella riga 2, ci sono 2 campi e non 3; nella riga 4, ci sono 4 campi e non 3. Miller \u00e8 in grado di gestire anche questa eterogeneit\u00e0, che deriva da errori, utilizzando il flag --allow-ragged-csv-input : mlr --csv --allow-ragged-csv-input ./eterogeneita_ragged.csv a,b,c,4 1,2,3, 4,5,, 7,8,9,10 Per il primo errore \u00e8 stata aggiunta una cella vuota; per il secondo \u00e8 stato aggiunto un campo a cui \u00e8 stato assegnato un'etichetta numerica - 4 - corrispondente al suo ordine nella lista dei campi.","title":"Ragged"},{"location":"miller/eterogeneita_record/#irregular","text":"Un altro tipo di eterogeneit\u00e0 \u00e8 legata a campi ordinati diversamente per ogni riga: eterogeneita_irregular.json {\"a\": 1, \"b\": 2, \"c\": 3} {\"c\": 6, \"a\": 4, \"b\": 5} {\"b\": 8, \"c\": 9, \"a\": 7} Se fosse necessario uniformare l'ordine si possono usare i verbi regularize o sort-within-records . Il verbo regularize riordina le righe nello stesso ordine della prima (qualunque sia l'ordine); il verbo sort-in-records usa semplicemente l'ordine alfabetico.","title":"Irregular"},{"location":"miller/eterogeneita_record/#sparse","text":"In ultimo c'\u00e8 l'eterogeneit\u00e0 pi\u00f9 frequente, legata a record che non sono composti tutti dagli stessi campi. Come ad esempio il JSON sottostante. eterogeneita_sparse.json { \"host\": \"xy01.east\", \"status\": \"running\", \"volume\": \"/dev/sda1\" } { \"host\": \"xy92.west\", \"status\": \"running\" } { \"purpose\": \"failover\", \"host\": \"xy55.east\", \"volume\": \"/dev/sda1\", \"reimaged\": true } Si pu\u00f2 utilizzare il verbo unsparsify per fare in modo che tutti i record abbiano gli stessi campi. mlr --json unsparsify ./eterogeneita_sparse.json { \"host\": \"xy01.east\", \"status\": \"running\", \"volume\": \"/dev/sda1\", \"purpose\": \"\", \"reimaged\": \"\" } { \"host\": \"xy92.west\", \"status\": \"running\", \"volume\": \"\", \"purpose\": \"\", \"reimaged\": \"\" } { \"host\": \"xy55.east\", \"status\": \"\", \"volume\": \"/dev/sda1\", \"purpose\": \"failover\", \"reimaged\": true }","title":"Sparse"},{"location":"miller/flag/","text":"Flag \u00b6 A seguire sono riportati i flag che si possono utilizzare in Miller. Ad esempio, nel comando mlr --icsv --ojson head -n 1 ./base_category.csv { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" } --icsv e --ojson sono i flag con cui impostare il formato di input e output . CSV \u00b6 Si tratta di flag applicabili al formato CSV . Lista: --allow-ragged-csv-input o --ragged : Se una riga dati ha meno campi della riga di intestazione, viene riempire con i campi rimanenti, vuoti. Se una riga di dati ha pi\u00f9 campi della riga di intestazione, Miller utilizza dei nomi di campo numerici interi, a partire da 1; --implicit-csv-header , associa ai campi un nome implicito, un numero intero che parte da 1 e viene incrementato di 1, dal primo campo a sinistra all'ultimo a destra; --headerless-csv-output , rimuove dall'output la riga di intestazione; -N : una scorciatoia per mettere insieme la coppia --implicit-csv-header --headerless-csv-output (utile in casi come questo ); JSON \u00b6 Si tratta di flag applicabili al formato CSV . Lista: --jlistwrap , include l'output JSON (che di default \u00e8 un JSON lines), tra [] ; --no-jvstack , dispone gli oggetti/array di output su una riga; Formati file \u00b6 Questi sono i flag legati ai formati. Lista: --asv or --asvlite : Use ASV format for input and output data. --csv or -c : il formato CSV come input e output. --csvlite : Use CSV-lite format for input and output data. --dkvp : Use DKVP format for input and output data. --gen-field-name : Specify field name for --igen. Defaults to \"i\". --gen-start : Specify start value for --igen. Defaults to 1. --gen-step : Specify step value for --igen. Defaults to 1. --gen-stop : Specify stop value for --igen. Defaults to 100. --iasv or --iasvlite : Use ASV format for input data. --icsv : il formato CSV come input. --icsvlite : Use CSV-lite format for input data. --idkvp : Use DKVP format for input data. --igen : Ignore input files and instead generate sequential numeric input using --gen-field-name, --gen-start, --gen-step, and --gen-stop values. See also the seqgen verb, which is more useful/intuitive. --ijson : il formato JSON come input. --ijsonl : Use JSON Lines format for input data. --inidx : Use NIDX format for input data. --io {format name} : Use format name for input and output data. For example: --io csv is the same as --csv . --ipprint : Use PPRINT format for input data. --itsv : il formato TSV come input. --itsvlite : Use TSV-lite format for input data. --iusv or --iusvlite : Use USV format for input data. --ixtab : Use XTAB format for input data. --json or -j : il formato JSON come input and output. --jsonl : Use JSON Lines format for input and output data. --nidx : Use NIDX format for input and output data. --oasv or --oasvlite : Use ASV format for output data. --ocsv : il formato CSV come output. --ocsvlite : Use CSV-lite format for output data. --odkvp : Use DKVP format for output data. --ojson : il formato JSON come output. --ojsonl : Use JSON Lines format for output data. --omd : Use markdown-tabular format for output data. --onidx : Use NIDX format for output data. --opprint : Use PPRINT format for output data. --otsv : il formato TSV come output. --otsvlite : Use TSV-lite format for output data. --ousv or --ousvlite : Use USV format for output data. --oxtab : Use XTAB format for output data. --pprint : Use PPRINT format for input and output data. --tsv : il formato TSV come input e output. --tsvlite or -t : Use TSV-lite format for input and output data. --usv or --usvlite : Use USV format for input and output data. --xtab : Use XTAB format for input and output data. -i {format name} : Use format name for input data. For example: -i csv is the same as --icsv . -o {format name} : Use format name for output data. For example: -o csv is the same as --ocsv .","title":"Flag"},{"location":"miller/flag/#flag","text":"A seguire sono riportati i flag che si possono utilizzare in Miller. Ad esempio, nel comando mlr --icsv --ojson head -n 1 ./base_category.csv { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" } --icsv e --ojson sono i flag con cui impostare il formato di input e output .","title":"Flag"},{"location":"miller/flag/#csv","text":"Si tratta di flag applicabili al formato CSV . Lista: --allow-ragged-csv-input o --ragged : Se una riga dati ha meno campi della riga di intestazione, viene riempire con i campi rimanenti, vuoti. Se una riga di dati ha pi\u00f9 campi della riga di intestazione, Miller utilizza dei nomi di campo numerici interi, a partire da 1; --implicit-csv-header , associa ai campi un nome implicito, un numero intero che parte da 1 e viene incrementato di 1, dal primo campo a sinistra all'ultimo a destra; --headerless-csv-output , rimuove dall'output la riga di intestazione; -N : una scorciatoia per mettere insieme la coppia --implicit-csv-header --headerless-csv-output (utile in casi come questo );","title":"CSV"},{"location":"miller/flag/#json","text":"Si tratta di flag applicabili al formato CSV . Lista: --jlistwrap , include l'output JSON (che di default \u00e8 un JSON lines), tra [] ; --no-jvstack , dispone gli oggetti/array di output su una riga;","title":"JSON"},{"location":"miller/flag/#formati-file","text":"Questi sono i flag legati ai formati. Lista: --asv or --asvlite : Use ASV format for input and output data. --csv or -c : il formato CSV come input e output. --csvlite : Use CSV-lite format for input and output data. --dkvp : Use DKVP format for input and output data. --gen-field-name : Specify field name for --igen. Defaults to \"i\". --gen-start : Specify start value for --igen. Defaults to 1. --gen-step : Specify step value for --igen. Defaults to 1. --gen-stop : Specify stop value for --igen. Defaults to 100. --iasv or --iasvlite : Use ASV format for input data. --icsv : il formato CSV come input. --icsvlite : Use CSV-lite format for input data. --idkvp : Use DKVP format for input data. --igen : Ignore input files and instead generate sequential numeric input using --gen-field-name, --gen-start, --gen-step, and --gen-stop values. See also the seqgen verb, which is more useful/intuitive. --ijson : il formato JSON come input. --ijsonl : Use JSON Lines format for input data. --inidx : Use NIDX format for input data. --io {format name} : Use format name for input and output data. For example: --io csv is the same as --csv . --ipprint : Use PPRINT format for input data. --itsv : il formato TSV come input. --itsvlite : Use TSV-lite format for input data. --iusv or --iusvlite : Use USV format for input data. --ixtab : Use XTAB format for input data. --json or -j : il formato JSON come input and output. --jsonl : Use JSON Lines format for input and output data. --nidx : Use NIDX format for input and output data. --oasv or --oasvlite : Use ASV format for output data. --ocsv : il formato CSV come output. --ocsvlite : Use CSV-lite format for output data. --odkvp : Use DKVP format for output data. --ojson : il formato JSON come output. --ojsonl : Use JSON Lines format for output data. --omd : Use markdown-tabular format for output data. --onidx : Use NIDX format for output data. --opprint : Use PPRINT format for output data. --otsv : il formato TSV come output. --otsvlite : Use TSV-lite format for output data. --ousv or --ousvlite : Use USV format for output data. --oxtab : Use XTAB format for output data. --pprint : Use PPRINT format for input and output data. --tsv : il formato TSV come input e output. --tsvlite or -t : Use TSV-lite format for input and output data. --usv or --usvlite : Use USV format for input and output data. --xtab : Use XTAB format for input and output data. -i {format name} : Use format name for input data. For example: -i csv is the same as --icsv . -o {format name} : Use format name for output data. For example: -o csv is the same as --ocsv .","title":"Formati file"},{"location":"miller/formati/","text":"Gestione formati \u00b6 Il formato nativo di Miller \u00e8 il DKVP (\" Delimited Key-Value Pairs \"), ovvero delle coppie chiave-valore, separate da virgola (la , \u00e8 il separatore di default). A seguire un esempio. Esempio del formato nativo. Notare che i record non hanno lo stesso numero di campi nome=andy,dataNascita=1973-05-08,altezza=176,peso=86.5,comuneNascita=Roma nome=chiara,dataNascita=1993-12-13,altezza=162,peso=58.3,comuneNascita=Milano nome=guido,altezza=196,peso=90.4,comuneNascita=Roma nome=sara,dataNascita=2000-02-22,altezza=166,peso=70.4,comuneNascita=Roma nome=giulia,dataNascita=1997-08-13,altezza=169,peso=68.3 Spesso si pensa ai dati come \"rettangolari\": se sono previsti 10 campi, ogni record sar\u00e0 composto da 10 valori. Ma non \u00e8 sempre cos\u00ec, come nel caso del formato JSON o appunto di quello nativo di Miller, in cui ogni record non ha necessariamente lo stesso numero di campi degli altri. Miller di default gestisce quindi l' eterogeneit\u00e0 dei record . Conversione di formato \u00b6 Miller legge e scrive diversi formati di testo strutturato . Per impostare quello di input e di output \u00e8 necessario utilizzare uno dei flag dedicati e una delle modalit\u00e0 per farlo. Ad esempio per convertire un file da CSV a TSV , si pu\u00f2 usare questo comando: mlr --icsv --otsv cat input.csv>output.csv Nel dettaglio: --icsv per impostare il formato di i nput ; --ocsv per impostare il formato di o utput ; cat \u00e8 uno dei verbi di Miller, quello di base, che passa i dati senza alcuna trasformazione dall' input all' output . Nota bene In un comando Miller, \u00e8 sempre necessario inserire almeno uno dei suoi verbi. Qui \u00e8 cat . C'\u00e8 anche la versione \"breve\" dello stesso comando, in cui --icsv --otsv , diventa --c2t (ovvero da CSV a TSV , che in inglese \u00e8 \" CSV TO TSV \"): mlr --c2t cat input.csv>output.csv Qui a seguire i flag di base, per passare da uno dei possibili formati di input a uno di quelli di output . IN/OUT CSV TSV JSON DKVP NIDX XTAB PPRINT Markdown CSV --c2t --c2j --c2d --c2n --c2x --c2p --c2m TSV --t2c --t2j --t2d --t2n --t2x --t2p --t2m JSON --j2c --j2t --j2d --j2n --j2x --j2p --j2m DKVP --d2c --d2t --d2j --d2n --d2x --d2p --d2m NIDX --n2c --n2t --n2j --n2d --n2x --n2p --n2m XTAB --x2c --x2t --x2j --x2d --x2n --x2p --x2m PPRINT --p2c --p2t --p2j --p2d --p2n --p2x --p2m Elenco formati \u00b6 Il file di riferimento di input , usato per produrre i vari formati di output \u00e8 base_category.csv . Per ognuno di questi, \u00e8 stato inserito il comando per generarlo a partire dal CSV di input . CSV \u00b6 mlr --csv cat base_category.csv andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano DKVP (il formato nativo) \u00b6 mlr --c2d cat base_category.csv nome=andy,dataNascita=1973-05-08,altezza=176,peso=86.5,comuneNascita=Roma nome=chiara,dataNascita=1993-12-13,altezza=162,peso=58.3,comuneNascita=Milano nome=guido,dataNascita=2001-01-22,altezza=196,peso=90.4,comuneNascita=Roma nome=sara,dataNascita=2000-02-22,altezza=166,peso=70.4,comuneNascita=Roma nome=giulia,dataNascita=1997-08-13,altezza=169,peso=68.3,comuneNascita=Milano A seguire, lo stesso input in altri dei formati supportati da Miller. TSV \u00b6 mlr --c2t cat base_category.csv nome dataNascita altezza peso comuneNascita andy 1973-05-08 176 86.5 Roma chiara 1993-12-13 162 58.3 Milano guido 2001-01-22 196 90.4 Roma sara 2000-02-22 166 70.4 Roma giulia 1997-08-13 169 68.3 Milano NIDX: Index-numbered \u00b6 mlr --c2n cat base_category.csv andy 1973-05-08 176 86.5 Roma chiara 1993-12-13 162 58.3 Milano guido 2001-01-22 196 90.4 Roma sara 2000-02-22 166 70.4 Roma giulia 1997-08-13 169 68.3 Milano JSON \u00b6 Warning Il JSON di output di default di Miller 5 non \u00e8 propriamente un JSON. In Miller 5 (versione attuale, che a breve sar\u00e0 superata dalla 6), l'output di default \u00e8 il JSON Lines . La scelta deriva dal fatto che \u00e8 un formato molto pi\u00f9 comodo per l'elaborazione con strumenti di parsing di testo e di versionamento; perch\u00e9 in questo formato ogni linea \u00e8 un JSON valido, e l'elaborazione per linea \u00e8 molto pi\u00f9 comoda e tipica per i client . \u00c8 quindi come sotto: mlr --c2j cat base_category.csv { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"altezza\" : 162 , \"peso\" : 58.3 , \"comuneNascita\" : \"Milano\" } { \"nome\" : \"guido\" , \"dataNascita\" : \"2001-01-22\" , \"altezza\" : 196 , \"peso\" : 90.4 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"giulia\" , \"dataNascita\" : \"1997-08-13\" , \"altezza\" : 169 , \"peso\" : 68.3 , \"comuneNascita\" : \"Milano\" } Se si vuole un \"vero\" JSON bisogna aggiungere il flag --jlistwrap : mlr --c2j --jlistwrap cat base_category.csv [ { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" } ,{ \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"altezza\" : 162 , \"peso\" : 58.3 , \"comuneNascita\" : \"Milano\" } ,{ \"nome\" : \"guido\" , \"dataNascita\" : \"2001-01-22\" , \"altezza\" : 196 , \"peso\" : 90.4 , \"comuneNascita\" : \"Roma\" } ,{ \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" } ,{ \"nome\" : \"giulia\" , \"dataNascita\" : \"1997-08-13\" , \"altezza\" : 169 , \"peso\" : 68.3 , \"comuneNascita\" : \"Milano\" } ] In Miller 6 (prossimo al rilascio) l' output di default \u00e8 invece un JSON : mlr --c2j cat base_category.csv [ { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"altezza\" : 162 , \"peso\" : 58.3 , \"comuneNascita\" : \"Milano\" }, { \"nome\" : \"guido\" , \"dataNascita\" : \"2001-01-22\" , \"altezza\" : 196 , \"peso\" : 90.4 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"giulia\" , \"dataNascita\" : \"1997-08-13\" , \"altezza\" : 169 , \"peso\" : 68.3 , \"comuneNascita\" : \"Milano\" } ] Se in Miller 6 si vuole un JSON Lines (formato molto consigliato), bisogna scegliere il flag --ojsonl : mlr --icsv --ojsonl cat base_category.csv { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"altezza\" : 162 , \"peso\" : 58.3 , \"comuneNascita\" : \"Milano\" } { \"nome\" : \"guido\" , \"dataNascita\" : \"2001-01-22\" , \"altezza\" : 196 , \"peso\" : 90.4 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"giulia\" , \"dataNascita\" : \"1997-08-13\" , \"altezza\" : 169 , \"peso\" : 68.3 , \"comuneNascita\" : \"Milano\" } PPRINT: Pretty-printed tabular \u00b6 mlr --c2p --barred cat base_category.csv +--------+-------------+---------+------+---------------+ | nome | dataNascita | altezza | peso | comuneNascita | +--------+-------------+---------+------+---------------+ | andy | 1973-05-08 | 176 | 86.5 | Roma | | chiara | 1993-12-13 | 162 | 58.3 | Milano | | guido | 2001-01-22 | 196 | 90.4 | Roma | | sara | 2000-02-22 | 166 | 70.4 | Roma | | giulia | 1997-08-13 | 169 | 68.3 | Milano | +--------+-------------+---------+------+---------------+ XTAB: Vertical tabular \u00b6 mlr --c2x cat base_category.csv nome andy dataNascita 1973-05-08 altezza 176 peso 86.5 comuneNascita Roma nome chiara dataNascita 1993-12-13 altezza 162 peso 58.3 comuneNascita Milano nome guido dataNascita 2001-01-22 altezza 196 peso 90.4 comuneNascita Roma nome sara dataNascita 2000-02-22 altezza 166 peso 70.4 comuneNascita Roma nome giulia dataNascita 1997-08-13 altezza 169 peso 68.3 comuneNascita Milano Markdown \u00b6 mlr --c2m cat base_category.csv | nome | dataNascita | altezza | peso | comuneNascita | | --- | --- | --- | --- | --- | | andy | 1973-05-08 | 176 | 86.5 | Roma | | chiara | 1993-12-13 | 162 | 58.3 | Milano | | guido | 2001-01-22 | 196 | 90.4 | Roma | | sara | 2000-02-22 | 166 | 70.4 | Roma | | giulia | 1997-08-13 | 169 | 68.3 | Milano | Casi speciali e consigli \u00b6 File CSV (anche TSV) senza riga di intestazione \u00b6 In questo file non \u00e8 presente la riga di intestazione. input.csv 1,861265,C,A,0.071 1,861265,C,A,0.148 1,861265,C,G,0.001 1,861265,C,G,0.108 1,861265,C,T,0 1,861265,C,T,0.216 2,193456,G,A,0.006 2,193456,G,A,0.094 2,193456,G,C,0.011 2,193456,G,C,0.152 2,193456,G,T,0.003 2,193456,G,T,0.056 Si pu\u00f2 fare riferimento ai campi in modo da numerico, con un progressivo numerico di una unit\u00e0 a partire da 1, da sinistra verso destra. Il flag utile al caso \u00e8 -N , che d\u00e0 per implicito che non ci sia la riga di intestazione (e ne assegna una temporanea con campi numerici) e che non venga aggiunta in output . Quindi se da questo file vorr\u00f2 estrarre le prime tre righe della prima colonna il comando sar\u00e0: mlr --csv -N cut -f 1 then head -n 3 input.csv 1 1 1","title":"Gestione formati"},{"location":"miller/formati/#gestione-formati","text":"Il formato nativo di Miller \u00e8 il DKVP (\" Delimited Key-Value Pairs \"), ovvero delle coppie chiave-valore, separate da virgola (la , \u00e8 il separatore di default). A seguire un esempio. Esempio del formato nativo. Notare che i record non hanno lo stesso numero di campi nome=andy,dataNascita=1973-05-08,altezza=176,peso=86.5,comuneNascita=Roma nome=chiara,dataNascita=1993-12-13,altezza=162,peso=58.3,comuneNascita=Milano nome=guido,altezza=196,peso=90.4,comuneNascita=Roma nome=sara,dataNascita=2000-02-22,altezza=166,peso=70.4,comuneNascita=Roma nome=giulia,dataNascita=1997-08-13,altezza=169,peso=68.3 Spesso si pensa ai dati come \"rettangolari\": se sono previsti 10 campi, ogni record sar\u00e0 composto da 10 valori. Ma non \u00e8 sempre cos\u00ec, come nel caso del formato JSON o appunto di quello nativo di Miller, in cui ogni record non ha necessariamente lo stesso numero di campi degli altri. Miller di default gestisce quindi l' eterogeneit\u00e0 dei record .","title":"Gestione formati"},{"location":"miller/formati/#conversione-di-formato","text":"Miller legge e scrive diversi formati di testo strutturato . Per impostare quello di input e di output \u00e8 necessario utilizzare uno dei flag dedicati e una delle modalit\u00e0 per farlo. Ad esempio per convertire un file da CSV a TSV , si pu\u00f2 usare questo comando: mlr --icsv --otsv cat input.csv>output.csv Nel dettaglio: --icsv per impostare il formato di i nput ; --ocsv per impostare il formato di o utput ; cat \u00e8 uno dei verbi di Miller, quello di base, che passa i dati senza alcuna trasformazione dall' input all' output . Nota bene In un comando Miller, \u00e8 sempre necessario inserire almeno uno dei suoi verbi. Qui \u00e8 cat . C'\u00e8 anche la versione \"breve\" dello stesso comando, in cui --icsv --otsv , diventa --c2t (ovvero da CSV a TSV , che in inglese \u00e8 \" CSV TO TSV \"): mlr --c2t cat input.csv>output.csv Qui a seguire i flag di base, per passare da uno dei possibili formati di input a uno di quelli di output . IN/OUT CSV TSV JSON DKVP NIDX XTAB PPRINT Markdown CSV --c2t --c2j --c2d --c2n --c2x --c2p --c2m TSV --t2c --t2j --t2d --t2n --t2x --t2p --t2m JSON --j2c --j2t --j2d --j2n --j2x --j2p --j2m DKVP --d2c --d2t --d2j --d2n --d2x --d2p --d2m NIDX --n2c --n2t --n2j --n2d --n2x --n2p --n2m XTAB --x2c --x2t --x2j --x2d --x2n --x2p --x2m PPRINT --p2c --p2t --p2j --p2d --p2n --p2x --p2m","title":"Conversione di formato"},{"location":"miller/formati/#elenco-formati","text":"Il file di riferimento di input , usato per produrre i vari formati di output \u00e8 base_category.csv . Per ognuno di questi, \u00e8 stato inserito il comando per generarlo a partire dal CSV di input .","title":"Elenco formati"},{"location":"miller/formati/#csv","text":"mlr --csv cat base_category.csv andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano","title":"CSV"},{"location":"miller/formati/#dkvp-il-formato-nativo","text":"mlr --c2d cat base_category.csv nome=andy,dataNascita=1973-05-08,altezza=176,peso=86.5,comuneNascita=Roma nome=chiara,dataNascita=1993-12-13,altezza=162,peso=58.3,comuneNascita=Milano nome=guido,dataNascita=2001-01-22,altezza=196,peso=90.4,comuneNascita=Roma nome=sara,dataNascita=2000-02-22,altezza=166,peso=70.4,comuneNascita=Roma nome=giulia,dataNascita=1997-08-13,altezza=169,peso=68.3,comuneNascita=Milano A seguire, lo stesso input in altri dei formati supportati da Miller.","title":"DKVP (il formato nativo)"},{"location":"miller/formati/#tsv","text":"mlr --c2t cat base_category.csv nome dataNascita altezza peso comuneNascita andy 1973-05-08 176 86.5 Roma chiara 1993-12-13 162 58.3 Milano guido 2001-01-22 196 90.4 Roma sara 2000-02-22 166 70.4 Roma giulia 1997-08-13 169 68.3 Milano","title":"TSV"},{"location":"miller/formati/#nidx-index-numbered","text":"mlr --c2n cat base_category.csv andy 1973-05-08 176 86.5 Roma chiara 1993-12-13 162 58.3 Milano guido 2001-01-22 196 90.4 Roma sara 2000-02-22 166 70.4 Roma giulia 1997-08-13 169 68.3 Milano","title":"NIDX: Index-numbered"},{"location":"miller/formati/#json","text":"Warning Il JSON di output di default di Miller 5 non \u00e8 propriamente un JSON. In Miller 5 (versione attuale, che a breve sar\u00e0 superata dalla 6), l'output di default \u00e8 il JSON Lines . La scelta deriva dal fatto che \u00e8 un formato molto pi\u00f9 comodo per l'elaborazione con strumenti di parsing di testo e di versionamento; perch\u00e9 in questo formato ogni linea \u00e8 un JSON valido, e l'elaborazione per linea \u00e8 molto pi\u00f9 comoda e tipica per i client . \u00c8 quindi come sotto: mlr --c2j cat base_category.csv { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"altezza\" : 162 , \"peso\" : 58.3 , \"comuneNascita\" : \"Milano\" } { \"nome\" : \"guido\" , \"dataNascita\" : \"2001-01-22\" , \"altezza\" : 196 , \"peso\" : 90.4 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"giulia\" , \"dataNascita\" : \"1997-08-13\" , \"altezza\" : 169 , \"peso\" : 68.3 , \"comuneNascita\" : \"Milano\" } Se si vuole un \"vero\" JSON bisogna aggiungere il flag --jlistwrap : mlr --c2j --jlistwrap cat base_category.csv [ { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" } ,{ \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"altezza\" : 162 , \"peso\" : 58.3 , \"comuneNascita\" : \"Milano\" } ,{ \"nome\" : \"guido\" , \"dataNascita\" : \"2001-01-22\" , \"altezza\" : 196 , \"peso\" : 90.4 , \"comuneNascita\" : \"Roma\" } ,{ \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" } ,{ \"nome\" : \"giulia\" , \"dataNascita\" : \"1997-08-13\" , \"altezza\" : 169 , \"peso\" : 68.3 , \"comuneNascita\" : \"Milano\" } ] In Miller 6 (prossimo al rilascio) l' output di default \u00e8 invece un JSON : mlr --c2j cat base_category.csv [ { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"altezza\" : 162 , \"peso\" : 58.3 , \"comuneNascita\" : \"Milano\" }, { \"nome\" : \"guido\" , \"dataNascita\" : \"2001-01-22\" , \"altezza\" : 196 , \"peso\" : 90.4 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"giulia\" , \"dataNascita\" : \"1997-08-13\" , \"altezza\" : 169 , \"peso\" : 68.3 , \"comuneNascita\" : \"Milano\" } ] Se in Miller 6 si vuole un JSON Lines (formato molto consigliato), bisogna scegliere il flag --ojsonl : mlr --icsv --ojsonl cat base_category.csv { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"altezza\" : 176 , \"peso\" : 86.5 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"altezza\" : 162 , \"peso\" : 58.3 , \"comuneNascita\" : \"Milano\" } { \"nome\" : \"guido\" , \"dataNascita\" : \"2001-01-22\" , \"altezza\" : 196 , \"peso\" : 90.4 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"giulia\" , \"dataNascita\" : \"1997-08-13\" , \"altezza\" : 169 , \"peso\" : 68.3 , \"comuneNascita\" : \"Milano\" }","title":"JSON"},{"location":"miller/formati/#pprint-pretty-printed-tabular","text":"mlr --c2p --barred cat base_category.csv +--------+-------------+---------+------+---------------+ | nome | dataNascita | altezza | peso | comuneNascita | +--------+-------------+---------+------+---------------+ | andy | 1973-05-08 | 176 | 86.5 | Roma | | chiara | 1993-12-13 | 162 | 58.3 | Milano | | guido | 2001-01-22 | 196 | 90.4 | Roma | | sara | 2000-02-22 | 166 | 70.4 | Roma | | giulia | 1997-08-13 | 169 | 68.3 | Milano | +--------+-------------+---------+------+---------------+","title":"PPRINT: Pretty-printed tabular"},{"location":"miller/formati/#xtab-vertical-tabular","text":"mlr --c2x cat base_category.csv nome andy dataNascita 1973-05-08 altezza 176 peso 86.5 comuneNascita Roma nome chiara dataNascita 1993-12-13 altezza 162 peso 58.3 comuneNascita Milano nome guido dataNascita 2001-01-22 altezza 196 peso 90.4 comuneNascita Roma nome sara dataNascita 2000-02-22 altezza 166 peso 70.4 comuneNascita Roma nome giulia dataNascita 1997-08-13 altezza 169 peso 68.3 comuneNascita Milano","title":"XTAB: Vertical tabular"},{"location":"miller/formati/#markdown","text":"mlr --c2m cat base_category.csv | nome | dataNascita | altezza | peso | comuneNascita | | --- | --- | --- | --- | --- | | andy | 1973-05-08 | 176 | 86.5 | Roma | | chiara | 1993-12-13 | 162 | 58.3 | Milano | | guido | 2001-01-22 | 196 | 90.4 | Roma | | sara | 2000-02-22 | 166 | 70.4 | Roma | | giulia | 1997-08-13 | 169 | 68.3 | Milano |","title":"Markdown"},{"location":"miller/formati/#casi-speciali-e-consigli","text":"","title":"Casi speciali e consigli"},{"location":"miller/formati/#file-csv-anche-tsv-senza-riga-di-intestazione","text":"In questo file non \u00e8 presente la riga di intestazione. input.csv 1,861265,C,A,0.071 1,861265,C,A,0.148 1,861265,C,G,0.001 1,861265,C,G,0.108 1,861265,C,T,0 1,861265,C,T,0.216 2,193456,G,A,0.006 2,193456,G,A,0.094 2,193456,G,C,0.011 2,193456,G,C,0.152 2,193456,G,T,0.003 2,193456,G,T,0.056 Si pu\u00f2 fare riferimento ai campi in modo da numerico, con un progressivo numerico di una unit\u00e0 a partire da 1, da sinistra verso destra. Il flag utile al caso \u00e8 -N , che d\u00e0 per implicito che non ci sia la riga di intestazione (e ne assegna una temporanea con campi numerici) e che non venga aggiunta in output . Quindi se da questo file vorr\u00f2 estrarre le prime tre righe della prima colonna il comando sar\u00e0: mlr --csv -N cut -f 1 then head -n 3 input.csv 1 1 1","title":"File CSV (anche TSV) senza riga di intestazione"},{"location":"miller/ricette/","text":"Ricette \u00b6 Concatenare in \"verticale\" pi\u00f9 file \u00b6 Il verbo \"tipico\" per concatenare due o pi\u00f9 file \u00e8 cat . Ad esempio se voglio unire in verticale questi due file CSV base.cv base_merge.csv nome,dataNascita,altezza,peso andy,1973-05-08,176,86.5 chiara,1993-12-13,162,58.3 guido,2001-01-22,196,90.4 nome,dataNascita,altezza,peso,coloreOcchi marco,1983-12-08,183,,verdi licia,1993-12-07,158,57.9,neri il comando da lanciare sar\u00e0 mlr --csv cat base.csv base_merge.csv >output.csv che dar\u00e0 in output +--------+-------------+---------+------+ | nome | dataNascita | altezza | peso | +--------+-------------+---------+------+ | andy | 1973-05-08 | 176 | 86.5 | | chiara | 1993-12-13 | 162 | 58.3 | | guido | 2001-01-22 | 196 | 90.4 | | marco | 1983-12-08 | 183 | - | | licia | 1993-12-07 | 158 | 57.9 | +--------+-------------+---------+------+ \u00c8 possibile fare il merge , l'unione in verticale, anche di due file con uno schema in parte diverso , perch\u00e9 Miller gestiste l' eterogeneit\u00e0 dei record . Se ad esempio si \u00e8 in presenza di un file che ha una colonna in pi\u00f9 ( coloreOcchi ) rispetto a base.csv , come questo ( base_altro.csv ) nome,dataNascita,altezza,peso,coloreOcchi marco,1983-12-08,183,,verdi licia,1993-12-07,158,57.9,neri il verbo da usare \u00e8 unsparsify . Il comando sar\u00e0: mlr --csv unsparsify base.csv base_altro.csv >output.csv In output, verr\u00e0 aggiunta la colonna coloreOcchi , che non sar\u00e0 valorizzata per i record del file che in partenza non aveva questa colonna: +--------+-------------+---------+------+-------------+ | nome | dataNascita | altezza | peso | coloreOcchi | +--------+-------------+---------+------+-------------+ | andy | 1973-05-08 | 176 | 86.5 | - | | chiara | 1993-12-13 | 162 | 58.3 | - | | guido | 2001-01-22 | 196 | 90.4 | - | | marco | 1983-12-08 | 183 | - | verdi | | licia | 1993-12-07 | 158 | 57.9 | neri | +--------+-------------+---------+------+-------------+ Suddividere un file di input in pi\u00f9 file di output, ogni xxx record \u00b6 mlr --csv put -q ' begin { @batch_size = 1000; } index = int(floor((NR-1) / @batch_size)); label = fmtnum(index,\"%04d\"); filename = \"part-\".label.\".json\"; tee > filename, $* ' ./input.csv Verr\u00e0 creato un file di output, con nome part-000XXX , ogni 1000 (si imposta tramite @batch_size ) record. Estrarre le righe che contengono il valore massimo di un campo \u00b6 Alcune delle righe sottostanti, sono identiche, fatta eccezione per il V campo. input.csv 1,861265,C,A,0.071 1,861265,C,A,0.148 1,861265,C,G,0.001 1,861265,C,G,0.108 1,861265,C,T,0 1,861265,C,T,0.216 2,193456,G,A,0.006 2,193456,G,A,0.094 2,193456,G,C,0.011 2,193456,G,C,0.152 2,193456,G,T,0.003 2,193456,G,T,0.056 Se si vogliono estrarre soltanto quelle con il valore massimo del V campo, raggruppate per i valori degli altri 4, il verbo da usare \u00e8 top mlr --csv -N top -f 5 -g 1,2,3,4 input.tsv 1,861265,C,A,1,0.148 1,861265,C,G,1,0.108 1,861265,C,T,1,0.216 2,193456,G,A,1,0.094 2,193456,G,C,1,0.152 2,193456,G,T,1,0.056 Vedi https://stackoverflow.com/a/70664880/757714 Eseguire un comando esterno all'interno di una funzione \u00b6 All'interno di un comando Miller \u00e8 possibile lanciare una utility esterna, usando la funzione system . Immaginiamo ad esempio di avere un file come questo input.txt a,b 1,\"15,1,2/AX,22,1/C,1/A,1/BA,2,3\" e di voler applicare il cosiddetto natural sorting alla stringa 15,1,2/AX,22,1/C,1/A,1/BA,2,3 , ottenendo questo ordinamento 1,1/A,1/BA,1/C,2,2/AX,3,15,22 . Utilizzando le utility standard della shell di Linux basterebbe fare cos\u00ec (in paste si usa - perch\u00e9 l'input \u00e8 l' stdin ): echo \"15,1,2/AX,22,1/C,1/A,1/BA,2,3\" | tr , \"\\n\" | sort -V | paste -sd, - Per riportare questa sintassi 1 in un comando Miller, il comando sarebbe come questo di sotto, in cui viene creato il campo toto , che raccoglie valori derivanti dal lancio di utility esterne, grazie alla funzione system . <input.txt mlr --c2p --barred cat then put -S '$toto=system(\"echo \".$b.\" | tr , \\\"\\n\\\" | sort -V | paste -sd, -\")' E l'output: +---+-------------------------------+-------------------------------+ | a | b | toto | +---+-------------------------------+-------------------------------+ | 1 | 15,1,2/AX,22,1/C,1/A,1/BA,2,3 | 1,1/A,1/BA,1/C,2,2/AX,3,15,22 | +---+-------------------------------+-------------------------------+ Nel comando bisogna avere cura di inserire eventuali escape a caratteri come \" . Fare un trova e sostituisci globale \u00b6 \u00c8 comodo utilizzare DSL , il linguaggio di scripting di Miller e usare un ciclo for : mlr --csv -S put 'for (k in $*) {$[k] = gsub($[k], \"e\", \"X\")}' foo.csv Per tutti i campi - k - verr\u00e0 applicata la funzione gsub (trova e sostituisci globale con supporto a espressioni regolari), che (in questo esempio) cerca la stringa e e la sostituisce con X . L'opzione -S per forzare che tutti i campi siano interpretati come stringhe. Rimuovere i ritorni a capo nelle celle \u00b6 Prendendo spunto dalla ricetta sul trova e sostituisce globale , basta cercare il carattere di ritorno a capo. In input un CSV come quello di sotto ( qui ), in cui all'interno delle celle, ci sono dei ritorno a capo materializzati da dei line feed , ovvero mappati con i caratteri speciali \\n . rimuovi-a-capo.txt Campo 1,Campo 2 \"Cella con A capo Fastidiosi\",Ipsum Lorem,\"uno Due Tre Quattro Cinque\" Si pu\u00f2 cercare appunto \\n e sostituirlo con spazio, e poi rimuovere eventuali doppi spazi usando il verbo clean-whitespace : mlr --csv -S put 'for (k in $*) {$[k] = gsub($[k], \"\\n\", \" \")}' then clean-whitespace rimuovi-a-capo.txt In output: Campo 1,Campo 2 Cella con A capo Fastidiosi,Ipsum Lorem,uno Due Tre Quattro Cinque Nota bene \\n non \u00e8 l'unico modo di materializzare un ritorno a capo , quindi \u00e8 possibile dover modificare l'esempio di sopra. qui commentata su explainshell \u21a9","title":"Ricette"},{"location":"miller/ricette/#ricette","text":"","title":"Ricette"},{"location":"miller/ricette/#concatenare-in-verticale-piu-file","text":"Il verbo \"tipico\" per concatenare due o pi\u00f9 file \u00e8 cat . Ad esempio se voglio unire in verticale questi due file CSV base.cv base_merge.csv nome,dataNascita,altezza,peso andy,1973-05-08,176,86.5 chiara,1993-12-13,162,58.3 guido,2001-01-22,196,90.4 nome,dataNascita,altezza,peso,coloreOcchi marco,1983-12-08,183,,verdi licia,1993-12-07,158,57.9,neri il comando da lanciare sar\u00e0 mlr --csv cat base.csv base_merge.csv >output.csv che dar\u00e0 in output +--------+-------------+---------+------+ | nome | dataNascita | altezza | peso | +--------+-------------+---------+------+ | andy | 1973-05-08 | 176 | 86.5 | | chiara | 1993-12-13 | 162 | 58.3 | | guido | 2001-01-22 | 196 | 90.4 | | marco | 1983-12-08 | 183 | - | | licia | 1993-12-07 | 158 | 57.9 | +--------+-------------+---------+------+ \u00c8 possibile fare il merge , l'unione in verticale, anche di due file con uno schema in parte diverso , perch\u00e9 Miller gestiste l' eterogeneit\u00e0 dei record . Se ad esempio si \u00e8 in presenza di un file che ha una colonna in pi\u00f9 ( coloreOcchi ) rispetto a base.csv , come questo ( base_altro.csv ) nome,dataNascita,altezza,peso,coloreOcchi marco,1983-12-08,183,,verdi licia,1993-12-07,158,57.9,neri il verbo da usare \u00e8 unsparsify . Il comando sar\u00e0: mlr --csv unsparsify base.csv base_altro.csv >output.csv In output, verr\u00e0 aggiunta la colonna coloreOcchi , che non sar\u00e0 valorizzata per i record del file che in partenza non aveva questa colonna: +--------+-------------+---------+------+-------------+ | nome | dataNascita | altezza | peso | coloreOcchi | +--------+-------------+---------+------+-------------+ | andy | 1973-05-08 | 176 | 86.5 | - | | chiara | 1993-12-13 | 162 | 58.3 | - | | guido | 2001-01-22 | 196 | 90.4 | - | | marco | 1983-12-08 | 183 | - | verdi | | licia | 1993-12-07 | 158 | 57.9 | neri | +--------+-------------+---------+------+-------------+","title":"Concatenare in \"verticale\" pi\u00f9 file"},{"location":"miller/ricette/#suddividere-un-file-di-input-in-piu-file-di-output-ogni-xxx-record","text":"mlr --csv put -q ' begin { @batch_size = 1000; } index = int(floor((NR-1) / @batch_size)); label = fmtnum(index,\"%04d\"); filename = \"part-\".label.\".json\"; tee > filename, $* ' ./input.csv Verr\u00e0 creato un file di output, con nome part-000XXX , ogni 1000 (si imposta tramite @batch_size ) record.","title":"Suddividere un file di input in pi\u00f9 file di output, ogni xxx record"},{"location":"miller/ricette/#estrarre-le-righe-che-contengono-il-valore-massimo-di-un-campo","text":"Alcune delle righe sottostanti, sono identiche, fatta eccezione per il V campo. input.csv 1,861265,C,A,0.071 1,861265,C,A,0.148 1,861265,C,G,0.001 1,861265,C,G,0.108 1,861265,C,T,0 1,861265,C,T,0.216 2,193456,G,A,0.006 2,193456,G,A,0.094 2,193456,G,C,0.011 2,193456,G,C,0.152 2,193456,G,T,0.003 2,193456,G,T,0.056 Se si vogliono estrarre soltanto quelle con il valore massimo del V campo, raggruppate per i valori degli altri 4, il verbo da usare \u00e8 top mlr --csv -N top -f 5 -g 1,2,3,4 input.tsv 1,861265,C,A,1,0.148 1,861265,C,G,1,0.108 1,861265,C,T,1,0.216 2,193456,G,A,1,0.094 2,193456,G,C,1,0.152 2,193456,G,T,1,0.056 Vedi https://stackoverflow.com/a/70664880/757714","title":"Estrarre le righe che contengono il valore massimo di un campo"},{"location":"miller/ricette/#eseguire-un-comando-esterno-allinterno-di-una-funzione","text":"All'interno di un comando Miller \u00e8 possibile lanciare una utility esterna, usando la funzione system . Immaginiamo ad esempio di avere un file come questo input.txt a,b 1,\"15,1,2/AX,22,1/C,1/A,1/BA,2,3\" e di voler applicare il cosiddetto natural sorting alla stringa 15,1,2/AX,22,1/C,1/A,1/BA,2,3 , ottenendo questo ordinamento 1,1/A,1/BA,1/C,2,2/AX,3,15,22 . Utilizzando le utility standard della shell di Linux basterebbe fare cos\u00ec (in paste si usa - perch\u00e9 l'input \u00e8 l' stdin ): echo \"15,1,2/AX,22,1/C,1/A,1/BA,2,3\" | tr , \"\\n\" | sort -V | paste -sd, - Per riportare questa sintassi 1 in un comando Miller, il comando sarebbe come questo di sotto, in cui viene creato il campo toto , che raccoglie valori derivanti dal lancio di utility esterne, grazie alla funzione system . <input.txt mlr --c2p --barred cat then put -S '$toto=system(\"echo \".$b.\" | tr , \\\"\\n\\\" | sort -V | paste -sd, -\")' E l'output: +---+-------------------------------+-------------------------------+ | a | b | toto | +---+-------------------------------+-------------------------------+ | 1 | 15,1,2/AX,22,1/C,1/A,1/BA,2,3 | 1,1/A,1/BA,1/C,2,2/AX,3,15,22 | +---+-------------------------------+-------------------------------+ Nel comando bisogna avere cura di inserire eventuali escape a caratteri come \" .","title":"Eseguire un comando esterno all'interno di una funzione"},{"location":"miller/ricette/#fare-un-trova-e-sostituisci-globale","text":"\u00c8 comodo utilizzare DSL , il linguaggio di scripting di Miller e usare un ciclo for : mlr --csv -S put 'for (k in $*) {$[k] = gsub($[k], \"e\", \"X\")}' foo.csv Per tutti i campi - k - verr\u00e0 applicata la funzione gsub (trova e sostituisci globale con supporto a espressioni regolari), che (in questo esempio) cerca la stringa e e la sostituisce con X . L'opzione -S per forzare che tutti i campi siano interpretati come stringhe.","title":"Fare un trova e sostituisci globale"},{"location":"miller/ricette/#rimuovere-i-ritorni-a-capo-nelle-celle","text":"Prendendo spunto dalla ricetta sul trova e sostituisce globale , basta cercare il carattere di ritorno a capo. In input un CSV come quello di sotto ( qui ), in cui all'interno delle celle, ci sono dei ritorno a capo materializzati da dei line feed , ovvero mappati con i caratteri speciali \\n . rimuovi-a-capo.txt Campo 1,Campo 2 \"Cella con A capo Fastidiosi\",Ipsum Lorem,\"uno Due Tre Quattro Cinque\" Si pu\u00f2 cercare appunto \\n e sostituirlo con spazio, e poi rimuovere eventuali doppi spazi usando il verbo clean-whitespace : mlr --csv -S put 'for (k in $*) {$[k] = gsub($[k], \"\\n\", \" \")}' then clean-whitespace rimuovi-a-capo.txt In output: Campo 1,Campo 2 Cella con A capo Fastidiosi,Ipsum Lorem,uno Due Tre Quattro Cinque Nota bene \\n non \u00e8 l'unico modo di materializzare un ritorno a capo , quindi \u00e8 possibile dover modificare l'esempio di sopra. qui commentata su explainshell \u21a9","title":"Rimuovere i ritorni a capo nelle celle"},{"location":"miller/verbi/","text":"Verbi \u00b6 I verbi sono i sub comandi di Miller. Queste le categorie: quelli analoghi allo Unix-toolkit: cat , cut , grep , head , join , sort , tac , tail , top , uniq . quelli con funziolit\u00e0 simili a quelli di awk : filter , put , sec2gmt , sec2gmtdate , step , tee . quelli statistici: bar , bootstrap , decimate , histogram , least-frequent , most-frequent , sample , shuffle , stats1 , stats2 . quelli orientati all' eterogeneit\u00e0 dei record , sebbene tutti i verbi di Miller sono in grado di gestire record eterogenei: group-by , group-like , having-fields . e altri ancora: check , count-distinct , label , merge-fields , nest , nothing , regularize , rename , reorder , reshape , seqgen . File di esempio \u00b6 File per sviluppare esempi Per la gran parte degli esempi sviluppati in questa pagina, verr\u00e0 usato il file base_category.csv (vedi sotto). \u00c8 stato scelto un file piccolo e semplice, per ragioni didattiche e di leggibilit\u00e0. nome dataNascita altezza peso comuneNascita andy 1973-05-08 176 86.5 Roma chiara 1993-12-13 162 58.3 Milano guido 2001-01-22 196 90.4 Roma sara 2000-02-22 166 70.4 Roma giulia 1997-08-13 169 68.3 Milano base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano Guida in linea per un verbo \u00b6 Per aprire la guida in linea di un verbo, basta lanciare mlr nomeVerbo --help . Ad esempio per il verbo cat , il comando mlr cat --help restituir\u00e0: Usage: mlr cat [options] Passes input records directly to output. Most useful for format conversion. Options: -n Prepend field \"n\" to each record with record-counter starting at 1 -g {comma-separated field name(s)} When used with -n/-N, writes record-counters keyed by specified field name(s). -v Write a low-level record-structure dump to stderr. -N {name} Prepend field {name} to each record with record-counter starting at 1 A seguire, per ogni verbo, sar\u00e0 inserito l' help ufficiale di ogni comando. Lista dei verbi \u00b6 altkv \u00b6 Mappa una lista di valori, come coppie alternate chiave/valore. mlr altkv --help Usage: mlr altkv [options] Given fields with values of the form a,b,c,d,e,f emits a=b,c=d,e=f pairs. Ad esempio echo \"a,b,c,d\" | mlr --ocsv altkv { \"a\" : \"b\" , \"c\" : \"d\" } bar \u00b6 Per creare dei grafici a barre (bruttini \ud83d\ude43), rimpiazzando dei valori numeri con una serie di asterichi. Per allinearli meglio si possono usare le opzioni di output --opprint o --oxtab . mlr bar --help Replaces a numeric field with a number of asterisks, allowing for cheesy bar plots. These align best with --opprint or --oxtab output format. Options: -f {a,b,c} Field names to convert to bars. --lo {lo} Lower-limit value for min-width bar: default '0.000000'. --hi {hi} Upper-limit value for max-width bar: default '100.000000'. -w {n} Bar-field width: default '40'. --auto Automatically computes limits, ignoring --lo and --hi. Holds all records in memory before producing any output. -c {character} Fill character: default '*'. -x {character} Out-of-bounds character: default '#'. -b {character} Blank character: default '.'. Nominally the fill, out-of-bounds, and blank characters will be strings of length 1. However you can make them all longer if you so desire. A partire ad esempio da questo file di input: bar.csv Area,percUtenti Nord,25 Centro,32 Sud e isole,43 mlr --c2p bar -f percUtenti ./bar.csv Area percUtenti Nord **********.............................. Centro ************............................ Sud e isole *****************....................... bootstrap \u00b6 L'uso tipico di bootstrap \u00e8 quello di estrarre un campione random dall'input, con un numero di record pari al numero record di input; con doppioni possibili. mlr bootstrap --help Emits an n-sample, with replacement, of the input records. See also mlr sample and mlr shuffle. Options: -n Number of samples to output. Defaults to number of input records. Must be non-negative. Sotto un esempio in cui a partire dal file di input, composto da 5 record distinti, sono estratti in modo randomico 5 record. mlr --csv bootstrap base_category.csv nome,dataNascita,altezza,peso,comuneNascita guido,2001-01-22,196,90.4,Roma chiara,1993-12-13,162,58.3,Milano chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma chiara,1993-12-13,162,58.3,Milano cat \u00b6 Utile sopratutto per conversione di formato (vedi formati ) e per concatenare in \"verticale\" file con lo stesso schema ( ricetta ). mlr cat --help Usage: mlr cat [options] Passes input records directly to output. Most useful for format conversion. Options: -n Prepend field \"n\" to each record with record-counter starting at 1 -g {comma-separated field name(s)} When used with -n/-N, writes record-counters keyed by specified field name(s). -v Write a low-level record-structure dump to stderr. -N {name} Prepend field {name} to each record with record-counter starting at 1 Il comando di base stampa a schermo il contenuto di un file: mlr --csv cat base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano Con l'opzione -n si aggiunge un campo con un progressivo numerico intero che parte da 1 : mlr --csv -n cat base.csv n,nome,dataNascita,altezza,peso,comuneNascita 1,andy,1973-05-08,176,86.5,Roma 2,chiara,1993-12-13,162,58.3,Milano 3,guido,2001-01-22,196,90.4,Roma 4,sara,2000-02-22,166,70.4,Roma 5,giulia,1997-08-13,169,68.3,Milano Se all'opzione -n si aggiunge -g seguito da dal nome di uno o pi\u00f9 campi, il \"contatore\" sar\u00e0 applicato per gruppo e partir\u00e0 da 1 distintamente per ogni gruppo. Qui sotto ad esempio il raggruppamento \u00e8 fatto per comuneNascita : mlr --csv cat -n -g comuneNascita base_category.csv n,nome,dataNascita,altezza,peso,comuneNascita 1,andy,1973-05-08,176,86.5,Roma 1,chiara,1993-12-13,162,58.3,Milano 2,guido,2001-01-22,196,90.4,Roma 3,sara,2000-02-22,166,70.4,Roma 2,giulia,1997-08-13,169,68.3,Milano check \u00b6 \u00c8 un verbo che non produce un output, salvo un report sulla correttezza di \"formattazione\" del file di input. mlr check --help Consumes records without printing any output. Useful for doing a well-formatted check on input data. Options: Se ad esempio si ha un file CSV , con un numero di campi nell'intestazione ( 4 ) diverso da quello del corpo ( 5 ), come questo check.csv nome,dataNascita,altezza,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma si avr\u00e0 l'output sottostante mlr --csv check check.csv mlr : mlr: CSV header/data length mismatch 4 != 5 at filename ./check.csv row 2. clean-whitespace \u00b6 \u00c8 un prezioso comando, per \"pulire\" i dati: rimuove gli spazi ridondanti. mlr clean-whitespace --help For each record, for each field in the record, whitespace-cleans the keys and/or values. Whitespace-cleaning entails stripping leading and trailing whitespace, and replacing multiple whitespace with singles. For finer-grained control, please see the DSL functions lstrip, rstrip, strip, collapse_whitespace, and clean_whitespace. Options: -k|--keys-only Do not touch values. -v|--values-only Do not touch keys. It is an error to specify -k as well as -v -- to clean keys and values, leave off -k as well as -v. Riuove in particolare uno o pi\u00f9 spazi bianchi a inizio e fine cella e due o pi\u00f9 spazi all'interndo della cella. Ad esempio nel file sottostante ci sono due spazi tra Busto e Arsizio , uno spazio a fine cella dopo andy e uno a inizio cella i corrispondenza di chiara . Questi sono ridondanti, e nella grandissima parte dei casi sono da rimuovere. clean-whitespace.json { \"nome\" : \"andy \" , \"dataNascita\" : \"1973-05-08\" , \"comuneNascita\" : \"Roma\" } { \"nome\" : \" chiara\" , \"dataNascita\" : \"1993-12-13\" , \"comuneNascita\" : \"Busto Arsizio\" } Per pulire il file: mlr --json clean-whitespace input.json { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"comuneNascita\" : \"Busto Arsizio\" } Due comode opzioni: -k fa la pulizia soltanto nei nomi dei campi, nelle chiavi; -v fa la pulizia soltanto nei valori. count \u00b6 Restituisce il numero di record. Miller tiene conto del formato, quindi per un CSV composto da 6 righe, 1 di intestazione pi\u00f9 5 di dati, restituir\u00e0 5. mlr count --help Prints number of records, optionally grouped by distinct values for specified field names. Options: -g {a,b,c} Optional group-by-field names for counts, e.g. a,b,c -n {n} Show only the number of distinct values. Not interesting without -g. -o {name} Field name for output-count. Default \"count\". File di esempio: base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano Comando di esempio: mlr --csv count ./base_category.csv count 5 Se si desidera soltanto l'output numerico, senza riga di intestazione, si pu\u00f2 fare in tantissimi modi. Uno \u00e8 quello di cambiare il formato di output in NIDX : mlr --c2n count ./base_category.csv 5 count-distinct \u00b6 Restituisce il numero di record che hanno valori distinti, per uno o pi\u00f9 campi specificati. mlr count-distinct --help Prints number of records having distinct values for specified field names. Same as uniq -c. Options: -f {a,b,c} Field names for distinct count. -n Show only the number of distinct values. Not compatible with -u. -o {name} Field name for output count. Default \"count\". Ignored with -u. -u Do unlashed counts for multiple field names. With -f a,b and without -u, computes counts for distinct combinations of a and b field values. With -f a,b and with -u, computes counts for distinct a field values and counts for distinct b field values separately. File di esempio: count-distinct.csv nome,dataNascita,altezza,peso,comuneNascita,sesso andy,1973-05-08,176,86.5,Roma,maschio chiara,1993-12-13,162,58.3,Milano,femmina guido,2001-01-22,196,90.4,Roma,maschio sara,2000-02-22,166,70.4,Roma,femmina giulia,1997-08-13,169,68.3,Milano,femmina Ad esempio il conteggio per combinazioni distinte di comune di nascita e sesso: mlr --csv count-distinct -f comuneNascita,sesso -o conteggio ./base_category.csv comuneNascita,sesso,conteggio Roma,maschio,2 Milano,femmina,2 Roma,femmina,1 Aggiungendo il parametro -u , si ottengono i valori distinti non per combinazioni distinti dei campi indicati, ma per ogni singolo campo. mlr --csv count-distinct -f comuneNascita,sesso -o conteggio -u ./base_category.csv field,value,count comuneNascita,Roma,3 comuneNascita,Milano,2 sesso,maschio,2 sesso,femmina,3 count-similar \u00b6 Aggiunge un campo, con il conteggio dei record che ha lo stesso valore per uno o pi\u00f9 campi. mlr count-similar --help Ingests all records, then emits each record augmented by a count of the number of other records having the same group-by field values. Options: -g {a,b,c} Group-by-field names for counts, e.g. a,b,c -o {name} Field name for output-counts. Defaults to \"count\". File di esempio: base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano A seguire ad esempio viene aggiunto il campo conteggio al file di input, con il conteggio dei valori distinti per il campo comuneNascita . mlr --csv count-similar -g comuneNascita -o conteggio ./base_category.csv nome,dataNascita,altezza,peso,comuneNascita,conteggio andy,1973-05-08,176,86.5,Roma,3 guido,2001-01-22,196,90.4,Roma,3 sara,2000-02-22,166,70.4,Roma,3 chiara,1993-12-13,162,58.3,Milano,2 giulia,1997-08-13,169,68.3,Milano,2 cut \u00b6 Estrae/rimuove dall'input uno o pi\u00f9 campi. mlr cut --help Passes through input records with specified fields included/excluded. Options: -f {a,b,c} Comma-separated field names for cut, e.g. a,b,c. -o Retain fields in the order specified here in the argument list. Default is to retain them in the order found in the input data. -x|--complement Exclude, rather than include, field names specified by -f. -r Treat field names as regular expressions. \"ab\", \"a.*b\" will match any field name containing the substring \"ab\" or matching \"a.*b\", respectively; anchors of the form \"^ab$\", \"^a.*b$\" may be used. The -o flag is ignored when -r is present. Examples: mlr cut -f hostname,status mlr cut -x -f hostname,status mlr cut -r -f '^status$,sda[0-9]' mlr cut -r -f '^status$,\"sda[0-9]\"' mlr cut -r -f '^status$,\"sda[0-9]\"i' (this is case-insensitive) File di esempio: base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano Per estrarre soltanto il campo nome : mlr --csv cut -f nome base_category.csv nome andy chiara guido sara giulia Per rimuovere il campo nome , bisogner\u00e0 aggiungere l'opzione -x : mlr --csv cut -x -f nome base_category.csv dataNascita,altezza,peso,comuneNascita 1973-05-08,176,86.5,Roma 1993-12-13,162,58.3,Milano 2001-01-22,196,90.4,Roma 2000-02-22,166,70.4,Roma 1997-08-13,169,68.3,Milano Per impostare il filtro tramite espressione regolare si usa l'opzione -r . Ad esempio estrarre soltanto i campi che iniziano per a e che terminano per o : mlr --csv cut -r -f \"^a\",\"o$\" base_category.csv altezza,peso 176,86.5 162,58.3 196,90.4 166,70.4 169,68.3 decimate \u00b6 Estrae un record ogni n , opzionalmente per categoria. mlr decimate --help Passes through one of every n records, optionally by category. Options: -b Decimate by printing first of every n. -e Decimate by printing last of every n (default). -g {a,b,c} Optional group-by-field names for decimate counts, e.g. a,b,c. -n {n} Decimation factor (default 10). fill-down \u00b6 Se un dato record ha un valore mancante per un dato campo, verr\u00e0 riempito dal valore corrispondente del record precedente, se presente. mlr fill-down --help If a given record has a missing value for a given field, fill that from the corresponding value from a previous record, if any. By default, a 'missing' field either is absent, or has the empty-string value. With -a, a field is 'missing' only if it is absent. Options: --all Operate on all fields in the input. -a|--only-if-absent If a given record has a missing value for a given field, fill that from the corresponding value from a previous record, if any. By default, a 'missing' field either is absent, or has the empty-string value. With -a, a field is 'missing' only if it is absent. -f Field names for fill-down. File di esempio: fill-down.csv a,b,c 1,,3 4,5, 7,,9 Con l'opzione -all il verbo viene applicato per tutti i record: mlr --csv fill-down --all fill-down.csv a,b,c 1,,3 4,5,3 7,5,9 Con -f \u00e8 possibile scegliere di applicaro a 1 o pi\u00f9 campi. fill-empty \u00b6 Il verbo fill-empty richiede Miller >= 6.0 Riempe le celle vuote, con un valore specifico mlr fill-empty --help Fills empty-string fields with specified fill-value. Options: -v {string} Fill-value: defaults to \"N/A\" -S Don't infer type -- so '-v 0' would fill string 0 not int 0. File di esempio: fill-down.csv a,b,c 1,,3 4,5, 7,,9 Come impopstazione predefinita, il valore assegnato sar\u00e0 N/A , ma \u00e8 possibile impostarlo con -v : mlr --csv fill-empty fill-down.csv a,b,c 1,N/A,3 4,5,N/A 7,N/A,9 filter \u00b6 mlr filter --help Options: -f {file name} File containing a DSL expression (see examples below). If the filename is a directory, all *.mlr files in that directory are loaded. -e {expression} You can use this after -f to add an expression. Example use case: define functions/subroutines in a file you specify with -f, then call them with an expression you specify with -e. (If you mix -e and -f then the expressions are evaluated in the order encountered. Since the expression pieces are simply concatenated, please be sure to use intervening semicolons to separate expressions.) -s name=value: Predefines out-of-stream variable @name to have Thus mlr put -s foo=97 '$column += @foo' is like mlr put 'begin {@foo = 97} $column += @foo'. The value part is subject to type-inferencing. May be specified more than once, e.g. -s name1=value1 -s name2=value2. Note: the value may be an environment variable, e.g. -s sequence=$SEQUENCE -x (default false) Prints records for which {expression} evaluates to false, not true, i.e. invert the sense of the filter expression. -q Does not include the modified record in the output stream. Useful for when all desired output is in begin and/or end blocks. -S and -F: There are no-ops in Miller 6 and above, since now type-inferencing is done by the record-readers before filter/put is executed. Supported as no-op pass-through flags for backward compatibility. Parser-info options: -w Print warnings about things like uninitialized variables. -W Same as -w, but exit the process if there are any warnings. -p Prints the expressions's AST (abstract syntax tree), which gives full transparency on the precedence and associativity rules of Miller's grammar, to stdout. -d Like -p but uses a parenthesized-expression format for the AST. -D Like -d but with output all on one line. -E Echo DSL expression before printing parse-tree -v Same as -E -p. -X Exit after parsing but before stream-processing. Useful with -v/-d/-D, if you only want to look at parser information. Records will pass the filter depending on the last bare-boolean statement in the DSL expression. That can be the result of <, ==, >, etc., the return value of a function call which returns boolean, etc. Examples: mlr --csv --from example.csv filter '$color == \"red\"' mlr --csv --from example.csv filter '$color == \"red\" && flag == true' More example filter expressions: First record in each file: 'FNR == 1' Subsampling: 'urand() < 0.001' Compound booleans: '$color != \"blue\" && $value > 4.2' '($x < 0.5 && $y < 0.5) || ($x > 0.5 && $y > 0.5)' Regexes with case-insensitive flag '($name =~ \"^sys.*east$\") || ($name =~ \"^dev.[0-9]+\"i)' Assignments, then bare-boolean filter statement: '$ab = $a+$b; $cd = $c+$d; $ab != $cd' Bare-boolean filter statement within a conditional: 'if (NR < 100) { $x > 0.3; } else { $x > 0.002; } ' Using 'any' higher-order function to see if $index is 10, 20, or 30: 'any([10,20,30], func(e) {return $index == e})' See also https://miller.readthedocs.io/reference-dsl for more context. flatten \u00b6 mlr flatten --help Flattens multi-level maps to single-level ones. Example: field with name 'a' and value '{\"b\": { \"c\": 4 }}' becomes name 'a.b.c' and value 4. Options: -f Comma-separated list of field names to flatten (default all). -s Separator, defaulting to mlr --flatsep value. format-values \u00b6 mlr format-values --help Applies format strings to all field values, depending on autodetected type. * If a field value is detected to be integer, applies integer format. * Else, if a field value is detected to be float, applies float format. * Else, applies string format. Note: this is a low-keystroke way to apply formatting to many fields. To get finer control, please see the fmtnum function within the mlr put DSL. Note: this verb lets you apply arbitrary format strings, which can produce undefined behavior and/or program crashes. See your system's \"man printf\". Options: -i {integer format} Defaults to \"%d\". Examples: \"%06lld\", \"%08llx\". Note that Miller integers are long long so you must use formats which apply to long long, e.g. with ll in them. Undefined behavior results otherwise. -f {float format} Defaults to \"%f\". Examples: \"%8.3lf\", \"%.6le\". Note that Miller floats are double-precision so you must use formats which apply to double, e.g. with l[efg] in them. Undefined behavior results otherwise. -s {string format} Defaults to \"%s\". Examples: \"_%s\", \"%08s\". Note that you must use formats which apply to string, e.g. with s in them. Undefined behavior results otherwise. -n Coerce field values autodetected as int to float, and then apply the float format. fraction \u00b6 mlr fraction --help For each record's value in specified fields, computes the ratio of that value to the sum of values in that field over all input records. E.g. with input records x=1 x=2 x=3 and x=4, emits output records x=1,x_fraction=0.1 x=2,x_fraction=0.2 x=3,x_fraction=0.3 and x=4,x_fraction=0.4 Note: this is internally a two-pass algorithm: on the first pass it retains input records and accumulates sums; on the second pass it computes quotients and emits output records. This means it produces no output until all input is read. Options: -f {a,b,c} Field name(s) for fraction calculation -g {d,e,f} Optional group-by-field name(s) for fraction counts -p Produce percents [0..100], not fractions [0..1]. Output field names end with \"_percent\" rather than \"_fraction\" -c Produce cumulative distributions, i.e. running sums: each output value folds in the sum of the previous for the specified group E.g. with input records x=1 x=2 x=3 and x=4, emits output records x=1,x_cumulative_fraction=0.1 x=2,x_cumulative_fraction=0.3 x=3,x_cumulative_fraction=0.6 and x=4,x_cumulative_fraction=1.0 gap \u00b6 mlr gap --help Emits an empty record every n records, or when certain values change. Options: Emits an empty record every n records, or when certain values change. -g {a,b,c} Print a gap whenever values of these fields (e.g. a,b,c) changes. -n {n} Print a gap every n records. One of -f or -g is required. -n is ignored if -g is present. grep \u00b6 mlr grep --help Passes through records which match the regular expression. Options: -i Use case-insensitive search. -v Invert: pass through records which do not match the regex. Note that \"mlr filter\" is more powerful, but requires you to know field names. By contrast, \"mlr grep\" allows you to regex-match the entire record. It does this by formatting each record in memory as DKVP, using command-line-specified ORS/OFS/OPS, and matching the resulting line against the regex specified here. In particular, the regex is not applied to the input stream: if you have CSV with header line \"x,y,z\" and data line \"1,2,3\" then the regex will be matched, not against either of these lines, but against the DKVP line \"x=1,y=2,z=3\". Furthermore, not all the options to system grep are supported, and this command is intended to be merely a keystroke-saver. To get all the features of system grep, you can do \"mlr --odkvp ... | grep ... | mlr --idkvp ...\" group-by \u00b6 mlr group-by --help Outputs records in batches having identical values at specified field names.Options: group-like \u00b6 mlr group-like --help Outputs records in batches having identical field names. Options: having-fields \u00b6 mlr having-fields --help Conditionally passes through records depending on each record's field names. Options: --at-least {comma-separated names} --which-are {comma-separated names} --at-most {comma-separated names} --all-matching {regular expression} --any-matching {regular expression} --none-matching {regular expression} Examples: mlr having-fields --which-are amount,status,owner mlr having-fields --any-matching 'sda[0-9]' mlr having-fields --any-matching '\"sda[0-9]\"' mlr having-fields --any-matching '\"sda[0-9]\"i' (this is case-insensitive) head \u00b6 mlr head --help Passes through the first n records, optionally by category. Without -g, ceases consuming more input (i.e. is fast) when n records have been read. Options: -g {a,b,c} Optional group-by-field names for head counts, e.g. a,b,c. -n {n} Head-count to print. Default 10. histogram \u00b6 mlr histogram --help Just a histogram. Input values < lo or > hi are not counted. -f {a,b,c} Value-field names for histogram counts --lo {lo} Histogram low value --hi {hi} Histogram high value --nbins {n} Number of histogram bins. Defaults to 20. --auto Automatically computes limits, ignoring --lo and --hi. Holds all values in memory before producing any output. -o {prefix} Prefix for output field name. Default: no prefix. json-parse \u00b6 mlr json-parse --help Tries to convert string field values to parsed JSON, e.g. \"[1,2,3]\" -> [1,2,3]. Options: -f {...} Comma-separated list of field names to json-parse (default all). json-stringify \u00b6 mlr json-stringify --help Produces string field values from field-value data, e.g. [1,2,3] -> \"[1,2,3]\". Options: -f {...} Comma-separated list of field names to json-parse (default all). --jvstack Produce multi-line JSON output. --no-jvstack Produce single-line JSON output per record (default). join \u00b6 mlr join --help Joins records from specified left file name with records from all file names at the end of the Miller argument list. Functionality is essentially the same as the system \"join\" command, but for record streams. Options: -f {left file name} -j {a,b,c} Comma-separated join-field names for output -l {a,b,c} Comma-separated join-field names for left input file; defaults to -j values if omitted. -r {a,b,c} Comma-separated join-field names for right input file(s); defaults to -j values if omitted. --lp {text} Additional prefix for non-join output field names from the left file --rp {text} Additional prefix for non-join output field names from the right file(s) --np Do not emit paired records --ul Emit unpaired records from the left file --ur Emit unpaired records from the right file(s) -s|--sorted-input Require sorted input: records must be sorted lexically by their join-field names, else not all records will be paired. The only likely use case for this is with a left file which is too big to fit into system memory otherwise. -u Enable unsorted input. (This is the default even without -u.) In this case, the entire left file will be loaded into memory. If you wish to use a prepipe command for the main input as well as here, it must be specified there as well as here. --prepipex {command} Likewise. File-format options default to those for the right file names on the Miller argument list, but may be overridden for the left file as follows. Please see -i {one of csv,dkvp,nidx,pprint,xtab} --irs {record-separator character} --ifs {field-separator character} --ips {pair-separator character} --repifs --implicit-csv-header --no-implicit-csv-header For example, if you have 'mlr --csv ... join -l foo ... ' then the left-file format will be specified CSV as well unless you override with 'mlr --csv ... join --ijson -l foo' etc. Likewise, if you have 'mlr --csv --implicit-csv-header ...' then the join-in file will be expected to be headerless as well unless you put '--no-implicit-csv-header' after 'join'. Please use \"mlr --usage-separator-options\" for information on specifying separators. Please see https://miller.readthedocs.io/en/latest/reference-verbs.html#join for more information including examples. label \u00b6 mlr label --help Given n comma-separated names, renames the first n fields of each record to have the respective name. (Fields past the nth are left with their original names.) Particularly useful with --inidx or --implicit-csv-header, to give useful names to otherwise integer-indexed fields. Options: least-frequent \u00b6 mlr least-frequent --help Shows the least frequently occurring distinct values for specified field names. The first entry is the statistical anti-mode; the remaining are runners-up. Options: -f {one or more comma-separated field names}. Required flag. -n {count}. Optional flag defaulting to 10. -b Suppress counts; show only field values. -o {name} Field name for output count. Default \"count\". See also \"mlr most-frequent\". merge-fields \u00b6 mlr merge-fields --help Computes univariate statistics for each input record, accumulated across specified fields. Options: -a {sum,count,...} Names of accumulators. One or more of: count Count instances of fields mode Find most-frequently-occurring values for fields; first-found wins tie antimode Find least-frequently-occurring values for fields; first-found wins tie sum Compute sums of specified fields mean Compute averages (sample means) of specified fields var Compute sample variance of specified fields stddev Compute sample standard deviation of specified fields meaneb Estimate error bars for averages (assuming no sample autocorrelation) skewness Compute sample skewness of specified fields kurtosis Compute sample kurtosis of specified fields min Compute minimum values of specified fields max Compute maximum values of specified fields -f {a,b,c} Value-field names on which to compute statistics. Requires -o. -r {a,b,c} Regular expressions for value-field names on which to compute statistics. Requires -o. -c {a,b,c} Substrings for collapse mode. All fields which have the same names after removing substrings will be accumulated together. Please see examples below. -i Use interpolated percentiles, like R's type=7; default like type=1. Not sensical for string-valued fields. -o {name} Output field basename for -f/-r. -k Keep the input fields which contributed to the output statistics; the default is to omit them. String-valued data make sense unless arithmetic on them is required, e.g. for sum, mean, interpolated percentiles, etc. In case of mixed data, numbers are less than strings. Example input data: \"a_in_x=1,a_out_x=2,b_in_y=4,b_out_x=8\". Example: mlr merge-fields -a sum,count -f a_in_x,a_out_x -o foo produces \"b_in_y=4,b_out_x=8,foo_sum=3,foo_count=2\" since \"a_in_x,a_out_x\" are summed over. Example: mlr merge-fields -a sum,count -r in_,out_ -o bar produces \"bar_sum=15,bar_count=4\" since all four fields are summed over. Example: mlr merge-fields -a sum,count -c in_,out_ produces \"a_x_sum=3,a_x_count=2,b_y_sum=4,b_y_count=1,b_x_sum=8,b_x_count=1\" since \"a_in_x\" and \"a_out_x\" both collapse to \"a_x\", \"b_in_y\" collapses to \"b_y\", and \"b_out_x\" collapses to \"b_x\". most-frequent \u00b6 mlr most-frequent --help Shows the most frequently occurring distinct values for specified field names. The first entry is the statistical mode; the remaining are runners-up. Options: -f {one or more comma-separated field names}. Required flag. -n {count}. Optional flag defaulting to 10. -b Suppress counts; show only field values. -o {name} Field name for output count. Default \"count\". See also \"mlr least-frequent\". nest \u00b6 mlr nest --help Explodes specified field values into separate fields/records, or reverses this. Options: --explode,--implode One is required. --values,--pairs One is required. --across-records,--across-fields One is required. -f {field name} Required. --nested-fs {string} Defaults to \";\". Field separator for nested values. --nested-ps {string} Defaults to \":\". Pair separator for nested key-value pairs. --evar {string} Shorthand for --explode --values ---across-records --nested-fs {string} --ivar {string} Shorthand for --implode --values ---across-records --nested-fs {string} Please use \"mlr --usage-separator-options\" for information on specifying separators. Examples: mlr nest --explode --values --across-records -f x with input record \"x=a;b;c,y=d\" produces output records \"x=a,y=d\" \"x=b,y=d\" \"x=c,y=d\" Use --implode to do the reverse. mlr nest --explode --values --across-fields -f x with input record \"x=a;b;c,y=d\" produces output records \"x_1=a,x_2=b,x_3=c,y=d\" Use --implode to do the reverse. mlr nest --explode --pairs --across-records -f x with input record \"x=a:1;b:2;c:3,y=d\" produces output records \"a=1,y=d\" \"b=2,y=d\" \"c=3,y=d\" mlr nest --explode --pairs --across-fields -f x with input record \"x=a:1;b:2;c:3,y=d\" produces output records \"a=1,b=2,c=3,y=d\" Notes: * With --pairs, --implode doesn't make sense since the original field name has been lost. * The combination \"--implode --values --across-records\" is non-streaming: no output records are produced until all input records have been read. In particular, this means it won't work in tail -f contexts. But all other flag combinations result in streaming (tail -f friendly) data processing. * It's up to you to ensure that the nested-fs is distinct from your data's IFS: e.g. by default the former is semicolon and the latter is comma. See also mlr reshape. nothing \u00b6 mlr nothing --help Drops all input records. Useful for testing, or after tee/print/etc. have produced other output. Options: put \u00b6 mlr put --help Options: -f {file name} File containing a DSL expression (see examples below). If the filename is a directory, all *.mlr files in that directory are loaded. -e {expression} You can use this after -f to add an expression. Example use case: define functions/subroutines in a file you specify with -f, then call them with an expression you specify with -e. (If you mix -e and -f then the expressions are evaluated in the order encountered. Since the expression pieces are simply concatenated, please be sure to use intervening semicolons to separate expressions.) -s name=value: Predefines out-of-stream variable @name to have Thus mlr put -s foo=97 '$column += @foo' is like mlr put 'begin {@foo = 97} $column += @foo'. The value part is subject to type-inferencing. May be specified more than once, e.g. -s name1=value1 -s name2=value2. Note: the value may be an environment variable, e.g. -s sequence=$SEQUENCE -x (default false) Prints records for which {expression} evaluates to false, not true, i.e. invert the sense of the filter expression. -q Does not include the modified record in the output stream. Useful for when all desired output is in begin and/or end blocks. -S and -F: There are no-ops in Miller 6 and above, since now type-inferencing is done by the record-readers before filter/put is executed. Supported as no-op pass-through flags for backward compatibility. Parser-info options: -w Print warnings about things like uninitialized variables. -W Same as -w, but exit the process if there are any warnings. -p Prints the expressions's AST (abstract syntax tree), which gives full transparency on the precedence and associativity rules of Miller's grammar, to stdout. -d Like -p but uses a parenthesized-expression format for the AST. -D Like -d but with output all on one line. -E Echo DSL expression before printing parse-tree -v Same as -E -p. -X Exit after parsing but before stream-processing. Useful with -v/-d/-D, if you only want to look at parser information. Examples: mlr --from example.csv put '$qr = $quantity * $rate' More example put expressions: If-statements: 'if ($flag == true) { $quantity *= 10}' 'if ($x > 0.0 { $y=log10($x); $z=sqrt($y) } else {$y = 0.0; $z = 0.0}' Newly created fields can be read after being written: '$new_field = $index**2; $qn = $quantity * $new_field' Regex-replacement: '$name = sub($name, \"http.*com\"i, \"\")' Regex-capture: 'if ($a =~ \"([a-z]+)_([0-9]+)) { $b = \"left_\\1\"; $c = \"right_\\2\" }' Built-in variables: '$filename = FILENAME' Aggregations (use mlr put -q): '@sum += $x; end {emit @sum}' '@sum[$shape] += $quantity; end {emit @sum, \"shape\"}' '@sum[$shape][$color] += $x; end {emit @sum, \"shape\", \"color\"}' ' @min = min(@min,$x); @max=max(@max,$x); end{emitf @min, @max} ' See also https://miller.readthedocs.io/reference-dsl for more context. regularize \u00b6 Il verbo regularize riordina le righe nello stesso ordine della prima (qualunque sia l'ordine). mlr regularize --help Outputs records sorted lexically ascending by keys. Options: remove-empty-columns \u00b6 mlr remove-empty-columns --help Omits fields which are empty on every input row. Non-streaming. Options: rename \u00b6 mlr rename --help Renames specified fields. Options: -r Treat old field names as regular expressions. \"ab\", \"a.*b\" will match any field name containing the substring \"ab\" or matching \"a.*b\", respectively; anchors of the form \"^ab$\", \"^a.*b$\" may be used. New field names may be plain strings, or may contain capture groups of the form \"\\1\" through \"\\9\". Wrapping the regex in double quotes is optional, but is required if you wish to follow it with 'i' to indicate case-insensitivity. -g Do global replacement within each field name rather than first-match replacement. Examples: mlr rename old_name,new_name' mlr rename old_name_1,new_name_1,old_name_2,new_name_2' mlr rename -r 'Date_[0-9]+,Date,' Rename all such fields to be \"Date\" mlr rename -r '\"Date_[0-9]+\",Date' Same mlr rename -r 'Date_([0-9]+).*,\\1' Rename all such fields to be of the form 20151015 mlr rename -r '\"name\"i,Name' Rename \"name\", \"Name\", \"NAME\", etc. to \"Name\" reorder \u00b6 mlr reorder --help Moves specified names to start of record, or end of record. Options: -e Put specified field names at record end: default is to put them at record start. -f {a,b,c} Field names to reorder. -b {x} Put field names specified with -f before field name specified by {x}, if any. If {x} isn't present in a given record, the specified fields will not be moved. -a {x} Put field names specified with -f after field name specified by {x}, if any. If {x} isn't present in a given record, the specified fields will not be moved. Examples: mlr reorder -f a,b sends input record \"d=4,b=2,a=1,c=3\" to \"a=1,b=2,d=4,c=3\". mlr reorder -e -f a,b sends input record \"d=4,b=2,a=1,c=3\" to \"d=4,c=3,a=1,b=2\". repeat \u00b6 mlr repeat --help Copies input records to output records multiple times. Options must be exactly one of the following: -n {repeat count} Repeat each input record this many times. -f {field name} Same, but take the repeat count from the specified field name of each input record. Example: echo x=0 | mlr repeat -n 4 then put '$x=urand()' produces: x=0.488189 x=0.484973 x=0.704983 x=0.147311 Example: echo a=1,b=2,c=3 | mlr repeat -f b produces: a=1,b=2,c=3 a=1,b=2,c=3 Example: echo a=1,b=2,c=3 | mlr repeat -f c produces: a=1,b=2,c=3 a=1,b=2,c=3 a=1,b=2,c=3 reshape \u00b6 Trasforma lo schema da wide a long e viceversa. Vedi approfondimento . mlr reshape --help Wide-to-long options: -i {input field names} -o {key-field name,value-field name} -r {input field regexes} -o {key-field name,value-field name} These pivot/reshape the input data such that the input fields are removed and separate records are emitted for each key/value pair. Note: this works with tail -f and produces output records for each input record seen. Long-to-wide options: -s {key-field name,value-field name} These pivot/reshape the input data to undo the wide-to-long operation. Note: this does not work with tail -f; it produces output records only after all input records have been read. Ad esempio da wide Studente Scuola Matematica Italiano Andy Liceo Cannizzaro 7 6 Lisa Liceo Garibaldi 6 7 Giovanna Liceo Garibaldi 7 7 a long Studente Scuola materia voto Andy Liceo Cannizzaro Matematica 7 Andy Liceo Cannizzaro Italiano 6 Lisa Liceo Garibaldi Matematica 6 Lisa Liceo Garibaldi Italiano 7 Giovanna Liceo Garibaldi Matematica 7 Giovanna Liceo Garibaldi Italiano 7 sample \u00b6 mlr sample --help Reservoir sampling (subsampling without replacement), optionally by category. See also mlr bootstrap and mlr shuffle. Options: -g {a,b,c} Optional: group-by-field names for samples, e.g. a,b,c. -k {k} Required: number of records to output in total, or by group if using -g. sec2gmtdate \u00b6 mlr sec2gmtdate --help Replaces a numeric field representing seconds since the epoch with the corresponding GMT year-month-day timestamp; leaves non-numbers as-is. This is nothing more than a keystroke-saver for the sec2gmtdate function: ../c/mlr sec2gmtdate time1,time2 is the same as ../c/mlr put '$time1=sec2gmtdate($time1);$time2=sec2gmtdate($time2)' sec2gmt \u00b6 mlr sec2gmt --help Replaces a numeric field representing seconds since the epoch with the corresponding GMT timestamp; leaves non-numbers as-is. This is nothing more than a keystroke-saver for the sec2gmt function: mlr sec2gmt time1,time2 is the same as mlr put '$time1 = sec2gmt($time1); $time2 = sec2gmt($time2)' Options: -1 through -9: format the seconds using 1..9 decimal places, respectively. --millis Input numbers are treated as milliseconds since the epoch. --micros Input numbers are treated as microseconds since the epoch. --nanos Input numbers are treated as nanoseconds since the epoch. seqgen \u00b6 mlr seqgen --help Passes input records directly to output. Most useful for format conversion. Produces a sequence of counters. Discards the input record stream. Produces output as specified by the options Options: -f {name} (default \"i\") Field name for counters. --start {value} (default 1) Inclusive start value. --step {value} (default 1) Step value. --stop {value} (default 100) Inclusive stop value. Start, stop, and/or step may be floating-point. Output is integer if start, stop, and step are all integers. Step may be negative. It may not be zero unless start == stop. shuffle \u00b6 mlr shuffle --help Outputs records randomly permuted. No output records are produced until all input records are read. See also mlr bootstrap and mlr sample. Options: skip-trivial-records \u00b6 mlr skip-trivial-records --help Passes through all records except those with zero fields, or those for which all fields have empty value. Options: sort \u00b6 mlr sort --help Sorts records primarily by the first specified field, secondarily by the second field, and so on. (Any records not having all specified sort keys will appear at the end of the output, in the order they were encountered, regardless of the specified sort order.) The sort is stable: records that compare equal will sort in the order they were encountered in the input record stream. Options: -f {comma-separated field names} Lexical ascending -r {comma-separated field names} Lexical descending -c {comma-separated field names} Case-folded lexical ascending -cr {comma-separated field names} Case-folded lexical descending -n {comma-separated field names} Numerical ascending; nulls sort last -nf {comma-separated field names} Same as -n -nr {comma-separated field names} Numerical descending; nulls sort first -t {comma-separated field names} Natural ascending -tr {comma-separated field names} Natural descending -h|--help Show this message. Example: mlr sort -f a,b -nr x,y,z which is the same as: mlr sort -f a -f b -nr x -nr y -nr z sort-within-records \u00b6 Riordina i campi in ordine lessicamente crescente per nome campo. mlr sort-within-records --help Outputs records sorted lexically ascending by keys. Options: -r Recursively sort subobjects/submaps, e.g. for JSON input. Ad esempio a questo file, in cui i campi hanno ordine diverso eterogeneita_irregular.json { \"a\": 1, \"b\": 2, \"c\": 3 } { \"c\": 6, \"a\": 4, \"b\": 5 } { \"b\": 8, \"c\": 9, \"a\": 7 } si potr\u00e0 applicare questo comando per ordinare i campi secondo l'ordine alfabetico dei loro nomi mlr --json sort-within-records ./eterogeneita_irregular.json { \"a\": 1, \"b\": 2, \"c\": 3 } { \"a\": 4, \"b\": 5, \"c\": 6 } { \"a\": 7, \"b\": 8, \"c\": 9 } stats1 \u00b6 mlr stats1 --help Computes univariate statistics for one or more given fields, accumulated across the input record stream. Options: -a {sum,count,...} Names of accumulators: one or more of: median This is the same as p50 p10 p25.2 p50 p98 p100 etc. count Count instances of fields mode Find most-frequently-occurring values for fields; first-found wins tie antimode Find least-frequently-occurring values for fields; first-found wins tie sum Compute sums of specified fields mean Compute averages (sample means) of specified fields var Compute sample variance of specified fields stddev Compute sample standard deviation of specified fields meaneb Estimate error bars for averages (assuming no sample autocorrelation) skewness Compute sample skewness of specified fields kurtosis Compute sample kurtosis of specified fields min Compute minimum values of specified fields max Compute maximum values of specified fields -f {a,b,c} Value-field names on which to compute statistics --fr {regex} Regex for value-field names on which to compute statistics (compute statistics on values in all field names matching regex --fx {regex} Inverted regex for value-field names on which to compute statistics (compute statistics on values in all field names not matching regex) -g {d,e,f} Optional group-by-field names --gr {regex} Regex for optional group-by-field names (group by values in field names matching regex) --gx {regex} Inverted regex for optional group-by-field names (group by values in field names not matching regex) --grfx {regex} Shorthand for --gr {regex} --fx {that same regex} -i Use interpolated percentiles, like R's type=7; default like type=1. Not sensical for string-valued fields.\\n\"); -s Print iterative stats. Useful in tail -f contexts (in which case please avoid pprint-format output since end of input stream will never be seen). Example: mlr stats1 -a min,p10,p50,p90,max -f value -g size,shape Example: mlr stats1 -a count,mode -f size Example: mlr stats1 -a count,mode -f size -g shape Example: mlr stats1 -a count,mode --fr '^[a-h].*$' -gr '^k.*$' This computes count and mode statistics on all field names beginning with a through h, grouped by all field names starting with k. Notes: * p50 and median are synonymous. * min and max output the same results as p0 and p100, respectively, but use less memory. * String-valued data make sense unless arithmetic on them is required, e.g. for sum, mean, interpolated percentiles, etc. In case of mixed data, numbers are less than strings. * count and mode allow text input; the rest require numeric input. In particular, 1 and 1.0 are distinct text for count and mode. * When there are mode ties, the first-encountered datum wins. stats2 \u00b6 mlr stats2 --help Computes bivariate statistics for one or more given field-name pairs, accumulated across the input record stream. -a {linreg-ols,corr,...} Names of accumulators: one or more of: linreg-ols Linear regression using ordinary least squares linreg-pca Linear regression using principal component analysis r2 Quality metric for linreg-ols (linreg-pca emits its own) logireg Logistic regression corr Sample correlation cov Sample covariance covx Sample-covariance matrix -f {a,b,c,d} Value-field name-pairs on which to compute statistics. There must be an even number of names. -g {e,f,g} Optional group-by-field names. -v Print additional output for linreg-pca. -s Print iterative stats. Useful in tail -f contexts (in which case please avoid pprint-format output since end of input stream will never be seen). --fit Rather than printing regression parameters, applies them to the input data to compute new fit fields. All input records are held in memory until end of input stream. Has effect only for linreg-ols, linreg-pca, and logireg. Only one of -s or --fit may be used. Example: mlr stats2 -a linreg-pca -f x,y Example: mlr stats2 -a linreg-ols,r2 -f x,y -g size,shape Example: mlr stats2 -a corr -f x,y step \u00b6 mlr step --help Computes values dependent on the previous record, optionally grouped by category. Options: -a {delta,rsum,...} Names of steppers: comma-separated, one or more of: delta Compute differences in field(s) between successive records shift Include value(s) in field(s) from previous record, if any from-first Compute differences in field(s) from first record ratio Compute ratios in field(s) between successive records rsum Compute running sums of field(s) between successive records counter Count instances of field(s) between successive records ewma Exponentially weighted moving average over successive records -f {a,b,c} Value-field names on which to compute statistics -g {d,e,f} Optional group-by-field names -F Computes integerable things (e.g. counter) in floating point. As of Miller 6 this happens automatically, but the flag is accepted as a no-op for backward compatibility with Miller 5 and below. -d {x,y,z} Weights for ewma. 1 means current sample gets all weight (no smoothing), near under under 1 is light smoothing, near over 0 is heavy smoothing. Multiple weights may be specified, e.g. \"mlr step -a ewma -f sys_load -d 0.01,0.1,0.9\". Default if omitted is \"-d 0.5\". -o {a,b,c} Custom suffixes for EWMA output fields. If omitted, these default to the -d values. If supplied, the number of -o values must be the same as the number of -d values. Examples: mlr step -a rsum -f request_size mlr step -a delta -f request_size -g hostname mlr step -a ewma -d 0.1,0.9 -f x,y mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y -g group_name Please see https://miller.readthedocs.io/en/latest/reference-verbs.html#filter or https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average for more information on EWMA. tac \u00b6 mlr tac --help Prints records in reverse order from the order in which they were encountered. Options: tail \u00b6 mlr tail --help Passes through the last n records, optionally by category. Options: -g {a,b,c} Optional group-by-field names for head counts, e.g. a,b,c. -n {n} Head-count to print. Default 10. tee \u00b6 mlr tee --help Options: -a Append to existing file, if any, rather than overwriting. -p Treat filename as a pipe-to command. Any of the output-format command-line flags (see mlr -h). Example: using mlr --icsv --opprint put '...' then tee --ojson ./mytap.dat then stats1 ... the input is CSV, the output is pretty-print tabular, but the tee-file output is written in JSON format. template \u00b6 mlr template --help Places input-record fields in the order specified by list of column names. If the input record is missing a specified field, it will be filled with the fill-with. If the input record possesses an unspecified field, it will be discarded. Options: -f {a,b,c} Comma-separated field names for template, e.g. a,b,c. -t {filename} CSV file whose header line will be used for template. --fill-with {filler string} What to fill absent fields with. Defaults to the empty string. Example: * Specified fields are a,b,c. * Input record is c=3,a=1,f=6. * Output record is a=1,b=,c=3. top \u00b6 Restituisce i record con i valori pi\u00f9 grandi (o pi\u00f9 piccoli), per uno o pi\u00f9 campi, anche raggruppando per campi. mlr top --help -f {a,b,c} Value-field names for top counts. -g {d,e,f} Optional group-by-field names for top counts. -n {count} How many records to print per category; default 1. -a Print all fields for top-value records; default is to print only value and group-by fields. Requires a single value-field name only. --min Print top smallest values; default is top largest values. -F Keep top values as floats even if they look like integers. -o {name} Field name for output indices. Default \"top_idx\". Prints the n records with smallest/largest values at specified fields, optionally by category. Qui un file (senza intestazione, nel comando di sotto si usa infatti il flag -N ), in cui i primi 4 campi sono a volte duplicati in pi\u00f9 record. input.csv 1,861265,C,A,0.071 1,861265,C,A,0.148 1,861265,C,G,0.001 1,861265,C,G,0.108 1,861265,C,T,0 1,861265,C,T,0.216 2,193456,G,A,0.006 2,193456,G,A,0.094 2,193456,G,C,0.011 2,193456,G,C,0.152 2,193456,G,T,0.003 2,193456,G,T,0.056 Se se si vogliono estrarre le righe con il valore massimo del campo 5 , a parit\u00e0 di valori dei campi 1 , 2 , 3 , 4 , il comando sar\u00e0: mlr --csv -N top -f 5 -g 1,2,3,4 input.tsv 1,861265,C,A,1,0.148 1,861265,C,G,1,0.108 1,861265,C,T,1,0.216 2,193456,G,A,1,0.094 2,193456,G,C,1,0.152 2,193456,G,T,1,0.056 In output soltanto i campi definiti nel comando. Se si vogliono tutti, bisogna aggiungere -a unflatten \u00b6 mlr unflatten --help Reverses flatten. Example: field with name 'a.b.c' and value 4 becomes name 'a' and value '{\"b\": { \"c\": 4 }}'. Options: -f {a,b,c} Comma-separated list of field names to unflatten (default all). -s {string} Separator, defaulting to mlr --flatsep value. uniq \u00b6 mlr uniq --help Prints distinct values for specified field names. With -c, same as count-distinct. For uniq, -f is a synonym for -g. Options: -g {d,e,f} Group-by-field names for uniq counts. -c Show repeat counts in addition to unique values. -n Show only the number of distinct values. -o {name} Field name for output count. Default \"count\". -a Output each unique record only once. Incompatible with -g. With -c, produces unique records, with repeat counts for each. With -n, produces only one record which is the unique-record count. With neither -c nor -n, produces unique records. unsparsify \u00b6 mlr unsparsify --help Usage: mlr unsparsify [options] Prints records with the union of field names over all input records. For field names absent in a given record but present in others, fills in a value. This verb retains all input before producing any output. Options: --fill-with {filler string} What to fill absent fields with. Defaults to the empty string. -f {a,b,c} Specify field names to be operated on. Any other fields won't be modified, and operation will be streaming. -h|--help Show this message. Example: if the input is two records, one being 'a=1,b=2' and the other being 'b=3,c=4', then the output is the two records 'a=1,b=2,c=' and 'a=,b=3,c=4'. Alcuni formati, come il JSON , non devono avere lo stesso numero di campi per record. Un esempio il file a seguire: input.json [ { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"giulia\" , \"comuneNascita\" : \"Milano\" } ] Con unsparsify , viene di default prodotto un output in cui tutti i record hanno gli stessi campi, e l'input viene fatto diventare rettangolare (vedi eterogeneit\u00e0 dei record ); laddove erano assenti viene assegnato un valore nullo. mlr --json unsparsify input.json [ { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"giulia\" , \"dataNascita\" : \"\" , \"altezza\" : \"\" , \"peso\" : \"\" , \"comuneNascita\" : \"Milano\" } ]","title":"Verbi"},{"location":"miller/verbi/#verbi","text":"I verbi sono i sub comandi di Miller. Queste le categorie: quelli analoghi allo Unix-toolkit: cat , cut , grep , head , join , sort , tac , tail , top , uniq . quelli con funziolit\u00e0 simili a quelli di awk : filter , put , sec2gmt , sec2gmtdate , step , tee . quelli statistici: bar , bootstrap , decimate , histogram , least-frequent , most-frequent , sample , shuffle , stats1 , stats2 . quelli orientati all' eterogeneit\u00e0 dei record , sebbene tutti i verbi di Miller sono in grado di gestire record eterogenei: group-by , group-like , having-fields . e altri ancora: check , count-distinct , label , merge-fields , nest , nothing , regularize , rename , reorder , reshape , seqgen .","title":"Verbi"},{"location":"miller/verbi/#file-di-esempio","text":"File per sviluppare esempi Per la gran parte degli esempi sviluppati in questa pagina, verr\u00e0 usato il file base_category.csv (vedi sotto). \u00c8 stato scelto un file piccolo e semplice, per ragioni didattiche e di leggibilit\u00e0. nome dataNascita altezza peso comuneNascita andy 1973-05-08 176 86.5 Roma chiara 1993-12-13 162 58.3 Milano guido 2001-01-22 196 90.4 Roma sara 2000-02-22 166 70.4 Roma giulia 1997-08-13 169 68.3 Milano base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano","title":"File di esempio"},{"location":"miller/verbi/#guida-in-linea-per-un-verbo","text":"Per aprire la guida in linea di un verbo, basta lanciare mlr nomeVerbo --help . Ad esempio per il verbo cat , il comando mlr cat --help restituir\u00e0: Usage: mlr cat [options] Passes input records directly to output. Most useful for format conversion. Options: -n Prepend field \"n\" to each record with record-counter starting at 1 -g {comma-separated field name(s)} When used with -n/-N, writes record-counters keyed by specified field name(s). -v Write a low-level record-structure dump to stderr. -N {name} Prepend field {name} to each record with record-counter starting at 1 A seguire, per ogni verbo, sar\u00e0 inserito l' help ufficiale di ogni comando.","title":"Guida in linea per un verbo"},{"location":"miller/verbi/#lista-dei-verbi","text":"","title":"Lista dei verbi"},{"location":"miller/verbi/#altkv","text":"Mappa una lista di valori, come coppie alternate chiave/valore. mlr altkv --help Usage: mlr altkv [options] Given fields with values of the form a,b,c,d,e,f emits a=b,c=d,e=f pairs. Ad esempio echo \"a,b,c,d\" | mlr --ocsv altkv { \"a\" : \"b\" , \"c\" : \"d\" }","title":"altkv"},{"location":"miller/verbi/#bar","text":"Per creare dei grafici a barre (bruttini \ud83d\ude43), rimpiazzando dei valori numeri con una serie di asterichi. Per allinearli meglio si possono usare le opzioni di output --opprint o --oxtab . mlr bar --help Replaces a numeric field with a number of asterisks, allowing for cheesy bar plots. These align best with --opprint or --oxtab output format. Options: -f {a,b,c} Field names to convert to bars. --lo {lo} Lower-limit value for min-width bar: default '0.000000'. --hi {hi} Upper-limit value for max-width bar: default '100.000000'. -w {n} Bar-field width: default '40'. --auto Automatically computes limits, ignoring --lo and --hi. Holds all records in memory before producing any output. -c {character} Fill character: default '*'. -x {character} Out-of-bounds character: default '#'. -b {character} Blank character: default '.'. Nominally the fill, out-of-bounds, and blank characters will be strings of length 1. However you can make them all longer if you so desire. A partire ad esempio da questo file di input: bar.csv Area,percUtenti Nord,25 Centro,32 Sud e isole,43 mlr --c2p bar -f percUtenti ./bar.csv Area percUtenti Nord **********.............................. Centro ************............................ Sud e isole *****************.......................","title":"bar"},{"location":"miller/verbi/#bootstrap","text":"L'uso tipico di bootstrap \u00e8 quello di estrarre un campione random dall'input, con un numero di record pari al numero record di input; con doppioni possibili. mlr bootstrap --help Emits an n-sample, with replacement, of the input records. See also mlr sample and mlr shuffle. Options: -n Number of samples to output. Defaults to number of input records. Must be non-negative. Sotto un esempio in cui a partire dal file di input, composto da 5 record distinti, sono estratti in modo randomico 5 record. mlr --csv bootstrap base_category.csv nome,dataNascita,altezza,peso,comuneNascita guido,2001-01-22,196,90.4,Roma chiara,1993-12-13,162,58.3,Milano chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma chiara,1993-12-13,162,58.3,Milano","title":"bootstrap"},{"location":"miller/verbi/#cat","text":"Utile sopratutto per conversione di formato (vedi formati ) e per concatenare in \"verticale\" file con lo stesso schema ( ricetta ). mlr cat --help Usage: mlr cat [options] Passes input records directly to output. Most useful for format conversion. Options: -n Prepend field \"n\" to each record with record-counter starting at 1 -g {comma-separated field name(s)} When used with -n/-N, writes record-counters keyed by specified field name(s). -v Write a low-level record-structure dump to stderr. -N {name} Prepend field {name} to each record with record-counter starting at 1 Il comando di base stampa a schermo il contenuto di un file: mlr --csv cat base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano Con l'opzione -n si aggiunge un campo con un progressivo numerico intero che parte da 1 : mlr --csv -n cat base.csv n,nome,dataNascita,altezza,peso,comuneNascita 1,andy,1973-05-08,176,86.5,Roma 2,chiara,1993-12-13,162,58.3,Milano 3,guido,2001-01-22,196,90.4,Roma 4,sara,2000-02-22,166,70.4,Roma 5,giulia,1997-08-13,169,68.3,Milano Se all'opzione -n si aggiunge -g seguito da dal nome di uno o pi\u00f9 campi, il \"contatore\" sar\u00e0 applicato per gruppo e partir\u00e0 da 1 distintamente per ogni gruppo. Qui sotto ad esempio il raggruppamento \u00e8 fatto per comuneNascita : mlr --csv cat -n -g comuneNascita base_category.csv n,nome,dataNascita,altezza,peso,comuneNascita 1,andy,1973-05-08,176,86.5,Roma 1,chiara,1993-12-13,162,58.3,Milano 2,guido,2001-01-22,196,90.4,Roma 3,sara,2000-02-22,166,70.4,Roma 2,giulia,1997-08-13,169,68.3,Milano","title":"cat"},{"location":"miller/verbi/#check","text":"\u00c8 un verbo che non produce un output, salvo un report sulla correttezza di \"formattazione\" del file di input. mlr check --help Consumes records without printing any output. Useful for doing a well-formatted check on input data. Options: Se ad esempio si ha un file CSV , con un numero di campi nell'intestazione ( 4 ) diverso da quello del corpo ( 5 ), come questo check.csv nome,dataNascita,altezza,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma si avr\u00e0 l'output sottostante mlr --csv check check.csv mlr : mlr: CSV header/data length mismatch 4 != 5 at filename ./check.csv row 2.","title":"check"},{"location":"miller/verbi/#clean-whitespace","text":"\u00c8 un prezioso comando, per \"pulire\" i dati: rimuove gli spazi ridondanti. mlr clean-whitespace --help For each record, for each field in the record, whitespace-cleans the keys and/or values. Whitespace-cleaning entails stripping leading and trailing whitespace, and replacing multiple whitespace with singles. For finer-grained control, please see the DSL functions lstrip, rstrip, strip, collapse_whitespace, and clean_whitespace. Options: -k|--keys-only Do not touch values. -v|--values-only Do not touch keys. It is an error to specify -k as well as -v -- to clean keys and values, leave off -k as well as -v. Riuove in particolare uno o pi\u00f9 spazi bianchi a inizio e fine cella e due o pi\u00f9 spazi all'interndo della cella. Ad esempio nel file sottostante ci sono due spazi tra Busto e Arsizio , uno spazio a fine cella dopo andy e uno a inizio cella i corrispondenza di chiara . Questi sono ridondanti, e nella grandissima parte dei casi sono da rimuovere. clean-whitespace.json { \"nome\" : \"andy \" , \"dataNascita\" : \"1973-05-08\" , \"comuneNascita\" : \"Roma\" } { \"nome\" : \" chiara\" , \"dataNascita\" : \"1993-12-13\" , \"comuneNascita\" : \"Busto Arsizio\" } Per pulire il file: mlr --json clean-whitespace input.json { \"nome\" : \"andy\" , \"dataNascita\" : \"1973-05-08\" , \"comuneNascita\" : \"Roma\" } { \"nome\" : \"chiara\" , \"dataNascita\" : \"1993-12-13\" , \"comuneNascita\" : \"Busto Arsizio\" } Due comode opzioni: -k fa la pulizia soltanto nei nomi dei campi, nelle chiavi; -v fa la pulizia soltanto nei valori.","title":"clean-whitespace"},{"location":"miller/verbi/#count","text":"Restituisce il numero di record. Miller tiene conto del formato, quindi per un CSV composto da 6 righe, 1 di intestazione pi\u00f9 5 di dati, restituir\u00e0 5. mlr count --help Prints number of records, optionally grouped by distinct values for specified field names. Options: -g {a,b,c} Optional group-by-field names for counts, e.g. a,b,c -n {n} Show only the number of distinct values. Not interesting without -g. -o {name} Field name for output-count. Default \"count\". File di esempio: base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano Comando di esempio: mlr --csv count ./base_category.csv count 5 Se si desidera soltanto l'output numerico, senza riga di intestazione, si pu\u00f2 fare in tantissimi modi. Uno \u00e8 quello di cambiare il formato di output in NIDX : mlr --c2n count ./base_category.csv 5","title":"count"},{"location":"miller/verbi/#count-distinct","text":"Restituisce il numero di record che hanno valori distinti, per uno o pi\u00f9 campi specificati. mlr count-distinct --help Prints number of records having distinct values for specified field names. Same as uniq -c. Options: -f {a,b,c} Field names for distinct count. -n Show only the number of distinct values. Not compatible with -u. -o {name} Field name for output count. Default \"count\". Ignored with -u. -u Do unlashed counts for multiple field names. With -f a,b and without -u, computes counts for distinct combinations of a and b field values. With -f a,b and with -u, computes counts for distinct a field values and counts for distinct b field values separately. File di esempio: count-distinct.csv nome,dataNascita,altezza,peso,comuneNascita,sesso andy,1973-05-08,176,86.5,Roma,maschio chiara,1993-12-13,162,58.3,Milano,femmina guido,2001-01-22,196,90.4,Roma,maschio sara,2000-02-22,166,70.4,Roma,femmina giulia,1997-08-13,169,68.3,Milano,femmina Ad esempio il conteggio per combinazioni distinte di comune di nascita e sesso: mlr --csv count-distinct -f comuneNascita,sesso -o conteggio ./base_category.csv comuneNascita,sesso,conteggio Roma,maschio,2 Milano,femmina,2 Roma,femmina,1 Aggiungendo il parametro -u , si ottengono i valori distinti non per combinazioni distinti dei campi indicati, ma per ogni singolo campo. mlr --csv count-distinct -f comuneNascita,sesso -o conteggio -u ./base_category.csv field,value,count comuneNascita,Roma,3 comuneNascita,Milano,2 sesso,maschio,2 sesso,femmina,3","title":"count-distinct"},{"location":"miller/verbi/#count-similar","text":"Aggiunge un campo, con il conteggio dei record che ha lo stesso valore per uno o pi\u00f9 campi. mlr count-similar --help Ingests all records, then emits each record augmented by a count of the number of other records having the same group-by field values. Options: -g {a,b,c} Group-by-field names for counts, e.g. a,b,c -o {name} Field name for output-counts. Defaults to \"count\". File di esempio: base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano A seguire ad esempio viene aggiunto il campo conteggio al file di input, con il conteggio dei valori distinti per il campo comuneNascita . mlr --csv count-similar -g comuneNascita -o conteggio ./base_category.csv nome,dataNascita,altezza,peso,comuneNascita,conteggio andy,1973-05-08,176,86.5,Roma,3 guido,2001-01-22,196,90.4,Roma,3 sara,2000-02-22,166,70.4,Roma,3 chiara,1993-12-13,162,58.3,Milano,2 giulia,1997-08-13,169,68.3,Milano,2","title":"count-similar"},{"location":"miller/verbi/#cut","text":"Estrae/rimuove dall'input uno o pi\u00f9 campi. mlr cut --help Passes through input records with specified fields included/excluded. Options: -f {a,b,c} Comma-separated field names for cut, e.g. a,b,c. -o Retain fields in the order specified here in the argument list. Default is to retain them in the order found in the input data. -x|--complement Exclude, rather than include, field names specified by -f. -r Treat field names as regular expressions. \"ab\", \"a.*b\" will match any field name containing the substring \"ab\" or matching \"a.*b\", respectively; anchors of the form \"^ab$\", \"^a.*b$\" may be used. The -o flag is ignored when -r is present. Examples: mlr cut -f hostname,status mlr cut -x -f hostname,status mlr cut -r -f '^status$,sda[0-9]' mlr cut -r -f '^status$,\"sda[0-9]\"' mlr cut -r -f '^status$,\"sda[0-9]\"i' (this is case-insensitive) File di esempio: base_category.csv nome,dataNascita,altezza,peso,comuneNascita andy,1973-05-08,176,86.5,Roma chiara,1993-12-13,162,58.3,Milano guido,2001-01-22,196,90.4,Roma sara,2000-02-22,166,70.4,Roma giulia,1997-08-13,169,68.3,Milano Per estrarre soltanto il campo nome : mlr --csv cut -f nome base_category.csv nome andy chiara guido sara giulia Per rimuovere il campo nome , bisogner\u00e0 aggiungere l'opzione -x : mlr --csv cut -x -f nome base_category.csv dataNascita,altezza,peso,comuneNascita 1973-05-08,176,86.5,Roma 1993-12-13,162,58.3,Milano 2001-01-22,196,90.4,Roma 2000-02-22,166,70.4,Roma 1997-08-13,169,68.3,Milano Per impostare il filtro tramite espressione regolare si usa l'opzione -r . Ad esempio estrarre soltanto i campi che iniziano per a e che terminano per o : mlr --csv cut -r -f \"^a\",\"o$\" base_category.csv altezza,peso 176,86.5 162,58.3 196,90.4 166,70.4 169,68.3","title":"cut"},{"location":"miller/verbi/#decimate","text":"Estrae un record ogni n , opzionalmente per categoria. mlr decimate --help Passes through one of every n records, optionally by category. Options: -b Decimate by printing first of every n. -e Decimate by printing last of every n (default). -g {a,b,c} Optional group-by-field names for decimate counts, e.g. a,b,c. -n {n} Decimation factor (default 10).","title":"decimate"},{"location":"miller/verbi/#fill-down","text":"Se un dato record ha un valore mancante per un dato campo, verr\u00e0 riempito dal valore corrispondente del record precedente, se presente. mlr fill-down --help If a given record has a missing value for a given field, fill that from the corresponding value from a previous record, if any. By default, a 'missing' field either is absent, or has the empty-string value. With -a, a field is 'missing' only if it is absent. Options: --all Operate on all fields in the input. -a|--only-if-absent If a given record has a missing value for a given field, fill that from the corresponding value from a previous record, if any. By default, a 'missing' field either is absent, or has the empty-string value. With -a, a field is 'missing' only if it is absent. -f Field names for fill-down. File di esempio: fill-down.csv a,b,c 1,,3 4,5, 7,,9 Con l'opzione -all il verbo viene applicato per tutti i record: mlr --csv fill-down --all fill-down.csv a,b,c 1,,3 4,5,3 7,5,9 Con -f \u00e8 possibile scegliere di applicaro a 1 o pi\u00f9 campi.","title":"fill-down"},{"location":"miller/verbi/#fill-empty","text":"Il verbo fill-empty richiede Miller >= 6.0 Riempe le celle vuote, con un valore specifico mlr fill-empty --help Fills empty-string fields with specified fill-value. Options: -v {string} Fill-value: defaults to \"N/A\" -S Don't infer type -- so '-v 0' would fill string 0 not int 0. File di esempio: fill-down.csv a,b,c 1,,3 4,5, 7,,9 Come impopstazione predefinita, il valore assegnato sar\u00e0 N/A , ma \u00e8 possibile impostarlo con -v : mlr --csv fill-empty fill-down.csv a,b,c 1,N/A,3 4,5,N/A 7,N/A,9","title":"fill-empty"},{"location":"miller/verbi/#filter","text":"mlr filter --help Options: -f {file name} File containing a DSL expression (see examples below). If the filename is a directory, all *.mlr files in that directory are loaded. -e {expression} You can use this after -f to add an expression. Example use case: define functions/subroutines in a file you specify with -f, then call them with an expression you specify with -e. (If you mix -e and -f then the expressions are evaluated in the order encountered. Since the expression pieces are simply concatenated, please be sure to use intervening semicolons to separate expressions.) -s name=value: Predefines out-of-stream variable @name to have Thus mlr put -s foo=97 '$column += @foo' is like mlr put 'begin {@foo = 97} $column += @foo'. The value part is subject to type-inferencing. May be specified more than once, e.g. -s name1=value1 -s name2=value2. Note: the value may be an environment variable, e.g. -s sequence=$SEQUENCE -x (default false) Prints records for which {expression} evaluates to false, not true, i.e. invert the sense of the filter expression. -q Does not include the modified record in the output stream. Useful for when all desired output is in begin and/or end blocks. -S and -F: There are no-ops in Miller 6 and above, since now type-inferencing is done by the record-readers before filter/put is executed. Supported as no-op pass-through flags for backward compatibility. Parser-info options: -w Print warnings about things like uninitialized variables. -W Same as -w, but exit the process if there are any warnings. -p Prints the expressions's AST (abstract syntax tree), which gives full transparency on the precedence and associativity rules of Miller's grammar, to stdout. -d Like -p but uses a parenthesized-expression format for the AST. -D Like -d but with output all on one line. -E Echo DSL expression before printing parse-tree -v Same as -E -p. -X Exit after parsing but before stream-processing. Useful with -v/-d/-D, if you only want to look at parser information. Records will pass the filter depending on the last bare-boolean statement in the DSL expression. That can be the result of <, ==, >, etc., the return value of a function call which returns boolean, etc. Examples: mlr --csv --from example.csv filter '$color == \"red\"' mlr --csv --from example.csv filter '$color == \"red\" && flag == true' More example filter expressions: First record in each file: 'FNR == 1' Subsampling: 'urand() < 0.001' Compound booleans: '$color != \"blue\" && $value > 4.2' '($x < 0.5 && $y < 0.5) || ($x > 0.5 && $y > 0.5)' Regexes with case-insensitive flag '($name =~ \"^sys.*east$\") || ($name =~ \"^dev.[0-9]+\"i)' Assignments, then bare-boolean filter statement: '$ab = $a+$b; $cd = $c+$d; $ab != $cd' Bare-boolean filter statement within a conditional: 'if (NR < 100) { $x > 0.3; } else { $x > 0.002; } ' Using 'any' higher-order function to see if $index is 10, 20, or 30: 'any([10,20,30], func(e) {return $index == e})' See also https://miller.readthedocs.io/reference-dsl for more context.","title":"filter"},{"location":"miller/verbi/#flatten","text":"mlr flatten --help Flattens multi-level maps to single-level ones. Example: field with name 'a' and value '{\"b\": { \"c\": 4 }}' becomes name 'a.b.c' and value 4. Options: -f Comma-separated list of field names to flatten (default all). -s Separator, defaulting to mlr --flatsep value.","title":"flatten"},{"location":"miller/verbi/#format-values","text":"mlr format-values --help Applies format strings to all field values, depending on autodetected type. * If a field value is detected to be integer, applies integer format. * Else, if a field value is detected to be float, applies float format. * Else, applies string format. Note: this is a low-keystroke way to apply formatting to many fields. To get finer control, please see the fmtnum function within the mlr put DSL. Note: this verb lets you apply arbitrary format strings, which can produce undefined behavior and/or program crashes. See your system's \"man printf\". Options: -i {integer format} Defaults to \"%d\". Examples: \"%06lld\", \"%08llx\". Note that Miller integers are long long so you must use formats which apply to long long, e.g. with ll in them. Undefined behavior results otherwise. -f {float format} Defaults to \"%f\". Examples: \"%8.3lf\", \"%.6le\". Note that Miller floats are double-precision so you must use formats which apply to double, e.g. with l[efg] in them. Undefined behavior results otherwise. -s {string format} Defaults to \"%s\". Examples: \"_%s\", \"%08s\". Note that you must use formats which apply to string, e.g. with s in them. Undefined behavior results otherwise. -n Coerce field values autodetected as int to float, and then apply the float format.","title":"format-values"},{"location":"miller/verbi/#fraction","text":"mlr fraction --help For each record's value in specified fields, computes the ratio of that value to the sum of values in that field over all input records. E.g. with input records x=1 x=2 x=3 and x=4, emits output records x=1,x_fraction=0.1 x=2,x_fraction=0.2 x=3,x_fraction=0.3 and x=4,x_fraction=0.4 Note: this is internally a two-pass algorithm: on the first pass it retains input records and accumulates sums; on the second pass it computes quotients and emits output records. This means it produces no output until all input is read. Options: -f {a,b,c} Field name(s) for fraction calculation -g {d,e,f} Optional group-by-field name(s) for fraction counts -p Produce percents [0..100], not fractions [0..1]. Output field names end with \"_percent\" rather than \"_fraction\" -c Produce cumulative distributions, i.e. running sums: each output value folds in the sum of the previous for the specified group E.g. with input records x=1 x=2 x=3 and x=4, emits output records x=1,x_cumulative_fraction=0.1 x=2,x_cumulative_fraction=0.3 x=3,x_cumulative_fraction=0.6 and x=4,x_cumulative_fraction=1.0","title":"fraction"},{"location":"miller/verbi/#gap","text":"mlr gap --help Emits an empty record every n records, or when certain values change. Options: Emits an empty record every n records, or when certain values change. -g {a,b,c} Print a gap whenever values of these fields (e.g. a,b,c) changes. -n {n} Print a gap every n records. One of -f or -g is required. -n is ignored if -g is present.","title":"gap"},{"location":"miller/verbi/#grep","text":"mlr grep --help Passes through records which match the regular expression. Options: -i Use case-insensitive search. -v Invert: pass through records which do not match the regex. Note that \"mlr filter\" is more powerful, but requires you to know field names. By contrast, \"mlr grep\" allows you to regex-match the entire record. It does this by formatting each record in memory as DKVP, using command-line-specified ORS/OFS/OPS, and matching the resulting line against the regex specified here. In particular, the regex is not applied to the input stream: if you have CSV with header line \"x,y,z\" and data line \"1,2,3\" then the regex will be matched, not against either of these lines, but against the DKVP line \"x=1,y=2,z=3\". Furthermore, not all the options to system grep are supported, and this command is intended to be merely a keystroke-saver. To get all the features of system grep, you can do \"mlr --odkvp ... | grep ... | mlr --idkvp ...\"","title":"grep"},{"location":"miller/verbi/#group-by","text":"mlr group-by --help Outputs records in batches having identical values at specified field names.Options:","title":"group-by"},{"location":"miller/verbi/#group-like","text":"mlr group-like --help Outputs records in batches having identical field names. Options:","title":"group-like"},{"location":"miller/verbi/#having-fields","text":"mlr having-fields --help Conditionally passes through records depending on each record's field names. Options: --at-least {comma-separated names} --which-are {comma-separated names} --at-most {comma-separated names} --all-matching {regular expression} --any-matching {regular expression} --none-matching {regular expression} Examples: mlr having-fields --which-are amount,status,owner mlr having-fields --any-matching 'sda[0-9]' mlr having-fields --any-matching '\"sda[0-9]\"' mlr having-fields --any-matching '\"sda[0-9]\"i' (this is case-insensitive)","title":"having-fields"},{"location":"miller/verbi/#head","text":"mlr head --help Passes through the first n records, optionally by category. Without -g, ceases consuming more input (i.e. is fast) when n records have been read. Options: -g {a,b,c} Optional group-by-field names for head counts, e.g. a,b,c. -n {n} Head-count to print. Default 10.","title":"head"},{"location":"miller/verbi/#histogram","text":"mlr histogram --help Just a histogram. Input values < lo or > hi are not counted. -f {a,b,c} Value-field names for histogram counts --lo {lo} Histogram low value --hi {hi} Histogram high value --nbins {n} Number of histogram bins. Defaults to 20. --auto Automatically computes limits, ignoring --lo and --hi. Holds all values in memory before producing any output. -o {prefix} Prefix for output field name. Default: no prefix.","title":"histogram"},{"location":"miller/verbi/#json-parse","text":"mlr json-parse --help Tries to convert string field values to parsed JSON, e.g. \"[1,2,3]\" -> [1,2,3]. Options: -f {...} Comma-separated list of field names to json-parse (default all).","title":"json-parse"},{"location":"miller/verbi/#json-stringify","text":"mlr json-stringify --help Produces string field values from field-value data, e.g. [1,2,3] -> \"[1,2,3]\". Options: -f {...} Comma-separated list of field names to json-parse (default all). --jvstack Produce multi-line JSON output. --no-jvstack Produce single-line JSON output per record (default).","title":"json-stringify"},{"location":"miller/verbi/#join","text":"mlr join --help Joins records from specified left file name with records from all file names at the end of the Miller argument list. Functionality is essentially the same as the system \"join\" command, but for record streams. Options: -f {left file name} -j {a,b,c} Comma-separated join-field names for output -l {a,b,c} Comma-separated join-field names for left input file; defaults to -j values if omitted. -r {a,b,c} Comma-separated join-field names for right input file(s); defaults to -j values if omitted. --lp {text} Additional prefix for non-join output field names from the left file --rp {text} Additional prefix for non-join output field names from the right file(s) --np Do not emit paired records --ul Emit unpaired records from the left file --ur Emit unpaired records from the right file(s) -s|--sorted-input Require sorted input: records must be sorted lexically by their join-field names, else not all records will be paired. The only likely use case for this is with a left file which is too big to fit into system memory otherwise. -u Enable unsorted input. (This is the default even without -u.) In this case, the entire left file will be loaded into memory. If you wish to use a prepipe command for the main input as well as here, it must be specified there as well as here. --prepipex {command} Likewise. File-format options default to those for the right file names on the Miller argument list, but may be overridden for the left file as follows. Please see -i {one of csv,dkvp,nidx,pprint,xtab} --irs {record-separator character} --ifs {field-separator character} --ips {pair-separator character} --repifs --implicit-csv-header --no-implicit-csv-header For example, if you have 'mlr --csv ... join -l foo ... ' then the left-file format will be specified CSV as well unless you override with 'mlr --csv ... join --ijson -l foo' etc. Likewise, if you have 'mlr --csv --implicit-csv-header ...' then the join-in file will be expected to be headerless as well unless you put '--no-implicit-csv-header' after 'join'. Please use \"mlr --usage-separator-options\" for information on specifying separators. Please see https://miller.readthedocs.io/en/latest/reference-verbs.html#join for more information including examples.","title":"join"},{"location":"miller/verbi/#label","text":"mlr label --help Given n comma-separated names, renames the first n fields of each record to have the respective name. (Fields past the nth are left with their original names.) Particularly useful with --inidx or --implicit-csv-header, to give useful names to otherwise integer-indexed fields. Options:","title":"label"},{"location":"miller/verbi/#least-frequent","text":"mlr least-frequent --help Shows the least frequently occurring distinct values for specified field names. The first entry is the statistical anti-mode; the remaining are runners-up. Options: -f {one or more comma-separated field names}. Required flag. -n {count}. Optional flag defaulting to 10. -b Suppress counts; show only field values. -o {name} Field name for output count. Default \"count\". See also \"mlr most-frequent\".","title":"least-frequent"},{"location":"miller/verbi/#merge-fields","text":"mlr merge-fields --help Computes univariate statistics for each input record, accumulated across specified fields. Options: -a {sum,count,...} Names of accumulators. One or more of: count Count instances of fields mode Find most-frequently-occurring values for fields; first-found wins tie antimode Find least-frequently-occurring values for fields; first-found wins tie sum Compute sums of specified fields mean Compute averages (sample means) of specified fields var Compute sample variance of specified fields stddev Compute sample standard deviation of specified fields meaneb Estimate error bars for averages (assuming no sample autocorrelation) skewness Compute sample skewness of specified fields kurtosis Compute sample kurtosis of specified fields min Compute minimum values of specified fields max Compute maximum values of specified fields -f {a,b,c} Value-field names on which to compute statistics. Requires -o. -r {a,b,c} Regular expressions for value-field names on which to compute statistics. Requires -o. -c {a,b,c} Substrings for collapse mode. All fields which have the same names after removing substrings will be accumulated together. Please see examples below. -i Use interpolated percentiles, like R's type=7; default like type=1. Not sensical for string-valued fields. -o {name} Output field basename for -f/-r. -k Keep the input fields which contributed to the output statistics; the default is to omit them. String-valued data make sense unless arithmetic on them is required, e.g. for sum, mean, interpolated percentiles, etc. In case of mixed data, numbers are less than strings. Example input data: \"a_in_x=1,a_out_x=2,b_in_y=4,b_out_x=8\". Example: mlr merge-fields -a sum,count -f a_in_x,a_out_x -o foo produces \"b_in_y=4,b_out_x=8,foo_sum=3,foo_count=2\" since \"a_in_x,a_out_x\" are summed over. Example: mlr merge-fields -a sum,count -r in_,out_ -o bar produces \"bar_sum=15,bar_count=4\" since all four fields are summed over. Example: mlr merge-fields -a sum,count -c in_,out_ produces \"a_x_sum=3,a_x_count=2,b_y_sum=4,b_y_count=1,b_x_sum=8,b_x_count=1\" since \"a_in_x\" and \"a_out_x\" both collapse to \"a_x\", \"b_in_y\" collapses to \"b_y\", and \"b_out_x\" collapses to \"b_x\".","title":"merge-fields"},{"location":"miller/verbi/#most-frequent","text":"mlr most-frequent --help Shows the most frequently occurring distinct values for specified field names. The first entry is the statistical mode; the remaining are runners-up. Options: -f {one or more comma-separated field names}. Required flag. -n {count}. Optional flag defaulting to 10. -b Suppress counts; show only field values. -o {name} Field name for output count. Default \"count\". See also \"mlr least-frequent\".","title":"most-frequent"},{"location":"miller/verbi/#nest","text":"mlr nest --help Explodes specified field values into separate fields/records, or reverses this. Options: --explode,--implode One is required. --values,--pairs One is required. --across-records,--across-fields One is required. -f {field name} Required. --nested-fs {string} Defaults to \";\". Field separator for nested values. --nested-ps {string} Defaults to \":\". Pair separator for nested key-value pairs. --evar {string} Shorthand for --explode --values ---across-records --nested-fs {string} --ivar {string} Shorthand for --implode --values ---across-records --nested-fs {string} Please use \"mlr --usage-separator-options\" for information on specifying separators. Examples: mlr nest --explode --values --across-records -f x with input record \"x=a;b;c,y=d\" produces output records \"x=a,y=d\" \"x=b,y=d\" \"x=c,y=d\" Use --implode to do the reverse. mlr nest --explode --values --across-fields -f x with input record \"x=a;b;c,y=d\" produces output records \"x_1=a,x_2=b,x_3=c,y=d\" Use --implode to do the reverse. mlr nest --explode --pairs --across-records -f x with input record \"x=a:1;b:2;c:3,y=d\" produces output records \"a=1,y=d\" \"b=2,y=d\" \"c=3,y=d\" mlr nest --explode --pairs --across-fields -f x with input record \"x=a:1;b:2;c:3,y=d\" produces output records \"a=1,b=2,c=3,y=d\" Notes: * With --pairs, --implode doesn't make sense since the original field name has been lost. * The combination \"--implode --values --across-records\" is non-streaming: no output records are produced until all input records have been read. In particular, this means it won't work in tail -f contexts. But all other flag combinations result in streaming (tail -f friendly) data processing. * It's up to you to ensure that the nested-fs is distinct from your data's IFS: e.g. by default the former is semicolon and the latter is comma. See also mlr reshape.","title":"nest"},{"location":"miller/verbi/#nothing","text":"mlr nothing --help Drops all input records. Useful for testing, or after tee/print/etc. have produced other output. Options:","title":"nothing"},{"location":"miller/verbi/#put","text":"mlr put --help Options: -f {file name} File containing a DSL expression (see examples below). If the filename is a directory, all *.mlr files in that directory are loaded. -e {expression} You can use this after -f to add an expression. Example use case: define functions/subroutines in a file you specify with -f, then call them with an expression you specify with -e. (If you mix -e and -f then the expressions are evaluated in the order encountered. Since the expression pieces are simply concatenated, please be sure to use intervening semicolons to separate expressions.) -s name=value: Predefines out-of-stream variable @name to have Thus mlr put -s foo=97 '$column += @foo' is like mlr put 'begin {@foo = 97} $column += @foo'. The value part is subject to type-inferencing. May be specified more than once, e.g. -s name1=value1 -s name2=value2. Note: the value may be an environment variable, e.g. -s sequence=$SEQUENCE -x (default false) Prints records for which {expression} evaluates to false, not true, i.e. invert the sense of the filter expression. -q Does not include the modified record in the output stream. Useful for when all desired output is in begin and/or end blocks. -S and -F: There are no-ops in Miller 6 and above, since now type-inferencing is done by the record-readers before filter/put is executed. Supported as no-op pass-through flags for backward compatibility. Parser-info options: -w Print warnings about things like uninitialized variables. -W Same as -w, but exit the process if there are any warnings. -p Prints the expressions's AST (abstract syntax tree), which gives full transparency on the precedence and associativity rules of Miller's grammar, to stdout. -d Like -p but uses a parenthesized-expression format for the AST. -D Like -d but with output all on one line. -E Echo DSL expression before printing parse-tree -v Same as -E -p. -X Exit after parsing but before stream-processing. Useful with -v/-d/-D, if you only want to look at parser information. Examples: mlr --from example.csv put '$qr = $quantity * $rate' More example put expressions: If-statements: 'if ($flag == true) { $quantity *= 10}' 'if ($x > 0.0 { $y=log10($x); $z=sqrt($y) } else {$y = 0.0; $z = 0.0}' Newly created fields can be read after being written: '$new_field = $index**2; $qn = $quantity * $new_field' Regex-replacement: '$name = sub($name, \"http.*com\"i, \"\")' Regex-capture: 'if ($a =~ \"([a-z]+)_([0-9]+)) { $b = \"left_\\1\"; $c = \"right_\\2\" }' Built-in variables: '$filename = FILENAME' Aggregations (use mlr put -q): '@sum += $x; end {emit @sum}' '@sum[$shape] += $quantity; end {emit @sum, \"shape\"}' '@sum[$shape][$color] += $x; end {emit @sum, \"shape\", \"color\"}' ' @min = min(@min,$x); @max=max(@max,$x); end{emitf @min, @max} ' See also https://miller.readthedocs.io/reference-dsl for more context.","title":"put"},{"location":"miller/verbi/#regularize","text":"Il verbo regularize riordina le righe nello stesso ordine della prima (qualunque sia l'ordine). mlr regularize --help Outputs records sorted lexically ascending by keys. Options:","title":"regularize"},{"location":"miller/verbi/#remove-empty-columns","text":"mlr remove-empty-columns --help Omits fields which are empty on every input row. Non-streaming. Options:","title":"remove-empty-columns"},{"location":"miller/verbi/#rename","text":"mlr rename --help Renames specified fields. Options: -r Treat old field names as regular expressions. \"ab\", \"a.*b\" will match any field name containing the substring \"ab\" or matching \"a.*b\", respectively; anchors of the form \"^ab$\", \"^a.*b$\" may be used. New field names may be plain strings, or may contain capture groups of the form \"\\1\" through \"\\9\". Wrapping the regex in double quotes is optional, but is required if you wish to follow it with 'i' to indicate case-insensitivity. -g Do global replacement within each field name rather than first-match replacement. Examples: mlr rename old_name,new_name' mlr rename old_name_1,new_name_1,old_name_2,new_name_2' mlr rename -r 'Date_[0-9]+,Date,' Rename all such fields to be \"Date\" mlr rename -r '\"Date_[0-9]+\",Date' Same mlr rename -r 'Date_([0-9]+).*,\\1' Rename all such fields to be of the form 20151015 mlr rename -r '\"name\"i,Name' Rename \"name\", \"Name\", \"NAME\", etc. to \"Name\"","title":"rename"},{"location":"miller/verbi/#reorder","text":"mlr reorder --help Moves specified names to start of record, or end of record. Options: -e Put specified field names at record end: default is to put them at record start. -f {a,b,c} Field names to reorder. -b {x} Put field names specified with -f before field name specified by {x}, if any. If {x} isn't present in a given record, the specified fields will not be moved. -a {x} Put field names specified with -f after field name specified by {x}, if any. If {x} isn't present in a given record, the specified fields will not be moved. Examples: mlr reorder -f a,b sends input record \"d=4,b=2,a=1,c=3\" to \"a=1,b=2,d=4,c=3\". mlr reorder -e -f a,b sends input record \"d=4,b=2,a=1,c=3\" to \"d=4,c=3,a=1,b=2\".","title":"reorder"},{"location":"miller/verbi/#repeat","text":"mlr repeat --help Copies input records to output records multiple times. Options must be exactly one of the following: -n {repeat count} Repeat each input record this many times. -f {field name} Same, but take the repeat count from the specified field name of each input record. Example: echo x=0 | mlr repeat -n 4 then put '$x=urand()' produces: x=0.488189 x=0.484973 x=0.704983 x=0.147311 Example: echo a=1,b=2,c=3 | mlr repeat -f b produces: a=1,b=2,c=3 a=1,b=2,c=3 Example: echo a=1,b=2,c=3 | mlr repeat -f c produces: a=1,b=2,c=3 a=1,b=2,c=3 a=1,b=2,c=3","title":"repeat"},{"location":"miller/verbi/#reshape","text":"Trasforma lo schema da wide a long e viceversa. Vedi approfondimento . mlr reshape --help Wide-to-long options: -i {input field names} -o {key-field name,value-field name} -r {input field regexes} -o {key-field name,value-field name} These pivot/reshape the input data such that the input fields are removed and separate records are emitted for each key/value pair. Note: this works with tail -f and produces output records for each input record seen. Long-to-wide options: -s {key-field name,value-field name} These pivot/reshape the input data to undo the wide-to-long operation. Note: this does not work with tail -f; it produces output records only after all input records have been read. Ad esempio da wide Studente Scuola Matematica Italiano Andy Liceo Cannizzaro 7 6 Lisa Liceo Garibaldi 6 7 Giovanna Liceo Garibaldi 7 7 a long Studente Scuola materia voto Andy Liceo Cannizzaro Matematica 7 Andy Liceo Cannizzaro Italiano 6 Lisa Liceo Garibaldi Matematica 6 Lisa Liceo Garibaldi Italiano 7 Giovanna Liceo Garibaldi Matematica 7 Giovanna Liceo Garibaldi Italiano 7","title":"reshape"},{"location":"miller/verbi/#sample","text":"mlr sample --help Reservoir sampling (subsampling without replacement), optionally by category. See also mlr bootstrap and mlr shuffle. Options: -g {a,b,c} Optional: group-by-field names for samples, e.g. a,b,c. -k {k} Required: number of records to output in total, or by group if using -g.","title":"sample"},{"location":"miller/verbi/#sec2gmtdate","text":"mlr sec2gmtdate --help Replaces a numeric field representing seconds since the epoch with the corresponding GMT year-month-day timestamp; leaves non-numbers as-is. This is nothing more than a keystroke-saver for the sec2gmtdate function: ../c/mlr sec2gmtdate time1,time2 is the same as ../c/mlr put '$time1=sec2gmtdate($time1);$time2=sec2gmtdate($time2)'","title":"sec2gmtdate"},{"location":"miller/verbi/#sec2gmt","text":"mlr sec2gmt --help Replaces a numeric field representing seconds since the epoch with the corresponding GMT timestamp; leaves non-numbers as-is. This is nothing more than a keystroke-saver for the sec2gmt function: mlr sec2gmt time1,time2 is the same as mlr put '$time1 = sec2gmt($time1); $time2 = sec2gmt($time2)' Options: -1 through -9: format the seconds using 1..9 decimal places, respectively. --millis Input numbers are treated as milliseconds since the epoch. --micros Input numbers are treated as microseconds since the epoch. --nanos Input numbers are treated as nanoseconds since the epoch.","title":"sec2gmt"},{"location":"miller/verbi/#seqgen","text":"mlr seqgen --help Passes input records directly to output. Most useful for format conversion. Produces a sequence of counters. Discards the input record stream. Produces output as specified by the options Options: -f {name} (default \"i\") Field name for counters. --start {value} (default 1) Inclusive start value. --step {value} (default 1) Step value. --stop {value} (default 100) Inclusive stop value. Start, stop, and/or step may be floating-point. Output is integer if start, stop, and step are all integers. Step may be negative. It may not be zero unless start == stop.","title":"seqgen"},{"location":"miller/verbi/#shuffle","text":"mlr shuffle --help Outputs records randomly permuted. No output records are produced until all input records are read. See also mlr bootstrap and mlr sample. Options:","title":"shuffle"},{"location":"miller/verbi/#skip-trivial-records","text":"mlr skip-trivial-records --help Passes through all records except those with zero fields, or those for which all fields have empty value. Options:","title":"skip-trivial-records"},{"location":"miller/verbi/#sort","text":"mlr sort --help Sorts records primarily by the first specified field, secondarily by the second field, and so on. (Any records not having all specified sort keys will appear at the end of the output, in the order they were encountered, regardless of the specified sort order.) The sort is stable: records that compare equal will sort in the order they were encountered in the input record stream. Options: -f {comma-separated field names} Lexical ascending -r {comma-separated field names} Lexical descending -c {comma-separated field names} Case-folded lexical ascending -cr {comma-separated field names} Case-folded lexical descending -n {comma-separated field names} Numerical ascending; nulls sort last -nf {comma-separated field names} Same as -n -nr {comma-separated field names} Numerical descending; nulls sort first -t {comma-separated field names} Natural ascending -tr {comma-separated field names} Natural descending -h|--help Show this message. Example: mlr sort -f a,b -nr x,y,z which is the same as: mlr sort -f a -f b -nr x -nr y -nr z","title":"sort"},{"location":"miller/verbi/#sort-within-records","text":"Riordina i campi in ordine lessicamente crescente per nome campo. mlr sort-within-records --help Outputs records sorted lexically ascending by keys. Options: -r Recursively sort subobjects/submaps, e.g. for JSON input. Ad esempio a questo file, in cui i campi hanno ordine diverso eterogeneita_irregular.json { \"a\": 1, \"b\": 2, \"c\": 3 } { \"c\": 6, \"a\": 4, \"b\": 5 } { \"b\": 8, \"c\": 9, \"a\": 7 } si potr\u00e0 applicare questo comando per ordinare i campi secondo l'ordine alfabetico dei loro nomi mlr --json sort-within-records ./eterogeneita_irregular.json { \"a\": 1, \"b\": 2, \"c\": 3 } { \"a\": 4, \"b\": 5, \"c\": 6 } { \"a\": 7, \"b\": 8, \"c\": 9 }","title":"sort-within-records"},{"location":"miller/verbi/#stats1","text":"mlr stats1 --help Computes univariate statistics for one or more given fields, accumulated across the input record stream. Options: -a {sum,count,...} Names of accumulators: one or more of: median This is the same as p50 p10 p25.2 p50 p98 p100 etc. count Count instances of fields mode Find most-frequently-occurring values for fields; first-found wins tie antimode Find least-frequently-occurring values for fields; first-found wins tie sum Compute sums of specified fields mean Compute averages (sample means) of specified fields var Compute sample variance of specified fields stddev Compute sample standard deviation of specified fields meaneb Estimate error bars for averages (assuming no sample autocorrelation) skewness Compute sample skewness of specified fields kurtosis Compute sample kurtosis of specified fields min Compute minimum values of specified fields max Compute maximum values of specified fields -f {a,b,c} Value-field names on which to compute statistics --fr {regex} Regex for value-field names on which to compute statistics (compute statistics on values in all field names matching regex --fx {regex} Inverted regex for value-field names on which to compute statistics (compute statistics on values in all field names not matching regex) -g {d,e,f} Optional group-by-field names --gr {regex} Regex for optional group-by-field names (group by values in field names matching regex) --gx {regex} Inverted regex for optional group-by-field names (group by values in field names not matching regex) --grfx {regex} Shorthand for --gr {regex} --fx {that same regex} -i Use interpolated percentiles, like R's type=7; default like type=1. Not sensical for string-valued fields.\\n\"); -s Print iterative stats. Useful in tail -f contexts (in which case please avoid pprint-format output since end of input stream will never be seen). Example: mlr stats1 -a min,p10,p50,p90,max -f value -g size,shape Example: mlr stats1 -a count,mode -f size Example: mlr stats1 -a count,mode -f size -g shape Example: mlr stats1 -a count,mode --fr '^[a-h].*$' -gr '^k.*$' This computes count and mode statistics on all field names beginning with a through h, grouped by all field names starting with k. Notes: * p50 and median are synonymous. * min and max output the same results as p0 and p100, respectively, but use less memory. * String-valued data make sense unless arithmetic on them is required, e.g. for sum, mean, interpolated percentiles, etc. In case of mixed data, numbers are less than strings. * count and mode allow text input; the rest require numeric input. In particular, 1 and 1.0 are distinct text for count and mode. * When there are mode ties, the first-encountered datum wins.","title":"stats1"},{"location":"miller/verbi/#stats2","text":"mlr stats2 --help Computes bivariate statistics for one or more given field-name pairs, accumulated across the input record stream. -a {linreg-ols,corr,...} Names of accumulators: one or more of: linreg-ols Linear regression using ordinary least squares linreg-pca Linear regression using principal component analysis r2 Quality metric for linreg-ols (linreg-pca emits its own) logireg Logistic regression corr Sample correlation cov Sample covariance covx Sample-covariance matrix -f {a,b,c,d} Value-field name-pairs on which to compute statistics. There must be an even number of names. -g {e,f,g} Optional group-by-field names. -v Print additional output for linreg-pca. -s Print iterative stats. Useful in tail -f contexts (in which case please avoid pprint-format output since end of input stream will never be seen). --fit Rather than printing regression parameters, applies them to the input data to compute new fit fields. All input records are held in memory until end of input stream. Has effect only for linreg-ols, linreg-pca, and logireg. Only one of -s or --fit may be used. Example: mlr stats2 -a linreg-pca -f x,y Example: mlr stats2 -a linreg-ols,r2 -f x,y -g size,shape Example: mlr stats2 -a corr -f x,y","title":"stats2"},{"location":"miller/verbi/#step","text":"mlr step --help Computes values dependent on the previous record, optionally grouped by category. Options: -a {delta,rsum,...} Names of steppers: comma-separated, one or more of: delta Compute differences in field(s) between successive records shift Include value(s) in field(s) from previous record, if any from-first Compute differences in field(s) from first record ratio Compute ratios in field(s) between successive records rsum Compute running sums of field(s) between successive records counter Count instances of field(s) between successive records ewma Exponentially weighted moving average over successive records -f {a,b,c} Value-field names on which to compute statistics -g {d,e,f} Optional group-by-field names -F Computes integerable things (e.g. counter) in floating point. As of Miller 6 this happens automatically, but the flag is accepted as a no-op for backward compatibility with Miller 5 and below. -d {x,y,z} Weights for ewma. 1 means current sample gets all weight (no smoothing), near under under 1 is light smoothing, near over 0 is heavy smoothing. Multiple weights may be specified, e.g. \"mlr step -a ewma -f sys_load -d 0.01,0.1,0.9\". Default if omitted is \"-d 0.5\". -o {a,b,c} Custom suffixes for EWMA output fields. If omitted, these default to the -d values. If supplied, the number of -o values must be the same as the number of -d values. Examples: mlr step -a rsum -f request_size mlr step -a delta -f request_size -g hostname mlr step -a ewma -d 0.1,0.9 -f x,y mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y mlr step -a ewma -d 0.1,0.9 -o smooth,rough -f x,y -g group_name Please see https://miller.readthedocs.io/en/latest/reference-verbs.html#filter or https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average for more information on EWMA.","title":"step"},{"location":"miller/verbi/#tac","text":"mlr tac --help Prints records in reverse order from the order in which they were encountered. Options:","title":"tac"},{"location":"miller/verbi/#tail","text":"mlr tail --help Passes through the last n records, optionally by category. Options: -g {a,b,c} Optional group-by-field names for head counts, e.g. a,b,c. -n {n} Head-count to print. Default 10.","title":"tail"},{"location":"miller/verbi/#tee","text":"mlr tee --help Options: -a Append to existing file, if any, rather than overwriting. -p Treat filename as a pipe-to command. Any of the output-format command-line flags (see mlr -h). Example: using mlr --icsv --opprint put '...' then tee --ojson ./mytap.dat then stats1 ... the input is CSV, the output is pretty-print tabular, but the tee-file output is written in JSON format.","title":"tee"},{"location":"miller/verbi/#template","text":"mlr template --help Places input-record fields in the order specified by list of column names. If the input record is missing a specified field, it will be filled with the fill-with. If the input record possesses an unspecified field, it will be discarded. Options: -f {a,b,c} Comma-separated field names for template, e.g. a,b,c. -t {filename} CSV file whose header line will be used for template. --fill-with {filler string} What to fill absent fields with. Defaults to the empty string. Example: * Specified fields are a,b,c. * Input record is c=3,a=1,f=6. * Output record is a=1,b=,c=3.","title":"template"},{"location":"miller/verbi/#top","text":"Restituisce i record con i valori pi\u00f9 grandi (o pi\u00f9 piccoli), per uno o pi\u00f9 campi, anche raggruppando per campi. mlr top --help -f {a,b,c} Value-field names for top counts. -g {d,e,f} Optional group-by-field names for top counts. -n {count} How many records to print per category; default 1. -a Print all fields for top-value records; default is to print only value and group-by fields. Requires a single value-field name only. --min Print top smallest values; default is top largest values. -F Keep top values as floats even if they look like integers. -o {name} Field name for output indices. Default \"top_idx\". Prints the n records with smallest/largest values at specified fields, optionally by category. Qui un file (senza intestazione, nel comando di sotto si usa infatti il flag -N ), in cui i primi 4 campi sono a volte duplicati in pi\u00f9 record. input.csv 1,861265,C,A,0.071 1,861265,C,A,0.148 1,861265,C,G,0.001 1,861265,C,G,0.108 1,861265,C,T,0 1,861265,C,T,0.216 2,193456,G,A,0.006 2,193456,G,A,0.094 2,193456,G,C,0.011 2,193456,G,C,0.152 2,193456,G,T,0.003 2,193456,G,T,0.056 Se se si vogliono estrarre le righe con il valore massimo del campo 5 , a parit\u00e0 di valori dei campi 1 , 2 , 3 , 4 , il comando sar\u00e0: mlr --csv -N top -f 5 -g 1,2,3,4 input.tsv 1,861265,C,A,1,0.148 1,861265,C,G,1,0.108 1,861265,C,T,1,0.216 2,193456,G,A,1,0.094 2,193456,G,C,1,0.152 2,193456,G,T,1,0.056 In output soltanto i campi definiti nel comando. Se si vogliono tutti, bisogna aggiungere -a","title":"top"},{"location":"miller/verbi/#unflatten","text":"mlr unflatten --help Reverses flatten. Example: field with name 'a.b.c' and value 4 becomes name 'a' and value '{\"b\": { \"c\": 4 }}'. Options: -f {a,b,c} Comma-separated list of field names to unflatten (default all). -s {string} Separator, defaulting to mlr --flatsep value.","title":"unflatten"},{"location":"miller/verbi/#uniq","text":"mlr uniq --help Prints distinct values for specified field names. With -c, same as count-distinct. For uniq, -f is a synonym for -g. Options: -g {d,e,f} Group-by-field names for uniq counts. -c Show repeat counts in addition to unique values. -n Show only the number of distinct values. -o {name} Field name for output count. Default \"count\". -a Output each unique record only once. Incompatible with -g. With -c, produces unique records, with repeat counts for each. With -n, produces only one record which is the unique-record count. With neither -c nor -n, produces unique records.","title":"uniq"},{"location":"miller/verbi/#unsparsify","text":"mlr unsparsify --help Usage: mlr unsparsify [options] Prints records with the union of field names over all input records. For field names absent in a given record but present in others, fills in a value. This verb retains all input before producing any output. Options: --fill-with {filler string} What to fill absent fields with. Defaults to the empty string. -f {a,b,c} Specify field names to be operated on. Any other fields won't be modified, and operation will be streaming. -h|--help Show this message. Example: if the input is two records, one being 'a=1,b=2' and the other being 'b=3,c=4', then the output is the two records 'a=1,b=2,c=' and 'a=,b=3,c=4'. Alcuni formati, come il JSON , non devono avere lo stesso numero di campi per record. Un esempio il file a seguire: input.json [ { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"giulia\" , \"comuneNascita\" : \"Milano\" } ] Con unsparsify , viene di default prodotto un output in cui tutti i record hanno gli stessi campi, e l'input viene fatto diventare rettangolare (vedi eterogeneit\u00e0 dei record ); laddove erano assenti viene assegnato un valore nullo. mlr --json unsparsify input.json [ { \"nome\" : \"sara\" , \"dataNascita\" : \"2000-02-22\" , \"altezza\" : 166 , \"peso\" : 70.4 , \"comuneNascita\" : \"Roma\" }, { \"nome\" : \"giulia\" , \"dataNascita\" : \"\" , \"altezza\" : \"\" , \"peso\" : \"\" , \"comuneNascita\" : \"Milano\" } ]","title":"unsparsify"},{"location":"scrapecli/","text":"","title":"Index"},{"location":"shell/","text":"","title":"Index"},{"location":"testostrutturato/","text":"","title":"Index"},{"location":"utilities/","text":"Utility \u00b6 In questo elenco, le principali utility citate nella guida e/o in generale consigliate. cat \u00b6 Concatena file e li stampa nello standard output . https://man7.org/linux/man-pages/man1/cat.1.html chardet \u00b6 Rileva l' encoding dei caratteri. https://github.com/chardet/chardet file \u00b6 Restituisce informazioni sui file. https://www.darwinsys.com/file/ head \u00b6 Stampa le prime 10 righe nello standard output . https://man7.org/linux/man-pages/man1/tail.1.html iconv \u00b6 Trasforma una codifica di caratteri in un'altra. https://pubs.opengroup.org/onlinepubs/009695399/functions/iconv.html jq \u00b6 Fa il parsing e trasforma file in formato JSON . https://stedolan.github.io/jq/ less \u00b6 Mostra il contenuto di un file, una pagina per volta, permettendo di eseguire ricerche di testo. https://man7.org/linux/man-pages/man1/less.1.html Miller \u00b6 Uno strumento per eseguire query , modellare e ristrutturare file di testo strutturati in vari formati, tra cui CSV, TSV, JSON e JSON Lines. https://miller.readthedocs.io/en/latest/ scrape-cli \u00b6 Per estrarre testo da pagine HTML, tramite XPATH e CSS selector. https://github.com/aborruso/scrape-cli stat \u00b6 Restituisce informazioi su file e sul filesystem. https://man7.org/linux/man-pages/man1/stat.1.html tail \u00b6 Stampa le ultime 10 righe nello standard output . https://man7.org/linux/man-pages/man1/tail.1.html tldr \u00b6 Una mini guida con i comandi essenziali per utilizzare le utility pi\u00f9 diffuse. https://github.com/tldr-pages/tldr wc \u00b6 Restituisce numero di linee, caratteri, parole e di byte di un file. https://man7.org/linux/man-pages/man1/wc.1.html yq \u00b6 Per fare il parsing e trasformare file in formato YAML (anche TOML e XML ). Lo fa trasformando l' input da YAML a JSON , via jq . https://github.com/kislyuk/yq","title":"Utility"},{"location":"utilities/#utility","text":"In questo elenco, le principali utility citate nella guida e/o in generale consigliate.","title":"Utility"},{"location":"utilities/#cat","text":"Concatena file e li stampa nello standard output . https://man7.org/linux/man-pages/man1/cat.1.html","title":"cat"},{"location":"utilities/#chardet","text":"Rileva l' encoding dei caratteri. https://github.com/chardet/chardet","title":"chardet"},{"location":"utilities/#file","text":"Restituisce informazioni sui file. https://www.darwinsys.com/file/","title":"file"},{"location":"utilities/#head","text":"Stampa le prime 10 righe nello standard output . https://man7.org/linux/man-pages/man1/tail.1.html","title":"head"},{"location":"utilities/#iconv","text":"Trasforma una codifica di caratteri in un'altra. https://pubs.opengroup.org/onlinepubs/009695399/functions/iconv.html","title":"iconv"},{"location":"utilities/#jq","text":"Fa il parsing e trasforma file in formato JSON . https://stedolan.github.io/jq/","title":"jq"},{"location":"utilities/#less","text":"Mostra il contenuto di un file, una pagina per volta, permettendo di eseguire ricerche di testo. https://man7.org/linux/man-pages/man1/less.1.html","title":"less"},{"location":"utilities/#miller","text":"Uno strumento per eseguire query , modellare e ristrutturare file di testo strutturati in vari formati, tra cui CSV, TSV, JSON e JSON Lines. https://miller.readthedocs.io/en/latest/","title":"Miller"},{"location":"utilities/#scrape-cli","text":"Per estrarre testo da pagine HTML, tramite XPATH e CSS selector. https://github.com/aborruso/scrape-cli","title":"scrape-cli"},{"location":"utilities/#stat","text":"Restituisce informazioi su file e sul filesystem. https://man7.org/linux/man-pages/man1/stat.1.html","title":"stat"},{"location":"utilities/#tail","text":"Stampa le ultime 10 righe nello standard output . https://man7.org/linux/man-pages/man1/tail.1.html","title":"tail"},{"location":"utilities/#tldr","text":"Una mini guida con i comandi essenziali per utilizzare le utility pi\u00f9 diffuse. https://github.com/tldr-pages/tldr","title":"tldr"},{"location":"utilities/#wc","text":"Restituisce numero di linee, caratteri, parole e di byte di un file. https://man7.org/linux/man-pages/man1/wc.1.html","title":"wc"},{"location":"utilities/#yq","text":"Per fare il parsing e trasformare file in formato YAML (anche TOML e XML ). Lo fa trasformando l' input da YAML a JSON , via jq . https://github.com/kislyuk/yq","title":"yq"}]}